{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "## Proyecto de Mantenimiento Predictivo - Moto-Compresores\n",
    "\n",
    "### üéØ Objetivo del Notebook\n",
    "Este notebook realiza la **limpieza y preprocesamiento** de los datos operacionales del moto-compresor para prepararlos para el modelado de Machine Learning. El objetivo es consolidar m√∫ltiples archivos de sensores, limpiar inconsistencias, manejar valores at√≠picos y crear un dataset estructurado y confiable.\n",
    "\n",
    "### üìã Tareas Principales\n",
    "1. **Carga y Consolidaci√≥n**: Integrar datos de 28 archivos Excel de sensores\n",
    "2. **Limpieza de Estructura**: Estandarizar nombres de columnas y tipos de datos\n",
    "3. **Conversi√≥n Temporal**: Procesar diferentes formatos de fecha/hora\n",
    "4. **Tratamiento de Valores Faltantes**: Interpolaci√≥n basada en tiempo\n",
    "5. **Manejo de Outliers**: Clipping estad√≠stico para preservar informaci√≥n √∫til\n",
    "6. **Validaci√≥n de Calidad**: M√©tricas de evaluaci√≥n del procesamiento\n",
    "\n",
    "### üõ†Ô∏è Librer√≠as Utilizadas\n",
    "- **pandas**: Manipulaci√≥n y an√°lisis de datos\n",
    "- **numpy**: Operaciones num√©ricas\n",
    "- **pathlib**: Manejo de rutas de archivos\n",
    "- **warnings**: Supresi√≥n de advertencias menores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas exitosamente\n",
      "üìä Versi√≥n de pandas: 2.3.1\n",
      "üî¢ Versi√≥n de numpy: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de librer√≠as esenciales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configuraci√≥n del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
    "print(f\"üìä Versi√≥n de pandas: {pd.__version__}\")\n",
    "print(f\"üî¢ Versi√≥n de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuraci√≥n de rutas:\n",
      "   Raw data: ../data/raw - ‚úÖ Existe\n",
      "   Processed: ../data/processed - ‚úÖ Existe\n",
      "   Eventos: ../eventos - ‚úÖ Existe\n",
      "\n",
      "üìä Archivos de datos encontrados: 28 archivos\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n de rutas de directorio usando pathlib\n",
    "# Esto garantiza compatibilidad cross-platform y rutas relativas robustas\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('..')\n",
    "\n",
    "# Rutas de datos\n",
    "ruta_raw = base_dir / 'data' / 'raw'\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Crear directorio processed si no existe\n",
    "ruta_processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Validaci√≥n de rutas\n",
    "print(\"üìÅ Configuraci√≥n de rutas:\")\n",
    "print(f\"   Raw data: {ruta_raw} - {'‚úÖ Existe' if ruta_raw.exists() else '‚ùå No existe'}\")\n",
    "print(f\"   Processed: {ruta_processed} - {'‚úÖ Existe' if ruta_processed.exists() else '‚ùå No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'‚úÖ Existe' if ruta_eventos.exists() else '‚ùå No existe'}\")\n",
    "\n",
    "# Listar archivos disponibles\n",
    "archivos_excel = list(ruta_raw.glob('*.xls*'))\n",
    "print(f\"\\nüìä Archivos de datos encontrados: {len(archivos_excel)} archivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìÅ Funci√≥n de Carga y Consolidaci√≥n de Datos\n",
    "\n",
    "### üîß Estrategia T√©cnica\n",
    "Desarrollaremos una funci√≥n robusta que:\n",
    "- **Detecta autom√°ticamente** la estructura de cada archivo Excel\n",
    "- **Identifica el encabezado** buscando palabras clave como 'COMPRESOR' y 'MOTOR'\n",
    "- **Maneja diferentes engines** (.xls con xlrd, .xlsx con openpyxl)\n",
    "- **Consolida datos** de m√∫ltiples archivos manteniendo consistencia\n",
    "\n",
    "Esta aproximaci√≥n es porque los archivos industriales pueden tener estructuras variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Ejecutando carga y consolidaci√≥n de datos...\n",
      "üîÑ Iniciando consolidaci√≥n de 28 archivos...\n",
      "\n",
      "üìã 01-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 01-2025.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 02-2024..xls: 697 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 02-2025.xls: 673 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 03-2024..xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 03-2025.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 04-2024..xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 04-2025.xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 05-2024..xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 06-2024..xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 07-2024..xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 08-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 09-2024.xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 1-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 10-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 10-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 11-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 11-2024.xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 12-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 12-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 2-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 3-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 4-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 5-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 6-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 7-2023...xls: 744 filas, 34 columnas (encabezado en fila 3)\n",
      "üìã 8-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "üìã 9-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "\n",
      "üîó Consolidando 28 DataFrames...\n",
      "\n",
      "üìä Resumen de Consolidaci√≥n:\n",
      "   ‚úÖ Archivos exitosos: 28\n",
      "   ‚ùå Archivos fallidos: 0\n",
      "   üìà Dataset final: 20,523 filas, 68 columnas\n"
     ]
    }
   ],
   "source": [
    "def detectar_fila_encabezado(df_raw):\n",
    "    \"\"\"\n",
    "    Detecta autom√°ticamente la fila donde comienzan los encabezados de columnas\n",
    "    buscando palabras clave relacionadas con el moto-compresor\n",
    "    \"\"\"\n",
    "    palabras_clave = ['COMPRESOR', 'MOTOR', 'HORA', 'TIEMPO', 'TEMP', 'PRES']\n",
    "    \n",
    "    for idx, fila in df_raw.iterrows():\n",
    "        # Convertir toda la fila a string y buscar palabras clave\n",
    "        fila_str = ' '.join([str(cell).upper() for cell in fila if pd.notna(cell)])\n",
    "        \n",
    "        # Si encontramos al menos 2 palabras clave, probablemente es el encabezado\n",
    "        coincidencias = sum(1 for palabra in palabras_clave if palabra in fila_str)\n",
    "        if coincidencias >= 2:\n",
    "            return idx\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cargar_archivo_sensor(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo de sensores con detecci√≥n autom√°tica de estructura\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar archivo completo para an√°lisis de estructura\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df_raw = pd.read_excel(file_path, header=None, engine='openpyxl')\n",
    "        else:\n",
    "            df_raw = pd.read_excel(file_path, header=None, engine='xlrd')\n",
    "        \n",
    "        # Detectar fila de encabezado\n",
    "        header_row = detectar_fila_encabezado(df_raw)\n",
    "        \n",
    "        if header_row is None:\n",
    "            print(f\"‚ö†Ô∏è  No se detect√≥ encabezado autom√°ticamente en {file_path.name}, usando fila 0\")\n",
    "            header_row = 0\n",
    "        \n",
    "        # Cargar datos con el encabezado correcto\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df = pd.read_excel(file_path, \n",
    "                             header=header_row,\n",
    "                             skiprows=range(0, header_row) if header_row > 0 else None,\n",
    "                             engine='openpyxl')\n",
    "        else:\n",
    "            df = pd.read_excel(file_path, \n",
    "                             header=header_row,\n",
    "                             skiprows=range(0, header_row) if header_row > 0 else None,\n",
    "                             engine='xlrd')\n",
    "        \n",
    "        # Informaci√≥n b√°sica del archivo\n",
    "        print(f\"üìã {file_path.name}: {df.shape[0]} filas, {df.shape[1]} columnas (encabezado en fila {header_row})\")\n",
    "        \n",
    "        return df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando {file_path.name}: {str(e)}\")\n",
    "        return None, False\n",
    "\n",
    "def cargar_y_consolidar_datos(ruta_raw):\n",
    "    \"\"\"\n",
    "    Carga y consolida todos los archivos de sensores de la ruta especificada\n",
    "    \"\"\"\n",
    "    archivos_excel = list(ruta_raw.glob('*.xls*'))\n",
    "    \n",
    "    if not archivos_excel:\n",
    "        raise FileNotFoundError(f\"No se encontraron archivos Excel en {ruta_raw}\")\n",
    "    \n",
    "    print(f\"üîÑ Iniciando consolidaci√≥n de {len(archivos_excel)} archivos...\\n\")\n",
    "    \n",
    "    dataframes_lista = []\n",
    "    archivos_exitosos = []\n",
    "    archivos_fallidos = []\n",
    "    \n",
    "    for archivo in sorted(archivos_excel):  # Ordenar para procesamiento consistente\n",
    "        df, exito = cargar_archivo_sensor(archivo)\n",
    "        \n",
    "        if exito and df is not None and not df.empty:\n",
    "            dataframes_lista.append(df)\n",
    "            archivos_exitosos.append(archivo.name)\n",
    "        else:\n",
    "            archivos_fallidos.append(archivo.name)\n",
    "    \n",
    "    if not dataframes_lista:\n",
    "        raise ValueError(\"No se pudo cargar ning√∫n archivo exitosamente\")\n",
    "    \n",
    "    # Consolidar todos los DataFrames\n",
    "    print(f\"\\nüîó Consolidando {len(dataframes_lista)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(dataframes_lista, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Reporte de consolidaci√≥n\n",
    "    print(f\"\\nüìä Resumen de Consolidaci√≥n:\")\n",
    "    print(f\"   ‚úÖ Archivos exitosos: {len(archivos_exitosos)}\")\n",
    "    print(f\"   ‚ùå Archivos fallidos: {len(archivos_fallidos)}\")\n",
    "    print(f\"   üìà Dataset final: {df_consolidado.shape[0]:,} filas, {df_consolidado.shape[1]} columnas\")\n",
    "    \n",
    "    if archivos_fallidos:\n",
    "        print(f\"\\n‚ö†Ô∏è  Archivos que fallaron: {', '.join(archivos_fallidos)}\")\n",
    "    \n",
    "    return df_consolidado, archivos_exitosos, archivos_fallidos\n",
    "\n",
    "# Ejecutar consolidaci√≥n\n",
    "print(\"üöÄ Ejecutando carga y consolidaci√≥n de datos...\")\n",
    "df_raw, exitosos, fallidos = cargar_y_consolidar_datos(ruta_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üßπ Pipeline de Limpieza de Datos\n",
    "\n",
    "### üìù Justificaci√≥n de la Estrategia\n",
    "Los datos industriales presentan desaf√≠os t√≠picos:\n",
    "- **Nombres inconsistentes**: Espacios, saltos de l√≠nea, caracteres especiales\n",
    "- **Formatos temporales variables**: N√∫meros de Excel vs. strings de fecha\n",
    "- **Tipos de datos mixtos**: Strings donde deber√≠an ser num√©ricos\n",
    "\n",
    "Implementaremos un pipeline robusto que maneja estos problemas sistem√°ticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Limpiando nombres de columnas...\n",
      "\n",
      "üìã Muestra de nombres originales:\n",
      "   1. 'ESTADO'\n",
      "   2. 'Hora'\n",
      "   3. 'RPM'\n",
      "   4. 'Presi√≥n\n",
      "Succi√≥n'\n",
      "   5. 'Presi√≥n\n",
      "Intermedia'\n",
      "\n",
      "üìã Muestra de nombres limpios:\n",
      "   1. 'estado'\n",
      "   2. 'hora'\n",
      "   3. 'rpm'\n",
      "   4. 'presion_succion'\n",
      "   5. 'presion_intermedia'\n",
      "\n",
      "‚úÖ Limpieza de nombres completada: 68 columnas procesadas\n"
     ]
    }
   ],
   "source": [
    "def limpiar_nombres_columnas(df):\n",
    "    \"\"\"\n",
    "    Limpia y estandariza los nombres de columnas a formato snake_case\n",
    "    \"\"\"\n",
    "    print(\"üî§ Limpiando nombres de columnas...\")\n",
    "    \n",
    "    # Mostrar algunos nombres originales para referencia\n",
    "    print(f\"\\nüìã Muestra de nombres originales:\")\n",
    "    for i, col in enumerate(df.columns[:5]):\n",
    "        print(f\"   {i+1}. '{col}'\")\n",
    "    \n",
    "    nombres_limpios = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Convertir a string si no lo es\n",
    "        nombre = str(col)\n",
    "        \n",
    "        # Convertir a min√∫sculas\n",
    "        nombre = nombre.lower()\n",
    "        \n",
    "        # Reemplazar vocales con acento por las vocales sin acento\n",
    "        nombre = nombre.replace('√°', 'a').replace('√©', 'e').replace('√≠', 'i').replace('√≥', 'o').replace('√∫', 'u')\n",
    "        \n",
    "        # Reemplazar saltos de l√≠nea y espacios m√∫ltiples por un espacio\n",
    "        nombre = re.sub(r'\\s+', ' ', nombre)\n",
    "        \n",
    "        # Reemplazar espacios por guiones bajos\n",
    "        nombre = nombre.replace(' ', '_')\n",
    "        \n",
    "        # Eliminar caracteres especiales, mantener solo letras, n√∫meros y guiones bajos\n",
    "        nombre = re.sub(r'[^a-z0-9_]', '', nombre)\n",
    "        \n",
    "        # Eliminar guiones bajos m√∫ltiples\n",
    "        nombre = re.sub(r'_+', '_', nombre)\n",
    "        \n",
    "        # Eliminar guiones bajos al inicio y final\n",
    "        nombre = nombre.strip('_')\n",
    "        \n",
    "        # Si el nombre est√° vac√≠o, generar uno gen√©rico\n",
    "        if not nombre:\n",
    "            nombre = f'columna_{len(nombres_limpios)}'\n",
    "        \n",
    "        \n",
    "        nombres_limpios.append(nombre)\n",
    "    \n",
    "    # Manejar nombres duplicados\n",
    "    nombres_finales = []\n",
    "    contador_nombres = {}\n",
    "    \n",
    "    for nombre in nombres_limpios:\n",
    "        if nombre in contador_nombres:\n",
    "            contador_nombres[nombre] += 1\n",
    "            nombre_final = f\"{nombre}_{contador_nombres[nombre]}\"\n",
    "        else:\n",
    "            contador_nombres[nombre] = 0\n",
    "            nombre_final = nombre\n",
    "        \n",
    "        nombres_finales.append(nombre_final)\n",
    "    \n",
    "    # Aplicar nombres limpios\n",
    "    df.columns = nombres_finales\n",
    "    \n",
    "    print(f\"\\nüìã Muestra de nombres limpios:\")\n",
    "    for i, col in enumerate(df.columns[:5]):\n",
    "        print(f\"   {i+1}. '{col}'\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Limpieza de nombres completada: {len(nombres_finales)} columnas procesadas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejecutar limpieza de nombres\n",
    "df_clean = limpiar_nombres_columnas(df_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Convirtiendo tipos de datos...\n",
      "‚è∞ Columna de tiempo detectada: 'hora'\n",
      "   ‚úÖ Columna ya en formato datetime\n",
      "   üìä Conversi√≥n temporal: 19,752/20,523 valores v√°lidos (96.2%)\n",
      "   üìÖ Rango temporal: 2023-01-01 00:00:00 a 2025-04-30 23:00:00\n",
      "   ‚úÖ Columna 'hora' establecida como √≠ndice\n",
      "\n",
      "üî¢ Convirtiendo columnas num√©ricas...\n",
      "   ‚ö†Ô∏è  estado: conversi√≥n perdi√≥ muchos valores (0/19752)\n",
      "   ‚ö†Ô∏è  unnamed_0: conversi√≥n perdi√≥ muchos valores (0/744)\n",
      "   ‚úÖ Conversiones num√©ricas exitosas: 65/67 columnas\n"
     ]
    }
   ],
   "source": [
    "def detectar_columna_tiempo(df):\n",
    "    \"\"\"\n",
    "    Detecta la columna de tiempo bas√°ndose en nombres comunes y contenido\n",
    "    \"\"\"\n",
    "    nombres_tiempo = ['hora', 'tiempo', 'time', 'fecha', 'date', 'timestamp']\n",
    "    \n",
    "    # Buscar por nombre\n",
    "    for col in df.columns:\n",
    "        if any(nombre in col.lower() for nombre in nombres_tiempo):\n",
    "            return col\n",
    "    \n",
    "    # Si no encontramos por nombre, buscar la primera columna que parezca temporal\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Verificar si los valores pueden ser fechas\n",
    "            muestra = df[col].dropna().head(100)\n",
    "            if muestra.empty:\n",
    "                continue\n",
    "                \n",
    "            # Intentar conversi√≥n a datetime\n",
    "            pd.to_datetime(muestra, errors='raise')\n",
    "            return col\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def convertir_tiempo_excel_inteligente(serie_tiempo):\n",
    "    \"\"\"\n",
    "    Convierte una serie de tiempo manejando diferentes formatos:\n",
    "    - N√∫meros de Excel (d√≠as desde 1899-12-30)\n",
    "    - Strings de fecha/hora\n",
    "    - Timestamps ya convertidos\n",
    "    \"\"\"\n",
    "    if serie_tiempo.empty:\n",
    "        return serie_tiempo\n",
    "    \n",
    "    # Tomar una muestra para detectar formato\n",
    "    muestra = serie_tiempo.dropna().head(10)\n",
    "    \n",
    "    if muestra.empty:\n",
    "        return serie_tiempo\n",
    "    \n",
    "    primer_valor = muestra.iloc[0]\n",
    "    \n",
    "    try:\n",
    "        # Caso 1: Ya es datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(serie_tiempo):\n",
    "            print(\"   ‚úÖ Columna ya en formato datetime\")\n",
    "            return serie_tiempo\n",
    "        \n",
    "        # Caso 2: N√∫meros de Excel (t√≠picamente entre 40000-50000 para fechas 2009-2037)\n",
    "        if pd.api.types.is_numeric_dtype(serie_tiempo):\n",
    "            if isinstance(primer_valor, (int, float)) and 30000 < primer_valor < 60000:\n",
    "                print(f\"   üî¢ Detectado formato num√©rico Excel (ej: {primer_valor})\")\n",
    "                return pd.to_datetime(serie_tiempo, unit='D', origin='1899-12-30', errors='coerce')\n",
    "        \n",
    "        # Caso 3: Strings de fecha/hora\n",
    "        if isinstance(primer_valor, str):\n",
    "            print(f\"   üìù Detectado formato string (ej: '{primer_valor}')\")\n",
    "            return pd.to_datetime(serie_tiempo, errors='coerce', infer_datetime_format=True)\n",
    "        \n",
    "        # Caso 4: Intentar conversi√≥n general\n",
    "        print(f\"   üîÑ Intentando conversi√≥n general para tipo: {type(primer_valor)}\")\n",
    "        return pd.to_datetime(serie_tiempo, errors='coerce')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error en conversi√≥n temporal: {str(e)}\")\n",
    "        return pd.to_datetime(serie_tiempo, errors='coerce')\n",
    "\n",
    "def convertir_tipos_datos(df):\n",
    "    \"\"\"\n",
    "    Convierte los tipos de datos apropiadamente:\n",
    "    - Columna de tiempo a datetime\n",
    "    - Resto de columnas a num√©rico (float64)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Convirtiendo tipos de datos...\")\n",
    "    \n",
    "    # Detectar columna de tiempo\n",
    "    col_tiempo = detectar_columna_tiempo(df)\n",
    "    \n",
    "    if col_tiempo:\n",
    "        print(f\"‚è∞ Columna de tiempo detectada: '{col_tiempo}'\")\n",
    "        \n",
    "        # Convertir columna de tiempo\n",
    "        df[col_tiempo] = convertir_tiempo_excel_inteligente(df[col_tiempo])\n",
    "        \n",
    "        # Verificar conversi√≥n exitosa\n",
    "        valores_validos = df[col_tiempo].notna().sum()\n",
    "        total_valores = len(df[col_tiempo])\n",
    "        \n",
    "        print(f\"   üìä Conversi√≥n temporal: {valores_validos:,}/{total_valores:,} valores v√°lidos ({valores_validos/total_valores*100:.1f}%)\")\n",
    "        \n",
    "        if valores_validos > 0:\n",
    "            print(f\"   üìÖ Rango temporal: {df[col_tiempo].min()} a {df[col_tiempo].max()}\")\n",
    "            \n",
    "            # Establecer como √≠ndice\n",
    "            df_indexed = df.set_index(col_tiempo)\n",
    "            print(f\"   ‚úÖ Columna '{col_tiempo}' establecida como √≠ndice\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Conversi√≥n temporal fall√≥, manteniendo √≠ndice num√©rico\")\n",
    "            df_indexed = df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No se detect√≥ columna de tiempo, manteniendo √≠ndice num√©rico\")\n",
    "        df_indexed = df\n",
    "    \n",
    "    # Convertir columnas restantes a num√©rico\n",
    "    print(\"\\nüî¢ Convirtiendo columnas num√©ricas...\")\n",
    "    \n",
    "    columnas_numericas = [col for col in df_indexed.columns if col != col_tiempo]\n",
    "    conversiones_exitosas = 0\n",
    "    \n",
    "    for col in columnas_numericas:\n",
    "        try:\n",
    "            # Intentar conversi√≥n a num√©rico\n",
    "            valores_originales = df_indexed[col].notna().sum()\n",
    "            df_indexed[col] = pd.to_numeric(df_indexed[col], errors='coerce')\n",
    "            valores_finales = df_indexed[col].notna().sum()\n",
    "            \n",
    "            if valores_finales >= valores_originales * 0.8:  # 80% de √©xito m√≠nimo\n",
    "                conversiones_exitosas += 1\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  {col}: conversi√≥n perdi√≥ muchos valores ({valores_finales}/{valores_originales})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error convirtiendo {col}: {str(e)}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Conversiones num√©ricas exitosas: {conversiones_exitosas}/{len(columnas_numericas)} columnas\")\n",
    "    \n",
    "    return df_indexed\n",
    "\n",
    "# Ejecutar conversi√≥n de tipos\n",
    "df_typed = convertir_tipos_datos(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            estado         rpm  presion_succion  presion_intermedia  presion_descarga  pres_aceite_comp  temp_cilindro_1  temp_cilindro_2  temp_cilindro_3  temp_cilindro_4  presion_aceite_motor  presion_agua  presion_mult_adm_izq  presion_mult_adm_der  presion_carter  temp_aceite_motor  temp_agua_motor  temp_mult_adm_izq  temp_mult_adm_der  temp_cil_1_l  temp_cil_1_r  temp_cil_2_l  temp_cil_2_r  temp_cil_3_l  temp_cil_3_r  temp_mult_esc_izq  temp_cil_4_l  temp_cil_4_r  temp_cil_5_l  temp_cil_5_r  temp_cil_6_l  temp_cil_6_r  temp_mult_esc_der  unnamed_0            unnamed_1  unnamed_2  unnamed_3  unnamed_4  unnamed_5  unnamed_6  unnamed_7  unnamed_8  unnamed_9  unnamed_10  unnamed_11  unnamed_12  unnamed_13  unnamed_14  unnamed_15  unnamed_16  unnamed_17  unnamed_18  unnamed_19  unnamed_20  unnamed_21  unnamed_22  unnamed_23  unnamed_24  unnamed_25  unnamed_26  unnamed_27  unnamed_28  unnamed_29  unnamed_30  unnamed_31  unnamed_32  unnamed_33\n",
      "hora                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "NaT            NaN         NaN              NaN                 NaN               NaN               NaN              NaN              NaN              NaN              NaN                   NaN           NaN                   NaN                   NaN             NaN                NaN              NaN                NaN                NaN           NaN           NaN           NaN           NaN           NaN           NaN                NaN           NaN           NaN           NaN           NaN           NaN           NaN                NaN        NaN -9223372036854775808        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\n",
      "2024-01-01     NaN  852.637329       251.783905          492.887909        481.991913         33.750099       173.210297       157.138702       158.029404       170.499405              46.11932      13.45822              0.679329              0.296916        0.791417         184.712204       155.938095           113.1427         114.187798    913.094727    666.346313    868.581726    692.105713    713.106018    333.250488          939.55542    738.036621    691.917419     991.60498    357.024689    778.001099    834.956421         910.073914        NaN -9223372036854775808        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20523 entries, NaT to 2023-09-30 23:00:00\n",
      "Data columns (total 67 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   estado                0 non-null      float64\n",
      " 1   rpm                   19678 non-null  float64\n",
      " 2   presion_succion       19679 non-null  float64\n",
      " 3   presion_intermedia    19679 non-null  float64\n",
      " 4   presion_descarga      19679 non-null  float64\n",
      " 5   pres_aceite_comp      19678 non-null  float64\n",
      " 6   temp_cilindro_1       19678 non-null  float64\n",
      " 7   temp_cilindro_2       19678 non-null  float64\n",
      " 8   temp_cilindro_3       19678 non-null  float64\n",
      " 9   temp_cilindro_4       19678 non-null  float64\n",
      " 10  presion_aceite_motor  19678 non-null  float64\n",
      " 11  presion_agua          19678 non-null  float64\n",
      " 12  presion_mult_adm_izq  19678 non-null  float64\n",
      " 13  presion_mult_adm_der  19678 non-null  float64\n",
      " 14  presion_carter        19678 non-null  float64\n",
      " 15  temp_aceite_motor     19678 non-null  float64\n",
      " 16  temp_agua_motor       19678 non-null  float64\n",
      " 17  temp_mult_adm_izq     19676 non-null  float64\n",
      " 18  temp_mult_adm_der     19676 non-null  float64\n",
      " 19  temp_cil_1_l          19678 non-null  float64\n",
      " 20  temp_cil_1_r          19677 non-null  float64\n",
      " 21  temp_cil_2_l          19678 non-null  float64\n",
      " 22  temp_cil_2_r          19676 non-null  float64\n",
      " 23  temp_cil_3_l          19677 non-null  float64\n",
      " 24  temp_cil_3_r          19677 non-null  float64\n",
      " 25  temp_mult_esc_izq     19675 non-null  float64\n",
      " 26  temp_cil_4_l          19677 non-null  float64\n",
      " 27  temp_cil_4_r          19677 non-null  float64\n",
      " 28  temp_cil_5_l          19676 non-null  float64\n",
      " 29  temp_cil_5_r          19677 non-null  float64\n",
      " 30  temp_cil_6_l          19676 non-null  float64\n",
      " 31  temp_cil_6_r          19676 non-null  float64\n",
      " 32  temp_mult_esc_der     19676 non-null  float64\n",
      " 33  unnamed_0             0 non-null      float64\n",
      " 34  unnamed_1             20523 non-null  int64  \n",
      " 35  unnamed_2             743 non-null    float64\n",
      " 36  unnamed_3             743 non-null    float64\n",
      " 37  unnamed_4             743 non-null    float64\n",
      " 38  unnamed_5             743 non-null    float64\n",
      " 39  unnamed_6             743 non-null    float64\n",
      " 40  unnamed_7             743 non-null    float64\n",
      " 41  unnamed_8             743 non-null    float64\n",
      " 42  unnamed_9             743 non-null    float64\n",
      " 43  unnamed_10            743 non-null    float64\n",
      " 44  unnamed_11            743 non-null    float64\n",
      " 45  unnamed_12            743 non-null    float64\n",
      " 46  unnamed_13            743 non-null    float64\n",
      " 47  unnamed_14            743 non-null    float64\n",
      " 48  unnamed_15            743 non-null    float64\n",
      " 49  unnamed_16            743 non-null    float64\n",
      " 50  unnamed_17            743 non-null    float64\n",
      " 51  unnamed_18            743 non-null    float64\n",
      " 52  unnamed_19            743 non-null    float64\n",
      " 53  unnamed_20            743 non-null    float64\n",
      " 54  unnamed_21            743 non-null    float64\n",
      " 55  unnamed_22            743 non-null    float64\n",
      " 56  unnamed_23            743 non-null    float64\n",
      " 57  unnamed_24            743 non-null    float64\n",
      " 58  unnamed_25            743 non-null    float64\n",
      " 59  unnamed_26            743 non-null    float64\n",
      " 60  unnamed_27            743 non-null    float64\n",
      " 61  unnamed_28            743 non-null    float64\n",
      " 62  unnamed_29            743 non-null    float64\n",
      " 63  unnamed_30            743 non-null    float64\n",
      " 64  unnamed_31            743 non-null    float64\n",
      " 65  unnamed_32            743 non-null    float64\n",
      " 66  unnamed_33            743 non-null    float64\n",
      "dtypes: float64(66), int64(1)\n",
      "memory usage: 10.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Verificar columnas y datos actuales\n",
    "print(df_typed.head(2))\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Tratamiento de Valores Faltantes y At√≠picos\n",
    "\n",
    "### ‚ö†Ô∏è Borrar registro de nulos y columnas no identificadas\n",
    "\n",
    "### üìä Estrategia para Valores Faltantes\n",
    "Para series temporales industriales, la **interpolaci√≥n basada en tiempo** es superior a otros m√©todos porque:\n",
    "- **Preserva tendencias temporales** naturales del proceso industrial\n",
    "- **Mantiene correlaciones** entre variables del moto-compresor\n",
    "- **Es m√°s realista** que forward-fill o valores promedio est√°ticos\n",
    "\n",
    "### üìà Estrategia para Outliers\n",
    "Aplicaremos **clipping estad√≠stico global** (percentiles 1-99) porque:\n",
    "- **Preserva informaci√≥n** de eventos operacionales reales\n",
    "- **Elimina errores de medici√≥n** extremos sin perder datos √∫tiles\n",
    "- **Mantiene consistencia** en rangos operacionales para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20523 entries, NaT to 2023-09-30 23:00:00\n",
      "Data columns (total 33 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   estado                0 non-null      float64\n",
      " 1   rpm                   19678 non-null  float64\n",
      " 2   presion_succion       19679 non-null  float64\n",
      " 3   presion_intermedia    19679 non-null  float64\n",
      " 4   presion_descarga      19679 non-null  float64\n",
      " 5   pres_aceite_comp      19678 non-null  float64\n",
      " 6   temp_cilindro_1       19678 non-null  float64\n",
      " 7   temp_cilindro_2       19678 non-null  float64\n",
      " 8   temp_cilindro_3       19678 non-null  float64\n",
      " 9   temp_cilindro_4       19678 non-null  float64\n",
      " 10  presion_aceite_motor  19678 non-null  float64\n",
      " 11  presion_agua          19678 non-null  float64\n",
      " 12  presion_mult_adm_izq  19678 non-null  float64\n",
      " 13  presion_mult_adm_der  19678 non-null  float64\n",
      " 14  presion_carter        19678 non-null  float64\n",
      " 15  temp_aceite_motor     19678 non-null  float64\n",
      " 16  temp_agua_motor       19678 non-null  float64\n",
      " 17  temp_mult_adm_izq     19676 non-null  float64\n",
      " 18  temp_mult_adm_der     19676 non-null  float64\n",
      " 19  temp_cil_1_l          19678 non-null  float64\n",
      " 20  temp_cil_1_r          19677 non-null  float64\n",
      " 21  temp_cil_2_l          19678 non-null  float64\n",
      " 22  temp_cil_2_r          19676 non-null  float64\n",
      " 23  temp_cil_3_l          19677 non-null  float64\n",
      " 24  temp_cil_3_r          19677 non-null  float64\n",
      " 25  temp_mult_esc_izq     19675 non-null  float64\n",
      " 26  temp_cil_4_l          19677 non-null  float64\n",
      " 27  temp_cil_4_r          19677 non-null  float64\n",
      " 28  temp_cil_5_l          19676 non-null  float64\n",
      " 29  temp_cil_5_r          19677 non-null  float64\n",
      " 30  temp_cil_6_l          19676 non-null  float64\n",
      " 31  temp_cil_6_r          19676 non-null  float64\n",
      " 32  temp_mult_esc_der     19676 non-null  float64\n",
      "dtypes: float64(33)\n",
      "memory usage: 5.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas con todos los valores vac√≠os o nulos\n",
    "df_typed.dropna(inplace=True, how='all')\n",
    "\n",
    "# Eliminar columnas que su nombre empiece con 'unnamed'\n",
    "unnamed_cols = [col for col in df_typed.columns if col.startswith('unnamed')]\n",
    "df_typed.drop(columns=unnamed_cols, inplace=True)\n",
    "\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analizando valores faltantes...\n",
      "\n",
      "üìä Resumen General:\n",
      "   Total de valores: 677,259\n",
      "   Valores faltantes: 47,583 (7.03%)\n",
      "   Columnas con valores faltantes: 33\n",
      "\n",
      "üìã Top 10 columnas con m√°s valores faltantes:\n",
      "   estado: 20,523 (100.0%)\n",
      "   temp_mult_esc_izq: 848 (4.13%)\n",
      "   temp_cil_5_l: 847 (4.13%)\n",
      "   temp_cil_6_r: 847 (4.13%)\n",
      "   temp_cil_6_l: 847 (4.13%)\n",
      "   temp_mult_esc_der: 847 (4.13%)\n",
      "   temp_cil_2_r: 847 (4.13%)\n",
      "   temp_mult_adm_der: 847 (4.13%)\n",
      "   temp_mult_adm_izq: 847 (4.13%)\n",
      "   rpm: 845 (4.12%)\n",
      "\n",
      "üîß Aplicando tratamiento de valores faltantes...\n",
      "   üìÖ Aplicando interpolaci√≥n temporal (method='time')...\n",
      "   ‚ö†Ô∏è  Advertencia: √çndice contiene valores NaN, usando interpolaci√≥n lineal\n",
      "   üìä Resultados:\n",
      "      Valores faltantes antes: 47,583\n",
      "      Valores faltantes despu√©s: 20,523\n",
      "      Valores interpolados: 27,060\n",
      "      Tasa de √©xito: 56.9%\n"
     ]
    }
   ],
   "source": [
    "def analizar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Analiza el patr√≥n de valores faltantes en el dataset\n",
    "    \"\"\"\n",
    "    print(\"üîç Analizando valores faltantes...\")\n",
    "    \n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Calcular estad√≠sticas de valores faltantes por columna\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'columna': df.columns,\n",
    "        'valores_faltantes': df.isnull().sum(),\n",
    "        'porcentaje_faltante': (df.isnull().sum() / total_filas * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Ordenar por porcentaje de valores faltantes\n",
    "    missing_stats = missing_stats.sort_values('porcentaje_faltante', ascending=False)\n",
    "    \n",
    "    # Mostrar estad√≠sticas generales\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    total_valores = df.size\n",
    "    porcentaje_total = (total_missing / total_valores * 100)\n",
    "    \n",
    "    print(f\"\\nüìä Resumen General:\")\n",
    "    print(f\"   Total de valores: {total_valores:,}\")\n",
    "    print(f\"   Valores faltantes: {total_missing:,} ({porcentaje_total:.2f}%)\")\n",
    "    print(f\"   Columnas con valores faltantes: {(missing_stats['valores_faltantes'] > 0).sum()}\")\n",
    "    \n",
    "    # Mostrar columnas con m√°s valores faltantes\n",
    "    columnas_con_faltantes = missing_stats[missing_stats['valores_faltantes'] > 0]\n",
    "    \n",
    "    if not columnas_con_faltantes.empty:\n",
    "        print(f\"\\nüìã Top 10 columnas con m√°s valores faltantes:\")\n",
    "        for _, row in columnas_con_faltantes.head(10).iterrows():\n",
    "            print(f\"   {row['columna']}: {row['valores_faltantes']:,} ({row['porcentaje_faltante']}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ ¬°No hay valores faltantes en el dataset!\")\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "def tratar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Trata los valores faltantes usando interpolaci√≥n temporal\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß Aplicando tratamiento de valores faltantes...\")\n",
    "    \n",
    "    # Estad√≠sticas antes del tratamiento\n",
    "    missing_antes = df.isnull().sum().sum()\n",
    "    \n",
    "    if missing_antes == 0:\n",
    "        print(\"‚úÖ No hay valores faltantes que tratar\")\n",
    "        return df, 0, 0\n",
    "    \n",
    "    try:\n",
    "        # Crear una copia del DataFrame para evitar modificar el original\n",
    "        df_filled = df.copy()\n",
    "        \n",
    "        # Aplicar interpolaci√≥n temporal si tenemos √≠ndice de tiempo\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            print(\"   üìÖ Aplicando interpolaci√≥n temporal (method='time')...\")\n",
    "            # Verificar si hay NaNs en el √≠ndice que podr√≠an causar problemas\n",
    "            if df.index.isna().any():\n",
    "                print(\"   ‚ö†Ô∏è  Advertencia: √çndice contiene valores NaN, usando interpolaci√≥n lineal\")\n",
    "                df_filled = df_filled.interpolate(method='linear', limit_direction='both')\n",
    "            else:\n",
    "                df_filled = df_filled.interpolate(method='time', limit_direction='both')\n",
    "        else:\n",
    "            print(\"   üìà Aplicando interpolaci√≥n lineal (no hay √≠ndice temporal)...\")\n",
    "            df_filled = df_filled.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        # Para valores que siguen faltando al inicio/final, usar backward/forward fill\n",
    "        # Reemplazar m√©todos deprecated\n",
    "        df_filled = df_filled.bfill().ffill()\n",
    "        \n",
    "        # Estad√≠sticas despu√©s del tratamiento\n",
    "        missing_despues = df_filled.isnull().sum().sum()\n",
    "        valores_interpolados = missing_antes - missing_despues\n",
    "        \n",
    "        print(f\"   üìä Resultados:\")\n",
    "        print(f\"      Valores faltantes antes: {missing_antes:,}\")\n",
    "        print(f\"      Valores faltantes despu√©s: {missing_despues:,}\")\n",
    "        print(f\"      Valores interpolados: {valores_interpolados:,}\")\n",
    "        \n",
    "        if missing_antes > 0:\n",
    "            print(f\"      Tasa de √©xito: {(valores_interpolados/missing_antes*100):.1f}%\")\n",
    "        \n",
    "        return df_filled, missing_antes, valores_interpolados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error durante la interpolaci√≥n: {str(e)}\")\n",
    "        print(\"   üîÑ Intentando m√©todo alternativo con forward/backward fill √∫nicamente...\")\n",
    "        \n",
    "        try:\n",
    "            # M√©todo alternativo: solo usar forward/backward fill\n",
    "            df_filled = df.copy()\n",
    "            df_filled = df_filled.bfill().ffill()\n",
    "            \n",
    "            missing_despues = df_filled.isnull().sum().sum()\n",
    "            valores_interpolados = missing_antes - missing_despues\n",
    "            \n",
    "            print(f\"   üìä Resultados (m√©todo alternativo):\")\n",
    "            print(f\"      Valores faltantes antes: {missing_antes:,}\")\n",
    "            print(f\"      Valores faltantes despu√©s: {missing_despues:,}\")\n",
    "            print(f\"      Valores rellenados: {valores_interpolados:,}\")\n",
    "            \n",
    "            if missing_antes > 0:\n",
    "                print(f\"      Tasa de √©xito: {(valores_interpolados/missing_antes*100):.1f}%\")\n",
    "            \n",
    "            return df_filled, missing_antes, valores_interpolados\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Error tambi√©n en m√©todo alternativo: {str(e2)}\")\n",
    "            print(\"   ‚ö†Ô∏è  Retornando DataFrame original sin cambios\")\n",
    "            return df, missing_antes, 0\n",
    "\n",
    "\n",
    "# Ejecutar an√°lisis y tratamiento de valores faltantes\n",
    "missing_stats = analizar_valores_faltantes(df_typed)\n",
    "df_no_missing, missing_original, interpolated = tratar_valores_faltantes(df_typed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analizando outliers en columnas num√©ricas...\n",
      "\n",
      "üìä Resumen de Outliers:\n",
      "   Columnas analizadas: 33\n",
      "   Total outliers (percentil 1-99): 11,958 (1.82%)\n",
      "\n",
      "üìã Top 10 columnas con m√°s outliers (m√©todo percentil):\n",
      "   rpm: 412 outliers (2.0%)\n",
      "   presion_intermedia: 412 outliers (2.0%)\n",
      "   presion_descarga: 412 outliers (2.0%)\n",
      "   pres_aceite_comp: 412 outliers (2.0%)\n",
      "   presion_agua: 412 outliers (2.0%)\n",
      "   presion_mult_adm_izq: 412 outliers (2.0%)\n",
      "   presion_aceite_motor: 412 outliers (2.0%)\n",
      "   temp_mult_adm_der: 412 outliers (2.0%)\n",
      "   temp_cil_4_l: 412 outliers (2.0%)\n",
      "   temp_cil_6_r: 412 outliers (2.0%)\n",
      "\n",
      "üîß Aplicando clipping de outliers...\n",
      "   üìä Procesando 33 columnas num√©ricas...\n",
      "      rpm: 412 valores clipped (2.0%) [-106.41, 872.91]\n",
      "      presion_succion: 411 valores clipped (2.0%) [-74.85, 486.60]\n",
      "      presion_intermedia: 412 valores clipped (2.0%) [-117.62, 496.31]\n",
      "      presion_descarga: 412 valores clipped (2.0%) [-124.46, 483.54]\n",
      "      pres_aceite_comp: 412 valores clipped (2.0%) [-24.55, 38.71]\n",
      "      temp_cilindro_1: 206 valores clipped (1.0%) [72.37, 2191.88]\n",
      "      temp_cilindro_2: 206 valores clipped (1.0%) [75.23, 2191.88]\n",
      "      temp_cilindro_3: 205 valores clipped (1.0%) [75.58, 2191.88]\n",
      "      temp_cilindro_4: 206 valores clipped (1.0%) [73.20, 2191.88]\n",
      "      presion_aceite_motor: 412 valores clipped (2.0%) [-24.23, 47.97]\n",
      "      presion_agua: 412 valores clipped (2.0%) [0.62, 18.38]\n",
      "      presion_mult_adm_izq: 412 valores clipped (2.0%) [-26.08, 1.97]\n",
      "      presion_mult_adm_der: 411 valores clipped (2.0%) [-25.95, 2.36]\n",
      "      presion_carter: 206 valores clipped (1.0%) [0.00, 1.18]\n",
      "      temp_aceite_motor: 411 valores clipped (2.0%) [65.09, 1125.83]\n",
      "      temp_agua_motor: 411 valores clipped (2.0%) [82.16, 1392.98]\n",
      "      temp_mult_adm_izq: 238 valores clipped (1.2%) [-99.74, 128.35]\n",
      "      temp_mult_adm_der: 412 valores clipped (2.0%) [-4.71, 127.52]\n",
      "      temp_cil_1_l: 412 valores clipped (2.0%) [77.46, 1281.55]\n",
      "      temp_cil_1_r: 409 valores clipped (2.0%) [79.11, 1281.33]\n",
      "      temp_cil_2_l: 411 valores clipped (2.0%) [79.97, 1281.74]\n",
      "      temp_cil_2_r: 409 valores clipped (2.0%) [79.38, 1281.35]\n",
      "      temp_cil_3_l: 408 valores clipped (2.0%) [80.11, 1281.74]\n",
      "      temp_cil_3_r: 412 valores clipped (2.0%) [78.29, 1280.80]\n",
      "      temp_mult_esc_izq: 411 valores clipped (2.0%) [69.32, 1279.89]\n",
      "      temp_cil_4_l: 412 valores clipped (2.0%) [80.25, 1281.74]\n",
      "      temp_cil_4_r: 410 valores clipped (2.0%) [79.70, 1281.42]\n",
      "      temp_cil_5_l: 409 valores clipped (2.0%) [80.06, 1281.65]\n",
      "      temp_cil_5_r: 412 valores clipped (2.0%) [79.00, 1381.84]\n",
      "      temp_cil_6_l: 412 valores clipped (2.0%) [79.62, 1281.33]\n",
      "      temp_cil_6_r: 412 valores clipped (2.0%) [77.90, 849.28]\n",
      "      temp_mult_esc_der: 412 valores clipped (2.0%) [68.56, 968.63]\n",
      "\n",
      "   ‚úÖ Clipping completado: 11,958 valores modificados en total\n"
     ]
    }
   ],
   "source": [
    "def analizar_outliers(df):\n",
    "    \"\"\"\n",
    "    Analiza outliers en las columnas num√©ricas del dataset\n",
    "    \"\"\"\n",
    "    print(\"üîç Analizando outliers en columnas num√©ricas...\")\n",
    "    \n",
    "    # Seleccionar solo columnas num√©ricas\n",
    "    cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(cols_numericas) == 0:\n",
    "        print(\"‚ö†Ô∏è  No se encontraron columnas num√©ricas\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    outlier_stats = []\n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        serie = df[col].dropna()\n",
    "        \n",
    "        if len(serie) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular percentiles\n",
    "        q01 = serie.quantile(0.01)\n",
    "        q99 = serie.quantile(0.99)\n",
    "        q25 = serie.quantile(0.25)\n",
    "        q75 = serie.quantile(0.75)\n",
    "        \n",
    "        # Contar outliers usando m√©todo de percentiles\n",
    "        outliers_bajos = (serie < q01).sum()\n",
    "        outliers_altos = (serie > q99).sum()\n",
    "        total_outliers = outliers_bajos + outliers_altos\n",
    "        \n",
    "        # Contar outliers usando m√©todo IQR (para comparaci√≥n)\n",
    "        iqr = q75 - q25\n",
    "        lower_iqr = q25 - 1.5 * iqr\n",
    "        upper_iqr = q75 + 1.5 * iqr\n",
    "        outliers_iqr = ((serie < lower_iqr) | (serie > upper_iqr)).sum()\n",
    "        \n",
    "        outlier_stats.append({\n",
    "            'columna': col,\n",
    "            'total_valores': len(serie),\n",
    "            'outliers_percentil': total_outliers,\n",
    "            'outliers_iqr': outliers_iqr,\n",
    "            'porcentaje_percentil': (total_outliers / len(serie) * 100),\n",
    "            'porcentaje_iqr': (outliers_iqr / len(serie) * 100),\n",
    "            'q01': q01,\n",
    "            'q99': q99,\n",
    "            'min_original': serie.min(),\n",
    "            'max_original': serie.max()\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_stats)\n",
    "    outlier_df = outlier_df.sort_values('porcentaje_percentil', ascending=False)\n",
    "    \n",
    "    # Resumen general\n",
    "    total_outliers_percentil = outlier_df['outliers_percentil'].sum()\n",
    "    total_valores = outlier_df['total_valores'].sum()\n",
    "    \n",
    "    print(f\"\\nüìä Resumen de Outliers:\")\n",
    "    print(f\"   Columnas analizadas: {len(cols_numericas)}\")\n",
    "    print(f\"   Total outliers (percentil 1-99): {total_outliers_percentil:,} ({total_outliers_percentil/total_valores*100:.2f}%)\")\n",
    "    \n",
    "    # Mostrar top columnas con m√°s outliers\n",
    "    print(f\"\\nüìã Top 10 columnas con m√°s outliers (m√©todo percentil):\")\n",
    "    for _, row in outlier_df.head(10).iterrows():\n",
    "        print(f\"   {row['columna']}: {row['outliers_percentil']:,} outliers ({row['porcentaje_percentil']:.1f}%)\")\n",
    "    \n",
    "    return outlier_df\n",
    "\n",
    "def aplicar_clipping_outliers(df):\n",
    "    \"\"\"\n",
    "    Aplica clipping de outliers usando percentiles 1 y 99\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß Aplicando clipping de outliers...\")\n",
    "    \n",
    "    # Seleccionar columnas num√©ricas\n",
    "    cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(cols_numericas) == 0:\n",
    "        print(\"‚ö†Ô∏è  No se encontraron columnas num√©ricas para procesar\")\n",
    "        return df, 0\n",
    "    \n",
    "    df_clipped = df.copy()\n",
    "    total_clipped = 0\n",
    "    \n",
    "    print(f\"   üìä Procesando {len(cols_numericas)} columnas num√©ricas...\")\n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        serie_original = df_clipped[col]\n",
    "        serie_valida = serie_original.dropna()\n",
    "        \n",
    "        if len(serie_valida) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular l√≠mites de clipping\n",
    "        limite_inferior = serie_valida.quantile(0.01)\n",
    "        limite_superior = serie_valida.quantile(0.99)\n",
    "        \n",
    "        # Aplicar clipping\n",
    "        serie_clipped = serie_original.clip(lower=limite_inferior, upper=limite_superior)\n",
    "        \n",
    "        # Contar valores modificados\n",
    "        valores_modificados = (serie_original != serie_clipped).sum()\n",
    "        total_clipped += valores_modificados\n",
    "        \n",
    "        # Actualizar la columna\n",
    "        df_clipped[col] = serie_clipped\n",
    "        \n",
    "        if valores_modificados > 0:\n",
    "            porcentaje = (valores_modificados / len(serie_original) * 100)\n",
    "            print(f\"      {col}: {valores_modificados:,} valores clipped ({porcentaje:.1f}%) [{limite_inferior:.2f}, {limite_superior:.2f}]\")\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Clipping completado: {total_clipped:,} valores modificados en total\")\n",
    "    \n",
    "    return df_clipped, total_clipped\n",
    "\n",
    "# Ejecutar an√°lisis y tratamiento de outliers\n",
    "outlier_stats = analizar_outliers(df_no_missing)\n",
    "df_final, total_clipped = aplicar_clipping_outliers(df_no_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìä Validaci√≥n de Calidad de Datos\n",
    "\n",
    "### üéØ M√©tricas de Calidad\n",
    "Evaluaremos la calidad del preprocesamiento mediante m√©tricas cuantitativas que demuestren la efectividad de cada paso del pipeline de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã REPORTE DE CALIDAD DEL PREPROCESAMIENTO\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  DIMENSIONES DEL DATASET\n",
      "   Filas originales: 20,523\n",
      "   Filas finales: 20,523\n",
      "   Columnas originales: 68\n",
      "   Columnas finales: 33\n",
      "\n",
      "2Ô∏è‚É£  CALIDAD DE DATOS\n",
      "   Valores faltantes originales: 47,583\n",
      "   Valores interpolados: 27,060\n",
      "   Valores faltantes finales: 20,523\n",
      "   Tasa de completitud: 96.97%\n",
      "\n",
      "3Ô∏è‚É£  TRATAMIENTO DE OUTLIERS\n",
      "   Valores clipped: 11,958\n",
      "   Porcentaje de valores modificados: 1.77%\n",
      "\n",
      "4Ô∏è‚É£  TIPOS DE DATOS\n",
      "   float64: 33 columnas\n",
      "\n",
      "5Ô∏è‚É£  INFORMACI√ìN TEMPORAL\n",
      "   √çndice temporal: ‚úÖ Configurado\n",
      "   Rango temporal: 2023-01-01 00:00:00 a 2025-04-30 23:00:00\n",
      "   Duraci√≥n total: 850 days 23:00:00\n",
      "\n",
      "6Ô∏è‚É£  RESUMEN DE COLUMNAS\n",
      "   Columnas num√©ricas: 33\n",
      "   Columnas temporales: 0\n",
      "   Otras columnas: 0\n",
      "\n",
      "7Ô∏è‚É£  ESTAD√çSTICAS DESCRIPTIVAS (Columnas Num√©ricas)\n",
      "   Promedio de medias: 252.59\n",
      "   Promedio de desv. est√°ndar: 231.57\n",
      "   Rango de valores m√≠nimos: [-124.46, 82.16]\n",
      "   Rango de valores m√°ximos: [1.18, 2191.88]\n",
      "\n",
      "============================================================\n",
      "‚úÖ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def generar_reporte_calidad(df_original, df_final, missing_original, interpolated, total_clipped):\n",
    "    \"\"\"\n",
    "    Genera un reporte completo de la calidad del preprocesamiento\n",
    "    \"\"\"\n",
    "    print(\"üìã REPORTE DE CALIDAD DEL PREPROCESAMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Estad√≠sticas dimensionales\n",
    "    print(\"\\n1Ô∏è‚É£  DIMENSIONES DEL DATASET\")\n",
    "    print(f\"   Filas originales: {len(df_original):,}\")\n",
    "    print(f\"   Filas finales: {len(df_final):,}\")\n",
    "    print(f\"   Columnas originales: {df_original.shape[1]}\")\n",
    "    print(f\"   Columnas finales: {df_final.shape[1]}\")\n",
    "    \n",
    "    # 2. Calidad de datos\n",
    "    print(\"\\n2Ô∏è‚É£  CALIDAD DE DATOS\")\n",
    "    missing_final = df_final.isnull().sum().sum()\n",
    "    total_valores = df_final.size\n",
    "    \n",
    "    print(f\"   Valores faltantes originales: {missing_original:,}\")\n",
    "    print(f\"   Valores interpolados: {interpolated:,}\")\n",
    "    print(f\"   Valores faltantes finales: {missing_final:,}\")\n",
    "    print(f\"   Tasa de completitud: {((total_valores - missing_final) / total_valores * 100):.2f}%\")\n",
    "    \n",
    "    # 3. Tratamiento de outliers\n",
    "    print(\"\\n3Ô∏è‚É£  TRATAMIENTO DE OUTLIERS\")\n",
    "    print(f\"   Valores clipped: {total_clipped:,}\")\n",
    "    print(f\"   Porcentaje de valores modificados: {(total_clipped / total_valores * 100):.2f}%\")\n",
    "    \n",
    "    # 4. Tipos de datos\n",
    "    print(\"\\n4Ô∏è‚É£  TIPOS DE DATOS\")\n",
    "    tipos_finales = df_final.dtypes.value_counts()\n",
    "    for tipo, cantidad in tipos_finales.items():\n",
    "        print(f\"   {tipo}: {cantidad} columnas\")\n",
    "    \n",
    "    # 5. Informaci√≥n temporal\n",
    "    print(\"\\n5Ô∏è‚É£  INFORMACI√ìN TEMPORAL\")\n",
    "    if isinstance(df_final.index, pd.DatetimeIndex):\n",
    "        print(f\"   √çndice temporal: ‚úÖ Configurado\")\n",
    "        print(f\"   Rango temporal: {df_final.index.min()} a {df_final.index.max()}\")\n",
    "        print(f\"   Duraci√≥n total: {df_final.index.max() - df_final.index.min()}\")\n",
    "    else:\n",
    "        print(f\"   √çndice temporal: ‚ùå No configurado (√≠ndice num√©rico)\")\n",
    "    \n",
    "    # 6. Resumen de columnas\n",
    "    print(\"\\n6Ô∏è‚É£  RESUMEN DE COLUMNAS\")\n",
    "    cols_numericas = df_final.select_dtypes(include=[np.number]).columns\n",
    "    cols_temporales = df_final.select_dtypes(include=['datetime']).columns\n",
    "    cols_otros = df_final.select_dtypes(exclude=[np.number, 'datetime']).columns\n",
    "    \n",
    "    print(f\"   Columnas num√©ricas: {len(cols_numericas)}\")\n",
    "    print(f\"   Columnas temporales: {len(cols_temporales)}\")\n",
    "    print(f\"   Otras columnas: {len(cols_otros)}\")\n",
    "    \n",
    "    # 7. Estad√≠sticas descriptivas b√°sicas\n",
    "    if len(cols_numericas) > 0:\n",
    "        print(\"\\n7Ô∏è‚É£  ESTAD√çSTICAS DESCRIPTIVAS (Columnas Num√©ricas)\")\n",
    "        stats_desc = df_final[cols_numericas].describe()\n",
    "        print(f\"   Promedio de medias: {stats_desc.loc['mean'].mean():.2f}\")\n",
    "        print(f\"   Promedio de desv. est√°ndar: {stats_desc.loc['std'].mean():.2f}\")\n",
    "        print(f\"   Rango de valores m√≠nimos: [{stats_desc.loc['min'].min():.2f}, {stats_desc.loc['min'].max():.2f}]\")\n",
    "        print(f\"   Rango de valores m√°ximos: [{stats_desc.loc['max'].min():.2f}, {stats_desc.loc['max'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generar reporte de calidad\n",
    "generar_reporte_calidad(df_raw, df_final, missing_original, interpolated, total_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üíæ Guardado del Dataset Procesado\n",
    "\n",
    "### üéØ Objetivo Final\n",
    "Guardar el dataset completamente procesado en formato CSV para uso en las siguientes fases del proyecto:\n",
    "- **Feature Engineering** (03_feature_engineering.ipynb)\n",
    "- **Model Training** (04_model_training.ipynb)\n",
    "- **Model Evaluation** (05_model_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Guardando dataset final...\n",
      "   üìä Dimensiones: 20,523 filas √ó 33 columnas\n",
      "   üìÑ Guardando CSV...\n",
      "      ‚úÖ CSV: 11.8 MB\n",
      "   üì¶ Guardando Parquet (subprocess aislado)...\n",
      "      ‚úÖ Parquet: 3.2 MB\n",
      "   üìÑ Generando archivos de metadatos...\n",
      "      ‚úÖ preproceso_metadata.txt\n",
      "      ‚úÖ columnas_resumen.csv\n",
      "      ‚úÖ estadistica_basica.csv\n",
      "\n",
      "üìã Resumen del guardado:\n",
      "   ‚úÖ CSV: 11.8 MB\n",
      "   ‚úÖ PARQUET: 3.2 MB\n",
      "\n",
      "üìÅ Archivos generados en data/processed/:\n",
      "   üóÉÔ∏è  timeseries_data.csv - Dataset principal\n",
      "   üì¶ timeseries_data.parquet - Dataset comprimido\n",
      "   üìÑ preproceso_metadata.txt - Metadatos del procesamiento\n",
      "   üìä columnas_resumen.csv - Resumen de columnas\n",
      "   üìà estadistica_basica.csv - Estad√≠sticas descriptivas\n",
      "‚úÖ Guardado completado: 2/2 formatos exitosos\n"
     ]
    }
   ],
   "source": [
    "def guardar_dataset_final(df, ruta_destino, nombre_base='timeseries_data',\n",
    "                         archivos_procesados=None, archivos_fallidos=None,\n",
    "                         valores_interpolados=0, valores_clipped=0):\n",
    "    \"\"\"\n",
    "    Guarda dataset en CSV (siempre) y Parquet (usando subprocess para evitar conflictos)\n",
    "    Incluye todos los archivos de metadatos\n",
    "    \"\"\"\n",
    "    ruta_destino = Path(ruta_destino)\n",
    "    ruta_destino.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Guardando dataset final...\")\n",
    "    print(f\"   üìä Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # ========== CSV (SIEMPRE) ==========\n",
    "    try:\n",
    "        archivo_csv = ruta_destino / f\"{nombre_base}.csv\"\n",
    "        print(f\"   üìÑ Guardando CSV...\")\n",
    "        df.to_csv(archivo_csv, index=True, encoding='utf-8')\n",
    "        tama√±o_mb = archivo_csv.stat().st_size / (1024 * 1024)\n",
    "        print(f\"      ‚úÖ CSV: {tama√±o_mb:.1f} MB\")\n",
    "        resultados['csv'] = {'exito': True, 'tama√±o_mb': tama√±o_mb}\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå CSV Error: {str(e)}\")\n",
    "        return {'csv': {'exito': False, 'error': str(e)}}\n",
    "    \n",
    "    # ========== PARQUET (SUBPROCESS AISLADO) ==========\n",
    "    archivo_parquet = ruta_destino / f\"{nombre_base}.parquet\"\n",
    "    print(f\"   üì¶ Guardando Parquet (subprocess aislado)...\")\n",
    "    \n",
    "    try:\n",
    "        # CSV temporal\n",
    "        temp_csv = ruta_destino / 'temp_for_parquet.csv'\n",
    "        df.to_csv(temp_csv, index=True)\n",
    "        \n",
    "        # Script aislado\n",
    "        script_content = f'''\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"{temp_csv}\", index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Limpiar para Parquet\n",
    "    if hasattr(df.index, 'hasnans') and df.index.hasnans:\n",
    "        df = df[df.index.notna()]\n",
    "    if str(type(df.index)) == \"<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\":\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    df.to_parquet(\"{archivo_parquet}\", engine='pyarrow', index=False, compression='snappy')\n",
    "    print(\"PARQUET_SUCCESS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PARQUET_ERROR: {{e}}\")\n",
    "'''\n",
    "        \n",
    "        # Ejecutar subprocess\n",
    "        result = subprocess.run([sys.executable, '-c', script_content], \n",
    "                              capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        # Limpiar\n",
    "        if temp_csv.exists():\n",
    "            temp_csv.unlink()\n",
    "        \n",
    "        # Verificar\n",
    "        if result.returncode == 0 and \"PARQUET_SUCCESS\" in result.stdout:\n",
    "            tama√±o_mb = archivo_parquet.stat().st_size / (1024 * 1024)\n",
    "            print(f\"      ‚úÖ Parquet: {tama√±o_mb:.1f} MB\")\n",
    "            resultados['parquet'] = {'exito': True, 'tama√±o_mb': tama√±o_mb}\n",
    "        else:\n",
    "            print(f\"      ‚ùå Parquet: Subprocess failed\")\n",
    "            resultados['parquet'] = {'exito': False, 'error': 'subprocess_failed'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Parquet Exception: {str(e)}\")\n",
    "        resultados['parquet'] = {'exito': False, 'error': str(e)}\n",
    "    \n",
    "    # ========== METADATOS COMPLETOS ==========\n",
    "    try:\n",
    "        print(f\"   üìÑ Generando archivos de metadatos...\")\n",
    "        \n",
    "        # 1. Metadatos principales\n",
    "        archivo_meta = ruta_destino / 'preproceso_metadata.txt'\n",
    "        with open(archivo_meta, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"METADATOS DEL PREPROCESAMIENTO\\n\")\n",
    "            f.write(f\"=\" * 40 + \"\\n\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Dimensiones: {df.shape[0]:,} √ó {df.shape[1]}\\n\")\n",
    "            f.write(f\"Archivos procesados: {len(archivos_procesados or [])}\\n\")\n",
    "            f.write(f\"Archivos fallidos: {len(archivos_fallidos or [])}\\n\")\n",
    "            f.write(f\"Valores interpolados: {valores_interpolados:,}\\n\")\n",
    "            f.write(f\"Valores clipped: {valores_clipped:,}\\n\")\n",
    "            f.write(f\"√çndice temporal: {'S√≠' if hasattr(df.index, 'tz') or 'datetime' in str(type(df.index)).lower() else 'No'}\\n\")\n",
    "            f.write(f\"\\nFormatos guardados:\\n\")\n",
    "            for formato, resultado in resultados.items():\n",
    "                status = \"‚úÖ\" if resultado['exito'] else \"‚ùå\" \n",
    "                f.write(f\"  {formato.upper()}: {status}\\n\")\n",
    "            \n",
    "            if archivos_procesados:\n",
    "                f.write(f\"\\nArchivos procesados exitosamente:\\n\")\n",
    "                for archivo in archivos_procesados:\n",
    "                    f.write(f\"  - {archivo}\\n\")\n",
    "            \n",
    "            if archivos_fallidos:\n",
    "                f.write(f\"\\nArchivos que fallaron:\\n\")\n",
    "                for archivo in archivos_fallidos:\n",
    "                    f.write(f\"  - {archivo}\\n\")\n",
    "        \n",
    "        # 2. Resumen de columnas\n",
    "        archivo_columnas = ruta_destino / 'columnas_resumen.csv'\n",
    "        columnas_info = pd.DataFrame({\n",
    "            'columna': df.columns,\n",
    "            'tipo_datos': [str(dtype) for dtype in df.dtypes],\n",
    "            'valores_no_nulos': df.count(),\n",
    "            'valores_nulos': df.isnull().sum(),\n",
    "            'porcentaje_completitud': ((df.count() / len(df)) * 100).round(2)\n",
    "        })\n",
    "        \n",
    "        # Agregar estad√≠sticas para columnas num√©ricas\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        columnas_info['es_numerica'] = columnas_info['columna'].isin(numeric_cols)\n",
    "        \n",
    "        # Estad√≠sticas b√°sicas para columnas num√©ricas\n",
    "        columnas_info['min_valor'] = np.nan\n",
    "        columnas_info['max_valor'] = np.nan\n",
    "        columnas_info['media'] = np.nan\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if not df[col].empty and df[col].notna().any():\n",
    "                idx = columnas_info['columna'] == col\n",
    "                try:\n",
    "                    columnas_info.loc[idx, 'min_valor'] = df[col].min()\n",
    "                    columnas_info.loc[idx, 'max_valor'] = df[col].max()\n",
    "                    columnas_info.loc[idx, 'media'] = df[col].mean()\n",
    "                except:\n",
    "                    pass  # Ignorar errores en estad√≠sticas\n",
    "        \n",
    "        columnas_info.to_csv(archivo_columnas, index=False, encoding='utf-8')\n",
    "        \n",
    "        # 3. Estad√≠sticas descriptivas b√°sicas (solo num√©ricas)\n",
    "        if len(numeric_cols) > 0:\n",
    "            archivo_stats = ruta_destino / 'estadistica_basica.csv'\n",
    "            stats_desc = df[numeric_cols].describe()\n",
    "            stats_desc.to_csv(archivo_stats, encoding='utf-8')\n",
    "        \n",
    "        print(f\"      ‚úÖ preproceso_metadata.txt\")\n",
    "        print(f\"      ‚úÖ columnas_resumen.csv\")\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"      ‚úÖ estadistica_basica.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è  Error en metadatos: {str(e)}\")\n",
    "        pass\n",
    "    \n",
    "    # ========== RESUMEN FINAL ==========\n",
    "    print(f\"\\nüìã Resumen del guardado:\")\n",
    "    exitosos = sum(1 for r in resultados.values() if r['exito'])\n",
    "    for formato, resultado in resultados.items():\n",
    "        if resultado['exito']:\n",
    "            print(f\"   ‚úÖ {formato.upper()}: {resultado['tama√±o_mb']:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {formato.upper()}: Error\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivos generados en data/processed/:\")\n",
    "    print(f\"   üóÉÔ∏è  {nombre_base}.csv - Dataset principal\")\n",
    "    if resultados.get('parquet', {}).get('exito'):\n",
    "        print(f\"   üì¶ {nombre_base}.parquet - Dataset comprimido\")\n",
    "    print(f\"   üìÑ preproceso_metadata.txt - Metadatos del procesamiento\")\n",
    "    print(f\"   üìä columnas_resumen.csv - Resumen de columnas\")\n",
    "    if len(df.select_dtypes(include=[np.number]).columns) > 0:\n",
    "        print(f\"   üìà estadistica_basica.csv - Estad√≠sticas descriptivas\")\n",
    "    \n",
    "    print(f\"‚úÖ Guardado completado: {exitosos}/{len(resultados)} formatos exitosos\")\n",
    "    \n",
    "    return resultados\n",
    "    \n",
    "exito_guardado = guardar_dataset_final(df_final, ruta_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Resumen del Preprocesamiento\n",
    "\n",
    "### ‚úÖ Tareas Completadas\n",
    "\n",
    "1. **Carga y Consolidaci√≥n Inteligente**\n",
    "   - ‚úÖ Detecci√≥n autom√°tica de estructura de archivos Excel\n",
    "   - ‚úÖ Consolidaci√≥n de 28 archivos en un dataset √∫nico\n",
    "   - ‚úÖ Manejo robusto de diferentes formatos (.xls/.xlsx)\n",
    "\n",
    "2. **Limpieza de Estructura de Datos**\n",
    "   - ‚úÖ Normalizaci√≥n de nombres de columnas a formato snake_case\n",
    "   - ‚úÖ Conversi√≥n inteligente de tipos de datos\n",
    "   - ‚úÖ Manejo de diferentes formatos temporales\n",
    "\n",
    "3. **Tratamiento de Calidad de Datos**\n",
    "   - ‚úÖ Interpolaci√≥n temporal de valores faltantes\n",
    "   - ‚úÖ Clipping estad√≠stico de outliers (percentiles 1-99)\n",
    "   - ‚úÖ Preservaci√≥n de informaci√≥n operacional relevante\n",
    "\n",
    "4. **Validaci√≥n y Documentaci√≥n**\n",
    "   - ‚úÖ M√©tricas completas de calidad de datos\n",
    "   - ‚úÖ Documentaci√≥n exhaustiva del procesamiento\n",
    "   - ‚úÖ Archivos de metadatos para trazabilidad\n",
    "\n",
    "### üîÑ Pipeline de Calidad Implementado\n",
    "\n",
    "```\n",
    "Datos Raw ‚Üí Detecci√≥n Autom√°tica ‚Üí Limpieza ‚Üí Conversi√≥n de Tipos ‚Üí \n",
    "Interpolaci√≥n ‚Üí Clipping ‚Üí Validaci√≥n ‚Üí Dataset Limpio\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üìä Estado**: ‚úÖ **Preprocesamiento Completado**  \n",
    "**‚û°Ô∏è  Siguiente fase**: `03_feature_engineering.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
