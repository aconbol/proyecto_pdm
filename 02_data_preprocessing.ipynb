{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "## Proyecto de Mantenimiento Predictivo - Moto-Compresores\n",
    "\n",
    "### 🎯 Objetivo del Notebook\n",
    "Este notebook realiza la **limpieza y preprocesamiento** de los datos operacionales del moto-compresor para prepararlos para el modelado de Machine Learning. El objetivo es consolidar múltiples archivos de sensores, limpiar inconsistencias, manejar valores atípicos y crear un dataset estructurado y confiable.\n",
    "\n",
    "### 📋 Tareas Principales\n",
    "1. **Carga y Consolidación**: Integrar datos de 28 archivos Excel de sensores\n",
    "2. **Limpieza de Estructura**: Estandarizar nombres de columnas y tipos de datos\n",
    "3. **Conversión Temporal**: Procesar diferentes formatos de fecha/hora\n",
    "4. **Tratamiento de Valores Faltantes**: Interpolación basada en tiempo\n",
    "5. **Manejo de Outliers**: Clipping estadístico para preservar información útil\n",
    "6. **Validación de Calidad**: Métricas de evaluación del procesamiento\n",
    "\n",
    "### 🛠️ Librerías Utilizadas\n",
    "- **pandas**: Manipulación y análisis de datos\n",
    "- **numpy**: Operaciones numéricas\n",
    "- **pathlib**: Manejo de rutas de archivos\n",
    "- **warnings**: Supresión de advertencias menores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías importadas exitosamente\n",
      "📊 Versión de pandas: 2.3.1\n",
      "🔢 Versión de numpy: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías esenciales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configuración del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"✅ Librerías importadas exitosamente\")\n",
    "print(f\"📊 Versión de pandas: {pd.__version__}\")\n",
    "print(f\"🔢 Versión de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Configuración de rutas:\n",
      "   Raw data: ../data/raw - ✅ Existe\n",
      "   Processed: ../data/processed - ✅ Existe\n",
      "   Eventos: ../eventos - ✅ Existe\n",
      "\n",
      "📊 Archivos de datos encontrados: 28 archivos\n"
     ]
    }
   ],
   "source": [
    "# Configuración de rutas de directorio usando pathlib\n",
    "# Esto garantiza compatibilidad cross-platform y rutas relativas robustas\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('..')\n",
    "\n",
    "# Rutas de datos\n",
    "ruta_raw = base_dir / 'data' / 'raw'\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Crear directorio processed si no existe\n",
    "ruta_processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Validación de rutas\n",
    "print(\"📁 Configuración de rutas:\")\n",
    "print(f\"   Raw data: {ruta_raw} - {'✅ Existe' if ruta_raw.exists() else '❌ No existe'}\")\n",
    "print(f\"   Processed: {ruta_processed} - {'✅ Existe' if ruta_processed.exists() else '❌ No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'✅ Existe' if ruta_eventos.exists() else '❌ No existe'}\")\n",
    "\n",
    "# Listar archivos disponibles\n",
    "archivos_excel = list(ruta_raw.glob('*.xls*'))\n",
    "print(f\"\\n📊 Archivos de datos encontrados: {len(archivos_excel)} archivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📁 Función de Carga y Consolidación de Datos\n",
    "\n",
    "### 🔧 Estrategia Técnica\n",
    "Desarrollaremos una función robusta que:\n",
    "- **Detecta automáticamente** la estructura de cada archivo Excel\n",
    "- **Identifica el encabezado** buscando palabras clave como 'COMPRESOR' y 'MOTOR'\n",
    "- **Maneja diferentes engines** (.xls con xlrd, .xlsx con openpyxl)\n",
    "- **Consolida datos** de múltiples archivos manteniendo consistencia\n",
    "\n",
    "Esta aproximación es porque los archivos industriales pueden tener estructuras variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Ejecutando carga y consolidación de datos...\n",
      "🔄 Iniciando consolidación de 28 archivos...\n",
      "\n",
      "📋 01-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 01-2025.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 02-2024..xls: 697 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 02-2025.xls: 673 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 03-2024..xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 03-2025.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 04-2024..xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 04-2025.xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 05-2024..xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 06-2024..xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 07-2024..xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 08-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 09-2024.xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 1-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 10-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 10-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 11-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 11-2024.xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 12-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 12-2024.xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 2-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 3-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 4-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 5-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 6-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 7-2023...xls: 744 filas, 34 columnas (encabezado en fila 3)\n",
      "📋 8-2023...xls: 745 filas, 34 columnas (encabezado en fila 2)\n",
      "📋 9-2023...xls: 721 filas, 34 columnas (encabezado en fila 2)\n",
      "\n",
      "🔗 Consolidando 28 DataFrames...\n",
      "\n",
      "📊 Resumen de Consolidación:\n",
      "   ✅ Archivos exitosos: 28\n",
      "   ❌ Archivos fallidos: 0\n",
      "   📈 Dataset final: 20,523 filas, 68 columnas\n"
     ]
    }
   ],
   "source": [
    "def detectar_fila_encabezado(df_raw):\n",
    "    \"\"\"\n",
    "    Detecta automáticamente la fila donde comienzan los encabezados de columnas\n",
    "    buscando palabras clave relacionadas con el moto-compresor\n",
    "    \"\"\"\n",
    "    palabras_clave = ['COMPRESOR', 'MOTOR', 'HORA', 'TIEMPO', 'TEMP', 'PRES']\n",
    "    \n",
    "    for idx, fila in df_raw.iterrows():\n",
    "        # Convertir toda la fila a string y buscar palabras clave\n",
    "        fila_str = ' '.join([str(cell).upper() for cell in fila if pd.notna(cell)])\n",
    "        \n",
    "        # Si encontramos al menos 2 palabras clave, probablemente es el encabezado\n",
    "        coincidencias = sum(1 for palabra in palabras_clave if palabra in fila_str)\n",
    "        if coincidencias >= 2:\n",
    "            return idx\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cargar_archivo_sensor(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo de sensores con detección automática de estructura\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar archivo completo para análisis de estructura\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df_raw = pd.read_excel(file_path, header=None, engine='openpyxl')\n",
    "        else:\n",
    "            df_raw = pd.read_excel(file_path, header=None, engine='xlrd')\n",
    "        \n",
    "        # Detectar fila de encabezado\n",
    "        header_row = detectar_fila_encabezado(df_raw)\n",
    "        \n",
    "        if header_row is None:\n",
    "            print(f\"⚠️  No se detectó encabezado automáticamente en {file_path.name}, usando fila 0\")\n",
    "            header_row = 0\n",
    "        \n",
    "        # Cargar datos con el encabezado correcto\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df = pd.read_excel(file_path, \n",
    "                             header=header_row,\n",
    "                             skiprows=range(0, header_row) if header_row > 0 else None,\n",
    "                             engine='openpyxl')\n",
    "        else:\n",
    "            df = pd.read_excel(file_path, \n",
    "                             header=header_row,\n",
    "                             skiprows=range(0, header_row) if header_row > 0 else None,\n",
    "                             engine='xlrd')\n",
    "        \n",
    "        # Información básica del archivo\n",
    "        print(f\"📋 {file_path.name}: {df.shape[0]} filas, {df.shape[1]} columnas (encabezado en fila {header_row})\")\n",
    "        \n",
    "        return df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando {file_path.name}: {str(e)}\")\n",
    "        return None, False\n",
    "\n",
    "def cargar_y_consolidar_datos(ruta_raw):\n",
    "    \"\"\"\n",
    "    Carga y consolida todos los archivos de sensores de la ruta especificada\n",
    "    \"\"\"\n",
    "    archivos_excel = list(ruta_raw.glob('*.xls*'))\n",
    "    \n",
    "    if not archivos_excel:\n",
    "        raise FileNotFoundError(f\"No se encontraron archivos Excel en {ruta_raw}\")\n",
    "    \n",
    "    print(f\"🔄 Iniciando consolidación de {len(archivos_excel)} archivos...\\n\")\n",
    "    \n",
    "    dataframes_lista = []\n",
    "    archivos_exitosos = []\n",
    "    archivos_fallidos = []\n",
    "    \n",
    "    for archivo in sorted(archivos_excel):  # Ordenar para procesamiento consistente\n",
    "        df, exito = cargar_archivo_sensor(archivo)\n",
    "        \n",
    "        if exito and df is not None and not df.empty:\n",
    "            dataframes_lista.append(df)\n",
    "            archivos_exitosos.append(archivo.name)\n",
    "        else:\n",
    "            archivos_fallidos.append(archivo.name)\n",
    "    \n",
    "    if not dataframes_lista:\n",
    "        raise ValueError(\"No se pudo cargar ningún archivo exitosamente\")\n",
    "    \n",
    "    # Consolidar todos los DataFrames\n",
    "    print(f\"\\n🔗 Consolidando {len(dataframes_lista)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(dataframes_lista, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Reporte de consolidación\n",
    "    print(f\"\\n📊 Resumen de Consolidación:\")\n",
    "    print(f\"   ✅ Archivos exitosos: {len(archivos_exitosos)}\")\n",
    "    print(f\"   ❌ Archivos fallidos: {len(archivos_fallidos)}\")\n",
    "    print(f\"   📈 Dataset final: {df_consolidado.shape[0]:,} filas, {df_consolidado.shape[1]} columnas\")\n",
    "    \n",
    "    if archivos_fallidos:\n",
    "        print(f\"\\n⚠️  Archivos que fallaron: {', '.join(archivos_fallidos)}\")\n",
    "    \n",
    "    return df_consolidado, archivos_exitosos, archivos_fallidos\n",
    "\n",
    "# Ejecutar consolidación\n",
    "print(\"🚀 Ejecutando carga y consolidación de datos...\")\n",
    "df_raw, exitosos, fallidos = cargar_y_consolidar_datos(ruta_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🧹 Pipeline de Limpieza de Datos\n",
    "\n",
    "### 📝 Justificación de la Estrategia\n",
    "Los datos industriales presentan desafíos típicos:\n",
    "- **Nombres inconsistentes**: Espacios, saltos de línea, caracteres especiales\n",
    "- **Formatos temporales variables**: Números de Excel vs. strings de fecha\n",
    "- **Tipos de datos mixtos**: Strings donde deberían ser numéricos\n",
    "\n",
    "Implementaremos un pipeline robusto que maneja estos problemas sistemáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Limpiando nombres de columnas...\n",
      "\n",
      "📋 Muestra de nombres originales:\n",
      "   1. 'ESTADO'\n",
      "   2. 'Hora'\n",
      "   3. 'RPM'\n",
      "   4. 'Presión\n",
      "Succión'\n",
      "   5. 'Presión\n",
      "Intermedia'\n",
      "\n",
      "📋 Muestra de nombres limpios:\n",
      "   1. 'estado'\n",
      "   2. 'hora'\n",
      "   3. 'rpm'\n",
      "   4. 'presion_succion'\n",
      "   5. 'presion_intermedia'\n",
      "\n",
      "✅ Limpieza de nombres completada: 68 columnas procesadas\n"
     ]
    }
   ],
   "source": [
    "def limpiar_nombres_columnas(df):\n",
    "    \"\"\"\n",
    "    Limpia y estandariza los nombres de columnas a formato snake_case\n",
    "    \"\"\"\n",
    "    print(\"🔤 Limpiando nombres de columnas...\")\n",
    "    \n",
    "    # Mostrar algunos nombres originales para referencia\n",
    "    print(f\"\\n📋 Muestra de nombres originales:\")\n",
    "    for i, col in enumerate(df.columns[:5]):\n",
    "        print(f\"   {i+1}. '{col}'\")\n",
    "    \n",
    "    nombres_limpios = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Convertir a string si no lo es\n",
    "        nombre = str(col)\n",
    "        \n",
    "        # Convertir a minúsculas\n",
    "        nombre = nombre.lower()\n",
    "        \n",
    "        # Reemplazar vocales con acento por las vocales sin acento\n",
    "        nombre = nombre.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u')\n",
    "        \n",
    "        # Reemplazar saltos de línea y espacios múltiples por un espacio\n",
    "        nombre = re.sub(r'\\s+', ' ', nombre)\n",
    "        \n",
    "        # Reemplazar espacios por guiones bajos\n",
    "        nombre = nombre.replace(' ', '_')\n",
    "        \n",
    "        # Eliminar caracteres especiales, mantener solo letras, números y guiones bajos\n",
    "        nombre = re.sub(r'[^a-z0-9_]', '', nombre)\n",
    "        \n",
    "        # Eliminar guiones bajos múltiples\n",
    "        nombre = re.sub(r'_+', '_', nombre)\n",
    "        \n",
    "        # Eliminar guiones bajos al inicio y final\n",
    "        nombre = nombre.strip('_')\n",
    "        \n",
    "        # Si el nombre está vacío, generar uno genérico\n",
    "        if not nombre:\n",
    "            nombre = f'columna_{len(nombres_limpios)}'\n",
    "        \n",
    "        \n",
    "        nombres_limpios.append(nombre)\n",
    "    \n",
    "    # Manejar nombres duplicados\n",
    "    nombres_finales = []\n",
    "    contador_nombres = {}\n",
    "    \n",
    "    for nombre in nombres_limpios:\n",
    "        if nombre in contador_nombres:\n",
    "            contador_nombres[nombre] += 1\n",
    "            nombre_final = f\"{nombre}_{contador_nombres[nombre]}\"\n",
    "        else:\n",
    "            contador_nombres[nombre] = 0\n",
    "            nombre_final = nombre\n",
    "        \n",
    "        nombres_finales.append(nombre_final)\n",
    "    \n",
    "    # Aplicar nombres limpios\n",
    "    df.columns = nombres_finales\n",
    "    \n",
    "    print(f\"\\n📋 Muestra de nombres limpios:\")\n",
    "    for i, col in enumerate(df.columns[:5]):\n",
    "        print(f\"   {i+1}. '{col}'\")\n",
    "    \n",
    "    print(f\"\\n✅ Limpieza de nombres completada: {len(nombres_finales)} columnas procesadas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejecutar limpieza de nombres\n",
    "df_clean = limpiar_nombres_columnas(df_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Convirtiendo tipos de datos...\n",
      "⏰ Columna de tiempo detectada: 'hora'\n",
      "   ✅ Columna ya en formato datetime\n",
      "   📊 Conversión temporal: 19,752/20,523 valores válidos (96.2%)\n",
      "   📅 Rango temporal: 2023-01-01 00:00:00 a 2025-04-30 23:00:00\n",
      "   ✅ Columna 'hora' establecida como índice\n",
      "\n",
      "🔢 Convirtiendo columnas numéricas...\n",
      "   ⚠️  estado: conversión perdió muchos valores (0/19752)\n",
      "   ⚠️  unnamed_0: conversión perdió muchos valores (0/744)\n",
      "   ✅ Conversiones numéricas exitosas: 65/67 columnas\n"
     ]
    }
   ],
   "source": [
    "def detectar_columna_tiempo(df):\n",
    "    \"\"\"\n",
    "    Detecta la columna de tiempo basándose en nombres comunes y contenido\n",
    "    \"\"\"\n",
    "    nombres_tiempo = ['hora', 'tiempo', 'time', 'fecha', 'date', 'timestamp']\n",
    "    \n",
    "    # Buscar por nombre\n",
    "    for col in df.columns:\n",
    "        if any(nombre in col.lower() for nombre in nombres_tiempo):\n",
    "            return col\n",
    "    \n",
    "    # Si no encontramos por nombre, buscar la primera columna que parezca temporal\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Verificar si los valores pueden ser fechas\n",
    "            muestra = df[col].dropna().head(100)\n",
    "            if muestra.empty:\n",
    "                continue\n",
    "                \n",
    "            # Intentar conversión a datetime\n",
    "            pd.to_datetime(muestra, errors='raise')\n",
    "            return col\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def convertir_tiempo_excel_inteligente(serie_tiempo):\n",
    "    \"\"\"\n",
    "    Convierte una serie de tiempo manejando diferentes formatos:\n",
    "    - Números de Excel (días desde 1899-12-30)\n",
    "    - Strings de fecha/hora\n",
    "    - Timestamps ya convertidos\n",
    "    \"\"\"\n",
    "    if serie_tiempo.empty:\n",
    "        return serie_tiempo\n",
    "    \n",
    "    # Tomar una muestra para detectar formato\n",
    "    muestra = serie_tiempo.dropna().head(10)\n",
    "    \n",
    "    if muestra.empty:\n",
    "        return serie_tiempo\n",
    "    \n",
    "    primer_valor = muestra.iloc[0]\n",
    "    \n",
    "    try:\n",
    "        # Caso 1: Ya es datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(serie_tiempo):\n",
    "            print(\"   ✅ Columna ya en formato datetime\")\n",
    "            return serie_tiempo\n",
    "        \n",
    "        # Caso 2: Números de Excel (típicamente entre 40000-50000 para fechas 2009-2037)\n",
    "        if pd.api.types.is_numeric_dtype(serie_tiempo):\n",
    "            if isinstance(primer_valor, (int, float)) and 30000 < primer_valor < 60000:\n",
    "                print(f\"   🔢 Detectado formato numérico Excel (ej: {primer_valor})\")\n",
    "                return pd.to_datetime(serie_tiempo, unit='D', origin='1899-12-30', errors='coerce')\n",
    "        \n",
    "        # Caso 3: Strings de fecha/hora\n",
    "        if isinstance(primer_valor, str):\n",
    "            print(f\"   📝 Detectado formato string (ej: '{primer_valor}')\")\n",
    "            return pd.to_datetime(serie_tiempo, errors='coerce', infer_datetime_format=True)\n",
    "        \n",
    "        # Caso 4: Intentar conversión general\n",
    "        print(f\"   🔄 Intentando conversión general para tipo: {type(primer_valor)}\")\n",
    "        return pd.to_datetime(serie_tiempo, errors='coerce')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Error en conversión temporal: {str(e)}\")\n",
    "        return pd.to_datetime(serie_tiempo, errors='coerce')\n",
    "\n",
    "def convertir_tipos_datos(df):\n",
    "    \"\"\"\n",
    "    Convierte los tipos de datos apropiadamente:\n",
    "    - Columna de tiempo a datetime\n",
    "    - Resto de columnas a numérico (float64)\n",
    "    \"\"\"\n",
    "    print(\"🔄 Convirtiendo tipos de datos...\")\n",
    "    \n",
    "    # Detectar columna de tiempo\n",
    "    col_tiempo = detectar_columna_tiempo(df)\n",
    "    \n",
    "    if col_tiempo:\n",
    "        print(f\"⏰ Columna de tiempo detectada: '{col_tiempo}'\")\n",
    "        \n",
    "        # Convertir columna de tiempo\n",
    "        df[col_tiempo] = convertir_tiempo_excel_inteligente(df[col_tiempo])\n",
    "        \n",
    "        # Verificar conversión exitosa\n",
    "        valores_validos = df[col_tiempo].notna().sum()\n",
    "        total_valores = len(df[col_tiempo])\n",
    "        \n",
    "        print(f\"   📊 Conversión temporal: {valores_validos:,}/{total_valores:,} valores válidos ({valores_validos/total_valores*100:.1f}%)\")\n",
    "        \n",
    "        if valores_validos > 0:\n",
    "            print(f\"   📅 Rango temporal: {df[col_tiempo].min()} a {df[col_tiempo].max()}\")\n",
    "            \n",
    "            # Establecer como índice\n",
    "            df_indexed = df.set_index(col_tiempo)\n",
    "            print(f\"   ✅ Columna '{col_tiempo}' establecida como índice\")\n",
    "        else:\n",
    "            print(f\"   ❌ Conversión temporal falló, manteniendo índice numérico\")\n",
    "            df_indexed = df\n",
    "    else:\n",
    "        print(\"⚠️  No se detectó columna de tiempo, manteniendo índice numérico\")\n",
    "        df_indexed = df\n",
    "    \n",
    "    # Convertir columnas restantes a numérico\n",
    "    print(\"\\n🔢 Convirtiendo columnas numéricas...\")\n",
    "    \n",
    "    columnas_numericas = [col for col in df_indexed.columns if col != col_tiempo]\n",
    "    conversiones_exitosas = 0\n",
    "    \n",
    "    for col in columnas_numericas:\n",
    "        try:\n",
    "            # Intentar conversión a numérico\n",
    "            valores_originales = df_indexed[col].notna().sum()\n",
    "            df_indexed[col] = pd.to_numeric(df_indexed[col], errors='coerce')\n",
    "            valores_finales = df_indexed[col].notna().sum()\n",
    "            \n",
    "            if valores_finales >= valores_originales * 0.8:  # 80% de éxito mínimo\n",
    "                conversiones_exitosas += 1\n",
    "            else:\n",
    "                print(f\"   ⚠️  {col}: conversión perdió muchos valores ({valores_finales}/{valores_originales})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error convirtiendo {col}: {str(e)}\")\n",
    "    \n",
    "    print(f\"   ✅ Conversiones numéricas exitosas: {conversiones_exitosas}/{len(columnas_numericas)} columnas\")\n",
    "    \n",
    "    return df_indexed\n",
    "\n",
    "# Ejecutar conversión de tipos\n",
    "df_typed = convertir_tipos_datos(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            estado         rpm  presion_succion  presion_intermedia  presion_descarga  pres_aceite_comp  temp_cilindro_1  temp_cilindro_2  temp_cilindro_3  temp_cilindro_4  presion_aceite_motor  presion_agua  presion_mult_adm_izq  presion_mult_adm_der  presion_carter  temp_aceite_motor  temp_agua_motor  temp_mult_adm_izq  temp_mult_adm_der  temp_cil_1_l  temp_cil_1_r  temp_cil_2_l  temp_cil_2_r  temp_cil_3_l  temp_cil_3_r  temp_mult_esc_izq  temp_cil_4_l  temp_cil_4_r  temp_cil_5_l  temp_cil_5_r  temp_cil_6_l  temp_cil_6_r  temp_mult_esc_der  unnamed_0            unnamed_1  unnamed_2  unnamed_3  unnamed_4  unnamed_5  unnamed_6  unnamed_7  unnamed_8  unnamed_9  unnamed_10  unnamed_11  unnamed_12  unnamed_13  unnamed_14  unnamed_15  unnamed_16  unnamed_17  unnamed_18  unnamed_19  unnamed_20  unnamed_21  unnamed_22  unnamed_23  unnamed_24  unnamed_25  unnamed_26  unnamed_27  unnamed_28  unnamed_29  unnamed_30  unnamed_31  unnamed_32  unnamed_33\n",
      "hora                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "NaT            NaN         NaN              NaN                 NaN               NaN               NaN              NaN              NaN              NaN              NaN                   NaN           NaN                   NaN                   NaN             NaN                NaN              NaN                NaN                NaN           NaN           NaN           NaN           NaN           NaN           NaN                NaN           NaN           NaN           NaN           NaN           NaN           NaN                NaN        NaN -9223372036854775808        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\n",
      "2024-01-01     NaN  852.637329       251.783905          492.887909        481.991913         33.750099       173.210297       157.138702       158.029404       170.499405              46.11932      13.45822              0.679329              0.296916        0.791417         184.712204       155.938095           113.1427         114.187798    913.094727    666.346313    868.581726    692.105713    713.106018    333.250488          939.55542    738.036621    691.917419     991.60498    357.024689    778.001099    834.956421         910.073914        NaN -9223372036854775808        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20523 entries, NaT to 2023-09-30 23:00:00\n",
      "Data columns (total 67 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   estado                0 non-null      float64\n",
      " 1   rpm                   19678 non-null  float64\n",
      " 2   presion_succion       19679 non-null  float64\n",
      " 3   presion_intermedia    19679 non-null  float64\n",
      " 4   presion_descarga      19679 non-null  float64\n",
      " 5   pres_aceite_comp      19678 non-null  float64\n",
      " 6   temp_cilindro_1       19678 non-null  float64\n",
      " 7   temp_cilindro_2       19678 non-null  float64\n",
      " 8   temp_cilindro_3       19678 non-null  float64\n",
      " 9   temp_cilindro_4       19678 non-null  float64\n",
      " 10  presion_aceite_motor  19678 non-null  float64\n",
      " 11  presion_agua          19678 non-null  float64\n",
      " 12  presion_mult_adm_izq  19678 non-null  float64\n",
      " 13  presion_mult_adm_der  19678 non-null  float64\n",
      " 14  presion_carter        19678 non-null  float64\n",
      " 15  temp_aceite_motor     19678 non-null  float64\n",
      " 16  temp_agua_motor       19678 non-null  float64\n",
      " 17  temp_mult_adm_izq     19676 non-null  float64\n",
      " 18  temp_mult_adm_der     19676 non-null  float64\n",
      " 19  temp_cil_1_l          19678 non-null  float64\n",
      " 20  temp_cil_1_r          19677 non-null  float64\n",
      " 21  temp_cil_2_l          19678 non-null  float64\n",
      " 22  temp_cil_2_r          19676 non-null  float64\n",
      " 23  temp_cil_3_l          19677 non-null  float64\n",
      " 24  temp_cil_3_r          19677 non-null  float64\n",
      " 25  temp_mult_esc_izq     19675 non-null  float64\n",
      " 26  temp_cil_4_l          19677 non-null  float64\n",
      " 27  temp_cil_4_r          19677 non-null  float64\n",
      " 28  temp_cil_5_l          19676 non-null  float64\n",
      " 29  temp_cil_5_r          19677 non-null  float64\n",
      " 30  temp_cil_6_l          19676 non-null  float64\n",
      " 31  temp_cil_6_r          19676 non-null  float64\n",
      " 32  temp_mult_esc_der     19676 non-null  float64\n",
      " 33  unnamed_0             0 non-null      float64\n",
      " 34  unnamed_1             20523 non-null  int64  \n",
      " 35  unnamed_2             743 non-null    float64\n",
      " 36  unnamed_3             743 non-null    float64\n",
      " 37  unnamed_4             743 non-null    float64\n",
      " 38  unnamed_5             743 non-null    float64\n",
      " 39  unnamed_6             743 non-null    float64\n",
      " 40  unnamed_7             743 non-null    float64\n",
      " 41  unnamed_8             743 non-null    float64\n",
      " 42  unnamed_9             743 non-null    float64\n",
      " 43  unnamed_10            743 non-null    float64\n",
      " 44  unnamed_11            743 non-null    float64\n",
      " 45  unnamed_12            743 non-null    float64\n",
      " 46  unnamed_13            743 non-null    float64\n",
      " 47  unnamed_14            743 non-null    float64\n",
      " 48  unnamed_15            743 non-null    float64\n",
      " 49  unnamed_16            743 non-null    float64\n",
      " 50  unnamed_17            743 non-null    float64\n",
      " 51  unnamed_18            743 non-null    float64\n",
      " 52  unnamed_19            743 non-null    float64\n",
      " 53  unnamed_20            743 non-null    float64\n",
      " 54  unnamed_21            743 non-null    float64\n",
      " 55  unnamed_22            743 non-null    float64\n",
      " 56  unnamed_23            743 non-null    float64\n",
      " 57  unnamed_24            743 non-null    float64\n",
      " 58  unnamed_25            743 non-null    float64\n",
      " 59  unnamed_26            743 non-null    float64\n",
      " 60  unnamed_27            743 non-null    float64\n",
      " 61  unnamed_28            743 non-null    float64\n",
      " 62  unnamed_29            743 non-null    float64\n",
      " 63  unnamed_30            743 non-null    float64\n",
      " 64  unnamed_31            743 non-null    float64\n",
      " 65  unnamed_32            743 non-null    float64\n",
      " 66  unnamed_33            743 non-null    float64\n",
      "dtypes: float64(66), int64(1)\n",
      "memory usage: 10.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Verificar columnas y datos actuales\n",
    "print(df_typed.head(2))\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🔧 Tratamiento de Valores Faltantes y Atípicos\n",
    "\n",
    "### ⚠️ Borrar registro de nulos y columnas no identificadas\n",
    "\n",
    "### 📊 Estrategia para Valores Faltantes\n",
    "Para series temporales industriales, la **interpolación basada en tiempo** es superior a otros métodos porque:\n",
    "- **Preserva tendencias temporales** naturales del proceso industrial\n",
    "- **Mantiene correlaciones** entre variables del moto-compresor\n",
    "- **Es más realista** que forward-fill o valores promedio estáticos\n",
    "\n",
    "### 📈 Estrategia para Outliers\n",
    "Aplicaremos **clipping estadístico global** (percentiles 1-99) porque:\n",
    "- **Preserva información** de eventos operacionales reales\n",
    "- **Elimina errores de medición** extremos sin perder datos útiles\n",
    "- **Mantiene consistencia** en rangos operacionales para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20523 entries, NaT to 2023-09-30 23:00:00\n",
      "Data columns (total 33 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   estado                0 non-null      float64\n",
      " 1   rpm                   19678 non-null  float64\n",
      " 2   presion_succion       19679 non-null  float64\n",
      " 3   presion_intermedia    19679 non-null  float64\n",
      " 4   presion_descarga      19679 non-null  float64\n",
      " 5   pres_aceite_comp      19678 non-null  float64\n",
      " 6   temp_cilindro_1       19678 non-null  float64\n",
      " 7   temp_cilindro_2       19678 non-null  float64\n",
      " 8   temp_cilindro_3       19678 non-null  float64\n",
      " 9   temp_cilindro_4       19678 non-null  float64\n",
      " 10  presion_aceite_motor  19678 non-null  float64\n",
      " 11  presion_agua          19678 non-null  float64\n",
      " 12  presion_mult_adm_izq  19678 non-null  float64\n",
      " 13  presion_mult_adm_der  19678 non-null  float64\n",
      " 14  presion_carter        19678 non-null  float64\n",
      " 15  temp_aceite_motor     19678 non-null  float64\n",
      " 16  temp_agua_motor       19678 non-null  float64\n",
      " 17  temp_mult_adm_izq     19676 non-null  float64\n",
      " 18  temp_mult_adm_der     19676 non-null  float64\n",
      " 19  temp_cil_1_l          19678 non-null  float64\n",
      " 20  temp_cil_1_r          19677 non-null  float64\n",
      " 21  temp_cil_2_l          19678 non-null  float64\n",
      " 22  temp_cil_2_r          19676 non-null  float64\n",
      " 23  temp_cil_3_l          19677 non-null  float64\n",
      " 24  temp_cil_3_r          19677 non-null  float64\n",
      " 25  temp_mult_esc_izq     19675 non-null  float64\n",
      " 26  temp_cil_4_l          19677 non-null  float64\n",
      " 27  temp_cil_4_r          19677 non-null  float64\n",
      " 28  temp_cil_5_l          19676 non-null  float64\n",
      " 29  temp_cil_5_r          19677 non-null  float64\n",
      " 30  temp_cil_6_l          19676 non-null  float64\n",
      " 31  temp_cil_6_r          19676 non-null  float64\n",
      " 32  temp_mult_esc_der     19676 non-null  float64\n",
      "dtypes: float64(33)\n",
      "memory usage: 5.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas con todos los valores vacíos o nulos\n",
    "df_typed.dropna(inplace=True, how='all')\n",
    "\n",
    "# Eliminar columnas que su nombre empiece con 'unnamed'\n",
    "unnamed_cols = [col for col in df_typed.columns if col.startswith('unnamed')]\n",
    "df_typed.drop(columns=unnamed_cols, inplace=True)\n",
    "\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analizando valores faltantes...\n",
      "\n",
      "📊 Resumen General:\n",
      "   Total de valores: 677,259\n",
      "   Valores faltantes: 47,583 (7.03%)\n",
      "   Columnas con valores faltantes: 33\n",
      "\n",
      "📋 Top 10 columnas con más valores faltantes:\n",
      "   estado: 20,523 (100.0%)\n",
      "   temp_mult_esc_izq: 848 (4.13%)\n",
      "   temp_cil_5_l: 847 (4.13%)\n",
      "   temp_cil_6_r: 847 (4.13%)\n",
      "   temp_cil_6_l: 847 (4.13%)\n",
      "   temp_mult_esc_der: 847 (4.13%)\n",
      "   temp_cil_2_r: 847 (4.13%)\n",
      "   temp_mult_adm_der: 847 (4.13%)\n",
      "   temp_mult_adm_izq: 847 (4.13%)\n",
      "   rpm: 845 (4.12%)\n",
      "\n",
      "🔧 Aplicando tratamiento de valores faltantes...\n",
      "   📅 Aplicando interpolación temporal (method='time')...\n",
      "   ⚠️  Advertencia: Índice contiene valores NaN, usando interpolación lineal\n",
      "   📊 Resultados:\n",
      "      Valores faltantes antes: 47,583\n",
      "      Valores faltantes después: 20,523\n",
      "      Valores interpolados: 27,060\n",
      "      Tasa de éxito: 56.9%\n"
     ]
    }
   ],
   "source": [
    "def analizar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Analiza el patrón de valores faltantes en el dataset\n",
    "    \"\"\"\n",
    "    print(\"🔍 Analizando valores faltantes...\")\n",
    "    \n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Calcular estadísticas de valores faltantes por columna\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'columna': df.columns,\n",
    "        'valores_faltantes': df.isnull().sum(),\n",
    "        'porcentaje_faltante': (df.isnull().sum() / total_filas * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Ordenar por porcentaje de valores faltantes\n",
    "    missing_stats = missing_stats.sort_values('porcentaje_faltante', ascending=False)\n",
    "    \n",
    "    # Mostrar estadísticas generales\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    total_valores = df.size\n",
    "    porcentaje_total = (total_missing / total_valores * 100)\n",
    "    \n",
    "    print(f\"\\n📊 Resumen General:\")\n",
    "    print(f\"   Total de valores: {total_valores:,}\")\n",
    "    print(f\"   Valores faltantes: {total_missing:,} ({porcentaje_total:.2f}%)\")\n",
    "    print(f\"   Columnas con valores faltantes: {(missing_stats['valores_faltantes'] > 0).sum()}\")\n",
    "    \n",
    "    # Mostrar columnas con más valores faltantes\n",
    "    columnas_con_faltantes = missing_stats[missing_stats['valores_faltantes'] > 0]\n",
    "    \n",
    "    if not columnas_con_faltantes.empty:\n",
    "        print(f\"\\n📋 Top 10 columnas con más valores faltantes:\")\n",
    "        for _, row in columnas_con_faltantes.head(10).iterrows():\n",
    "            print(f\"   {row['columna']}: {row['valores_faltantes']:,} ({row['porcentaje_faltante']}%)\")\n",
    "    else:\n",
    "        print(\"\\n✅ ¡No hay valores faltantes en el dataset!\")\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "def tratar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Trata los valores faltantes usando interpolación temporal\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 Aplicando tratamiento de valores faltantes...\")\n",
    "    \n",
    "    # Estadísticas antes del tratamiento\n",
    "    missing_antes = df.isnull().sum().sum()\n",
    "    \n",
    "    if missing_antes == 0:\n",
    "        print(\"✅ No hay valores faltantes que tratar\")\n",
    "        return df, 0, 0\n",
    "    \n",
    "    try:\n",
    "        # Crear una copia del DataFrame para evitar modificar el original\n",
    "        df_filled = df.copy()\n",
    "        \n",
    "        # Aplicar interpolación temporal si tenemos índice de tiempo\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            print(\"   📅 Aplicando interpolación temporal (method='time')...\")\n",
    "            # Verificar si hay NaNs en el índice que podrían causar problemas\n",
    "            if df.index.isna().any():\n",
    "                print(\"   ⚠️  Advertencia: Índice contiene valores NaN, usando interpolación lineal\")\n",
    "                df_filled = df_filled.interpolate(method='linear', limit_direction='both')\n",
    "            else:\n",
    "                df_filled = df_filled.interpolate(method='time', limit_direction='both')\n",
    "        else:\n",
    "            print(\"   📈 Aplicando interpolación lineal (no hay índice temporal)...\")\n",
    "            df_filled = df_filled.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        # Para valores que siguen faltando al inicio/final, usar backward/forward fill\n",
    "        # Reemplazar métodos deprecated\n",
    "        df_filled = df_filled.bfill().ffill()\n",
    "        \n",
    "        # Estadísticas después del tratamiento\n",
    "        missing_despues = df_filled.isnull().sum().sum()\n",
    "        valores_interpolados = missing_antes - missing_despues\n",
    "        \n",
    "        print(f\"   📊 Resultados:\")\n",
    "        print(f\"      Valores faltantes antes: {missing_antes:,}\")\n",
    "        print(f\"      Valores faltantes después: {missing_despues:,}\")\n",
    "        print(f\"      Valores interpolados: {valores_interpolados:,}\")\n",
    "        \n",
    "        if missing_antes > 0:\n",
    "            print(f\"      Tasa de éxito: {(valores_interpolados/missing_antes*100):.1f}%\")\n",
    "        \n",
    "        return df_filled, missing_antes, valores_interpolados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error durante la interpolación: {str(e)}\")\n",
    "        print(\"   🔄 Intentando método alternativo con forward/backward fill únicamente...\")\n",
    "        \n",
    "        try:\n",
    "            # Método alternativo: solo usar forward/backward fill\n",
    "            df_filled = df.copy()\n",
    "            df_filled = df_filled.bfill().ffill()\n",
    "            \n",
    "            missing_despues = df_filled.isnull().sum().sum()\n",
    "            valores_interpolados = missing_antes - missing_despues\n",
    "            \n",
    "            print(f\"   📊 Resultados (método alternativo):\")\n",
    "            print(f\"      Valores faltantes antes: {missing_antes:,}\")\n",
    "            print(f\"      Valores faltantes después: {missing_despues:,}\")\n",
    "            print(f\"      Valores rellenados: {valores_interpolados:,}\")\n",
    "            \n",
    "            if missing_antes > 0:\n",
    "                print(f\"      Tasa de éxito: {(valores_interpolados/missing_antes*100):.1f}%\")\n",
    "            \n",
    "            return df_filled, missing_antes, valores_interpolados\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ Error también en método alternativo: {str(e2)}\")\n",
    "            print(\"   ⚠️  Retornando DataFrame original sin cambios\")\n",
    "            return df, missing_antes, 0\n",
    "\n",
    "\n",
    "# Ejecutar análisis y tratamiento de valores faltantes\n",
    "missing_stats = analizar_valores_faltantes(df_typed)\n",
    "df_no_missing, missing_original, interpolated = tratar_valores_faltantes(df_typed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analizando outliers en columnas numéricas...\n",
      "\n",
      "📊 Resumen de Outliers:\n",
      "   Columnas analizadas: 33\n",
      "   Total outliers (percentil 1-99): 11,958 (1.82%)\n",
      "\n",
      "📋 Top 10 columnas con más outliers (método percentil):\n",
      "   rpm: 412 outliers (2.0%)\n",
      "   presion_intermedia: 412 outliers (2.0%)\n",
      "   presion_descarga: 412 outliers (2.0%)\n",
      "   pres_aceite_comp: 412 outliers (2.0%)\n",
      "   presion_agua: 412 outliers (2.0%)\n",
      "   presion_mult_adm_izq: 412 outliers (2.0%)\n",
      "   presion_aceite_motor: 412 outliers (2.0%)\n",
      "   temp_mult_adm_der: 412 outliers (2.0%)\n",
      "   temp_cil_4_l: 412 outliers (2.0%)\n",
      "   temp_cil_6_r: 412 outliers (2.0%)\n",
      "\n",
      "🔧 Aplicando clipping de outliers...\n",
      "   📊 Procesando 33 columnas numéricas...\n",
      "      rpm: 412 valores clipped (2.0%) [-106.41, 872.91]\n",
      "      presion_succion: 411 valores clipped (2.0%) [-74.85, 486.60]\n",
      "      presion_intermedia: 412 valores clipped (2.0%) [-117.62, 496.31]\n",
      "      presion_descarga: 412 valores clipped (2.0%) [-124.46, 483.54]\n",
      "      pres_aceite_comp: 412 valores clipped (2.0%) [-24.55, 38.71]\n",
      "      temp_cilindro_1: 206 valores clipped (1.0%) [72.37, 2191.88]\n",
      "      temp_cilindro_2: 206 valores clipped (1.0%) [75.23, 2191.88]\n",
      "      temp_cilindro_3: 205 valores clipped (1.0%) [75.58, 2191.88]\n",
      "      temp_cilindro_4: 206 valores clipped (1.0%) [73.20, 2191.88]\n",
      "      presion_aceite_motor: 412 valores clipped (2.0%) [-24.23, 47.97]\n",
      "      presion_agua: 412 valores clipped (2.0%) [0.62, 18.38]\n",
      "      presion_mult_adm_izq: 412 valores clipped (2.0%) [-26.08, 1.97]\n",
      "      presion_mult_adm_der: 411 valores clipped (2.0%) [-25.95, 2.36]\n",
      "      presion_carter: 206 valores clipped (1.0%) [0.00, 1.18]\n",
      "      temp_aceite_motor: 411 valores clipped (2.0%) [65.09, 1125.83]\n",
      "      temp_agua_motor: 411 valores clipped (2.0%) [82.16, 1392.98]\n",
      "      temp_mult_adm_izq: 238 valores clipped (1.2%) [-99.74, 128.35]\n",
      "      temp_mult_adm_der: 412 valores clipped (2.0%) [-4.71, 127.52]\n",
      "      temp_cil_1_l: 412 valores clipped (2.0%) [77.46, 1281.55]\n",
      "      temp_cil_1_r: 409 valores clipped (2.0%) [79.11, 1281.33]\n",
      "      temp_cil_2_l: 411 valores clipped (2.0%) [79.97, 1281.74]\n",
      "      temp_cil_2_r: 409 valores clipped (2.0%) [79.38, 1281.35]\n",
      "      temp_cil_3_l: 408 valores clipped (2.0%) [80.11, 1281.74]\n",
      "      temp_cil_3_r: 412 valores clipped (2.0%) [78.29, 1280.80]\n",
      "      temp_mult_esc_izq: 411 valores clipped (2.0%) [69.32, 1279.89]\n",
      "      temp_cil_4_l: 412 valores clipped (2.0%) [80.25, 1281.74]\n",
      "      temp_cil_4_r: 410 valores clipped (2.0%) [79.70, 1281.42]\n",
      "      temp_cil_5_l: 409 valores clipped (2.0%) [80.06, 1281.65]\n",
      "      temp_cil_5_r: 412 valores clipped (2.0%) [79.00, 1381.84]\n",
      "      temp_cil_6_l: 412 valores clipped (2.0%) [79.62, 1281.33]\n",
      "      temp_cil_6_r: 412 valores clipped (2.0%) [77.90, 849.28]\n",
      "      temp_mult_esc_der: 412 valores clipped (2.0%) [68.56, 968.63]\n",
      "\n",
      "   ✅ Clipping completado: 11,958 valores modificados en total\n"
     ]
    }
   ],
   "source": [
    "def analizar_outliers(df):\n",
    "    \"\"\"\n",
    "    Analiza outliers en las columnas numéricas del dataset\n",
    "    \"\"\"\n",
    "    print(\"🔍 Analizando outliers en columnas numéricas...\")\n",
    "    \n",
    "    # Seleccionar solo columnas numéricas\n",
    "    cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(cols_numericas) == 0:\n",
    "        print(\"⚠️  No se encontraron columnas numéricas\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    outlier_stats = []\n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        serie = df[col].dropna()\n",
    "        \n",
    "        if len(serie) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular percentiles\n",
    "        q01 = serie.quantile(0.01)\n",
    "        q99 = serie.quantile(0.99)\n",
    "        q25 = serie.quantile(0.25)\n",
    "        q75 = serie.quantile(0.75)\n",
    "        \n",
    "        # Contar outliers usando método de percentiles\n",
    "        outliers_bajos = (serie < q01).sum()\n",
    "        outliers_altos = (serie > q99).sum()\n",
    "        total_outliers = outliers_bajos + outliers_altos\n",
    "        \n",
    "        # Contar outliers usando método IQR (para comparación)\n",
    "        iqr = q75 - q25\n",
    "        lower_iqr = q25 - 1.5 * iqr\n",
    "        upper_iqr = q75 + 1.5 * iqr\n",
    "        outliers_iqr = ((serie < lower_iqr) | (serie > upper_iqr)).sum()\n",
    "        \n",
    "        outlier_stats.append({\n",
    "            'columna': col,\n",
    "            'total_valores': len(serie),\n",
    "            'outliers_percentil': total_outliers,\n",
    "            'outliers_iqr': outliers_iqr,\n",
    "            'porcentaje_percentil': (total_outliers / len(serie) * 100),\n",
    "            'porcentaje_iqr': (outliers_iqr / len(serie) * 100),\n",
    "            'q01': q01,\n",
    "            'q99': q99,\n",
    "            'min_original': serie.min(),\n",
    "            'max_original': serie.max()\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_stats)\n",
    "    outlier_df = outlier_df.sort_values('porcentaje_percentil', ascending=False)\n",
    "    \n",
    "    # Resumen general\n",
    "    total_outliers_percentil = outlier_df['outliers_percentil'].sum()\n",
    "    total_valores = outlier_df['total_valores'].sum()\n",
    "    \n",
    "    print(f\"\\n📊 Resumen de Outliers:\")\n",
    "    print(f\"   Columnas analizadas: {len(cols_numericas)}\")\n",
    "    print(f\"   Total outliers (percentil 1-99): {total_outliers_percentil:,} ({total_outliers_percentil/total_valores*100:.2f}%)\")\n",
    "    \n",
    "    # Mostrar top columnas con más outliers\n",
    "    print(f\"\\n📋 Top 10 columnas con más outliers (método percentil):\")\n",
    "    for _, row in outlier_df.head(10).iterrows():\n",
    "        print(f\"   {row['columna']}: {row['outliers_percentil']:,} outliers ({row['porcentaje_percentil']:.1f}%)\")\n",
    "    \n",
    "    return outlier_df\n",
    "\n",
    "def aplicar_clipping_outliers(df):\n",
    "    \"\"\"\n",
    "    Aplica clipping de outliers usando percentiles 1 y 99\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 Aplicando clipping de outliers...\")\n",
    "    \n",
    "    # Seleccionar columnas numéricas\n",
    "    cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(cols_numericas) == 0:\n",
    "        print(\"⚠️  No se encontraron columnas numéricas para procesar\")\n",
    "        return df, 0\n",
    "    \n",
    "    df_clipped = df.copy()\n",
    "    total_clipped = 0\n",
    "    \n",
    "    print(f\"   📊 Procesando {len(cols_numericas)} columnas numéricas...\")\n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        serie_original = df_clipped[col]\n",
    "        serie_valida = serie_original.dropna()\n",
    "        \n",
    "        if len(serie_valida) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular límites de clipping\n",
    "        limite_inferior = serie_valida.quantile(0.01)\n",
    "        limite_superior = serie_valida.quantile(0.99)\n",
    "        \n",
    "        # Aplicar clipping\n",
    "        serie_clipped = serie_original.clip(lower=limite_inferior, upper=limite_superior)\n",
    "        \n",
    "        # Contar valores modificados\n",
    "        valores_modificados = (serie_original != serie_clipped).sum()\n",
    "        total_clipped += valores_modificados\n",
    "        \n",
    "        # Actualizar la columna\n",
    "        df_clipped[col] = serie_clipped\n",
    "        \n",
    "        if valores_modificados > 0:\n",
    "            porcentaje = (valores_modificados / len(serie_original) * 100)\n",
    "            print(f\"      {col}: {valores_modificados:,} valores clipped ({porcentaje:.1f}%) [{limite_inferior:.2f}, {limite_superior:.2f}]\")\n",
    "    \n",
    "    print(f\"\\n   ✅ Clipping completado: {total_clipped:,} valores modificados en total\")\n",
    "    \n",
    "    return df_clipped, total_clipped\n",
    "\n",
    "# Ejecutar análisis y tratamiento de outliers\n",
    "outlier_stats = analizar_outliers(df_no_missing)\n",
    "df_final, total_clipped = aplicar_clipping_outliers(df_no_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📊 Validación de Calidad de Datos\n",
    "\n",
    "### 🎯 Métricas de Calidad\n",
    "Evaluaremos la calidad del preprocesamiento mediante métricas cuantitativas que demuestren la efectividad de cada paso del pipeline de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 REPORTE DE CALIDAD DEL PREPROCESAMIENTO\n",
      "============================================================\n",
      "\n",
      "1️⃣  DIMENSIONES DEL DATASET\n",
      "   Filas originales: 20,523\n",
      "   Filas finales: 20,523\n",
      "   Columnas originales: 68\n",
      "   Columnas finales: 33\n",
      "\n",
      "2️⃣  CALIDAD DE DATOS\n",
      "   Valores faltantes originales: 47,583\n",
      "   Valores interpolados: 27,060\n",
      "   Valores faltantes finales: 20,523\n",
      "   Tasa de completitud: 96.97%\n",
      "\n",
      "3️⃣  TRATAMIENTO DE OUTLIERS\n",
      "   Valores clipped: 11,958\n",
      "   Porcentaje de valores modificados: 1.77%\n",
      "\n",
      "4️⃣  TIPOS DE DATOS\n",
      "   float64: 33 columnas\n",
      "\n",
      "5️⃣  INFORMACIÓN TEMPORAL\n",
      "   Índice temporal: ✅ Configurado\n",
      "   Rango temporal: 2023-01-01 00:00:00 a 2025-04-30 23:00:00\n",
      "   Duración total: 850 days 23:00:00\n",
      "\n",
      "6️⃣  RESUMEN DE COLUMNAS\n",
      "   Columnas numéricas: 33\n",
      "   Columnas temporales: 0\n",
      "   Otras columnas: 0\n",
      "\n",
      "7️⃣  ESTADÍSTICAS DESCRIPTIVAS (Columnas Numéricas)\n",
      "   Promedio de medias: 252.59\n",
      "   Promedio de desv. estándar: 231.57\n",
      "   Rango de valores mínimos: [-124.46, 82.16]\n",
      "   Rango de valores máximos: [1.18, 2191.88]\n",
      "\n",
      "============================================================\n",
      "✅ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def generar_reporte_calidad(df_original, df_final, missing_original, interpolated, total_clipped):\n",
    "    \"\"\"\n",
    "    Genera un reporte completo de la calidad del preprocesamiento\n",
    "    \"\"\"\n",
    "    print(\"📋 REPORTE DE CALIDAD DEL PREPROCESAMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Estadísticas dimensionales\n",
    "    print(\"\\n1️⃣  DIMENSIONES DEL DATASET\")\n",
    "    print(f\"   Filas originales: {len(df_original):,}\")\n",
    "    print(f\"   Filas finales: {len(df_final):,}\")\n",
    "    print(f\"   Columnas originales: {df_original.shape[1]}\")\n",
    "    print(f\"   Columnas finales: {df_final.shape[1]}\")\n",
    "    \n",
    "    # 2. Calidad de datos\n",
    "    print(\"\\n2️⃣  CALIDAD DE DATOS\")\n",
    "    missing_final = df_final.isnull().sum().sum()\n",
    "    total_valores = df_final.size\n",
    "    \n",
    "    print(f\"   Valores faltantes originales: {missing_original:,}\")\n",
    "    print(f\"   Valores interpolados: {interpolated:,}\")\n",
    "    print(f\"   Valores faltantes finales: {missing_final:,}\")\n",
    "    print(f\"   Tasa de completitud: {((total_valores - missing_final) / total_valores * 100):.2f}%\")\n",
    "    \n",
    "    # 3. Tratamiento de outliers\n",
    "    print(\"\\n3️⃣  TRATAMIENTO DE OUTLIERS\")\n",
    "    print(f\"   Valores clipped: {total_clipped:,}\")\n",
    "    print(f\"   Porcentaje de valores modificados: {(total_clipped / total_valores * 100):.2f}%\")\n",
    "    \n",
    "    # 4. Tipos de datos\n",
    "    print(\"\\n4️⃣  TIPOS DE DATOS\")\n",
    "    tipos_finales = df_final.dtypes.value_counts()\n",
    "    for tipo, cantidad in tipos_finales.items():\n",
    "        print(f\"   {tipo}: {cantidad} columnas\")\n",
    "    \n",
    "    # 5. Información temporal\n",
    "    print(\"\\n5️⃣  INFORMACIÓN TEMPORAL\")\n",
    "    if isinstance(df_final.index, pd.DatetimeIndex):\n",
    "        print(f\"   Índice temporal: ✅ Configurado\")\n",
    "        print(f\"   Rango temporal: {df_final.index.min()} a {df_final.index.max()}\")\n",
    "        print(f\"   Duración total: {df_final.index.max() - df_final.index.min()}\")\n",
    "    else:\n",
    "        print(f\"   Índice temporal: ❌ No configurado (índice numérico)\")\n",
    "    \n",
    "    # 6. Resumen de columnas\n",
    "    print(\"\\n6️⃣  RESUMEN DE COLUMNAS\")\n",
    "    cols_numericas = df_final.select_dtypes(include=[np.number]).columns\n",
    "    cols_temporales = df_final.select_dtypes(include=['datetime']).columns\n",
    "    cols_otros = df_final.select_dtypes(exclude=[np.number, 'datetime']).columns\n",
    "    \n",
    "    print(f\"   Columnas numéricas: {len(cols_numericas)}\")\n",
    "    print(f\"   Columnas temporales: {len(cols_temporales)}\")\n",
    "    print(f\"   Otras columnas: {len(cols_otros)}\")\n",
    "    \n",
    "    # 7. Estadísticas descriptivas básicas\n",
    "    if len(cols_numericas) > 0:\n",
    "        print(\"\\n7️⃣  ESTADÍSTICAS DESCRIPTIVAS (Columnas Numéricas)\")\n",
    "        stats_desc = df_final[cols_numericas].describe()\n",
    "        print(f\"   Promedio de medias: {stats_desc.loc['mean'].mean():.2f}\")\n",
    "        print(f\"   Promedio de desv. estándar: {stats_desc.loc['std'].mean():.2f}\")\n",
    "        print(f\"   Rango de valores mínimos: [{stats_desc.loc['min'].min():.2f}, {stats_desc.loc['min'].max():.2f}]\")\n",
    "        print(f\"   Rango de valores máximos: [{stats_desc.loc['max'].min():.2f}, {stats_desc.loc['max'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generar reporte de calidad\n",
    "generar_reporte_calidad(df_raw, df_final, missing_original, interpolated, total_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 💾 Guardado del Dataset Procesado\n",
    "\n",
    "### 🎯 Objetivo Final\n",
    "Guardar el dataset completamente procesado en formato CSV para uso en las siguientes fases del proyecto:\n",
    "- **Feature Engineering** (03_feature_engineering.ipynb)\n",
    "- **Model Training** (04_model_training.ipynb)\n",
    "- **Model Evaluation** (05_model_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Guardando dataset final...\n",
      "   📊 Dimensiones: 20,523 filas × 33 columnas\n",
      "   📄 Guardando CSV...\n",
      "      ✅ CSV: 11.8 MB\n",
      "   📦 Guardando Parquet (subprocess aislado)...\n",
      "      ✅ Parquet: 3.2 MB\n",
      "   📄 Generando archivos de metadatos...\n",
      "      ✅ preproceso_metadata.txt\n",
      "      ✅ columnas_resumen.csv\n",
      "      ✅ estadistica_basica.csv\n",
      "\n",
      "📋 Resumen del guardado:\n",
      "   ✅ CSV: 11.8 MB\n",
      "   ✅ PARQUET: 3.2 MB\n",
      "\n",
      "📁 Archivos generados en data/processed/:\n",
      "   🗃️  timeseries_data.csv - Dataset principal\n",
      "   📦 timeseries_data.parquet - Dataset comprimido\n",
      "   📄 preproceso_metadata.txt - Metadatos del procesamiento\n",
      "   📊 columnas_resumen.csv - Resumen de columnas\n",
      "   📈 estadistica_basica.csv - Estadísticas descriptivas\n",
      "✅ Guardado completado: 2/2 formatos exitosos\n"
     ]
    }
   ],
   "source": [
    "def guardar_dataset_final(df, ruta_destino, nombre_base='timeseries_data',\n",
    "                         archivos_procesados=None, archivos_fallidos=None,\n",
    "                         valores_interpolados=0, valores_clipped=0):\n",
    "    \"\"\"\n",
    "    Guarda dataset en CSV (siempre) y Parquet (usando subprocess para evitar conflictos)\n",
    "    Incluye todos los archivos de metadatos\n",
    "    \"\"\"\n",
    "    ruta_destino = Path(ruta_destino)\n",
    "    ruta_destino.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"💾 Guardando dataset final...\")\n",
    "    print(f\"   📊 Dimensiones: {df.shape[0]:,} filas × {df.shape[1]} columnas\")\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # ========== CSV (SIEMPRE) ==========\n",
    "    try:\n",
    "        archivo_csv = ruta_destino / f\"{nombre_base}.csv\"\n",
    "        print(f\"   📄 Guardando CSV...\")\n",
    "        df.to_csv(archivo_csv, index=True, encoding='utf-8')\n",
    "        tamaño_mb = archivo_csv.stat().st_size / (1024 * 1024)\n",
    "        print(f\"      ✅ CSV: {tamaño_mb:.1f} MB\")\n",
    "        resultados['csv'] = {'exito': True, 'tamaño_mb': tamaño_mb}\n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ CSV Error: {str(e)}\")\n",
    "        return {'csv': {'exito': False, 'error': str(e)}}\n",
    "    \n",
    "    # ========== PARQUET (SUBPROCESS AISLADO) ==========\n",
    "    archivo_parquet = ruta_destino / f\"{nombre_base}.parquet\"\n",
    "    print(f\"   📦 Guardando Parquet (subprocess aislado)...\")\n",
    "    \n",
    "    try:\n",
    "        # CSV temporal\n",
    "        temp_csv = ruta_destino / 'temp_for_parquet.csv'\n",
    "        df.to_csv(temp_csv, index=True)\n",
    "        \n",
    "        # Script aislado\n",
    "        script_content = f'''\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"{temp_csv}\", index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Limpiar para Parquet\n",
    "    if hasattr(df.index, 'hasnans') and df.index.hasnans:\n",
    "        df = df[df.index.notna()]\n",
    "    if str(type(df.index)) == \"<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\":\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    df.to_parquet(\"{archivo_parquet}\", engine='pyarrow', index=False, compression='snappy')\n",
    "    print(\"PARQUET_SUCCESS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PARQUET_ERROR: {{e}}\")\n",
    "'''\n",
    "        \n",
    "        # Ejecutar subprocess\n",
    "        result = subprocess.run([sys.executable, '-c', script_content], \n",
    "                              capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        # Limpiar\n",
    "        if temp_csv.exists():\n",
    "            temp_csv.unlink()\n",
    "        \n",
    "        # Verificar\n",
    "        if result.returncode == 0 and \"PARQUET_SUCCESS\" in result.stdout:\n",
    "            tamaño_mb = archivo_parquet.stat().st_size / (1024 * 1024)\n",
    "            print(f\"      ✅ Parquet: {tamaño_mb:.1f} MB\")\n",
    "            resultados['parquet'] = {'exito': True, 'tamaño_mb': tamaño_mb}\n",
    "        else:\n",
    "            print(f\"      ❌ Parquet: Subprocess failed\")\n",
    "            resultados['parquet'] = {'exito': False, 'error': 'subprocess_failed'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ Parquet Exception: {str(e)}\")\n",
    "        resultados['parquet'] = {'exito': False, 'error': str(e)}\n",
    "    \n",
    "    # ========== METADATOS COMPLETOS ==========\n",
    "    try:\n",
    "        print(f\"   📄 Generando archivos de metadatos...\")\n",
    "        \n",
    "        # 1. Metadatos principales\n",
    "        archivo_meta = ruta_destino / 'preproceso_metadata.txt'\n",
    "        with open(archivo_meta, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"METADATOS DEL PREPROCESAMIENTO\\n\")\n",
    "            f.write(f\"=\" * 40 + \"\\n\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Dimensiones: {df.shape[0]:,} × {df.shape[1]}\\n\")\n",
    "            f.write(f\"Archivos procesados: {len(archivos_procesados or [])}\\n\")\n",
    "            f.write(f\"Archivos fallidos: {len(archivos_fallidos or [])}\\n\")\n",
    "            f.write(f\"Valores interpolados: {valores_interpolados:,}\\n\")\n",
    "            f.write(f\"Valores clipped: {valores_clipped:,}\\n\")\n",
    "            f.write(f\"Índice temporal: {'Sí' if hasattr(df.index, 'tz') or 'datetime' in str(type(df.index)).lower() else 'No'}\\n\")\n",
    "            f.write(f\"\\nFormatos guardados:\\n\")\n",
    "            for formato, resultado in resultados.items():\n",
    "                status = \"✅\" if resultado['exito'] else \"❌\" \n",
    "                f.write(f\"  {formato.upper()}: {status}\\n\")\n",
    "            \n",
    "            if archivos_procesados:\n",
    "                f.write(f\"\\nArchivos procesados exitosamente:\\n\")\n",
    "                for archivo in archivos_procesados:\n",
    "                    f.write(f\"  - {archivo}\\n\")\n",
    "            \n",
    "            if archivos_fallidos:\n",
    "                f.write(f\"\\nArchivos que fallaron:\\n\")\n",
    "                for archivo in archivos_fallidos:\n",
    "                    f.write(f\"  - {archivo}\\n\")\n",
    "        \n",
    "        # 2. Resumen de columnas\n",
    "        archivo_columnas = ruta_destino / 'columnas_resumen.csv'\n",
    "        columnas_info = pd.DataFrame({\n",
    "            'columna': df.columns,\n",
    "            'tipo_datos': [str(dtype) for dtype in df.dtypes],\n",
    "            'valores_no_nulos': df.count(),\n",
    "            'valores_nulos': df.isnull().sum(),\n",
    "            'porcentaje_completitud': ((df.count() / len(df)) * 100).round(2)\n",
    "        })\n",
    "        \n",
    "        # Agregar estadísticas para columnas numéricas\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        columnas_info['es_numerica'] = columnas_info['columna'].isin(numeric_cols)\n",
    "        \n",
    "        # Estadísticas básicas para columnas numéricas\n",
    "        columnas_info['min_valor'] = np.nan\n",
    "        columnas_info['max_valor'] = np.nan\n",
    "        columnas_info['media'] = np.nan\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if not df[col].empty and df[col].notna().any():\n",
    "                idx = columnas_info['columna'] == col\n",
    "                try:\n",
    "                    columnas_info.loc[idx, 'min_valor'] = df[col].min()\n",
    "                    columnas_info.loc[idx, 'max_valor'] = df[col].max()\n",
    "                    columnas_info.loc[idx, 'media'] = df[col].mean()\n",
    "                except:\n",
    "                    pass  # Ignorar errores en estadísticas\n",
    "        \n",
    "        columnas_info.to_csv(archivo_columnas, index=False, encoding='utf-8')\n",
    "        \n",
    "        # 3. Estadísticas descriptivas básicas (solo numéricas)\n",
    "        if len(numeric_cols) > 0:\n",
    "            archivo_stats = ruta_destino / 'estadistica_basica.csv'\n",
    "            stats_desc = df[numeric_cols].describe()\n",
    "            stats_desc.to_csv(archivo_stats, encoding='utf-8')\n",
    "        \n",
    "        print(f\"      ✅ preproceso_metadata.txt\")\n",
    "        print(f\"      ✅ columnas_resumen.csv\")\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"      ✅ estadistica_basica.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ⚠️  Error en metadatos: {str(e)}\")\n",
    "        pass\n",
    "    \n",
    "    # ========== RESUMEN FINAL ==========\n",
    "    print(f\"\\n📋 Resumen del guardado:\")\n",
    "    exitosos = sum(1 for r in resultados.values() if r['exito'])\n",
    "    for formato, resultado in resultados.items():\n",
    "        if resultado['exito']:\n",
    "            print(f\"   ✅ {formato.upper()}: {resultado['tamaño_mb']:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"   ❌ {formato.upper()}: Error\")\n",
    "    \n",
    "    print(f\"\\n📁 Archivos generados en data/processed/:\")\n",
    "    print(f\"   🗃️  {nombre_base}.csv - Dataset principal\")\n",
    "    if resultados.get('parquet', {}).get('exito'):\n",
    "        print(f\"   📦 {nombre_base}.parquet - Dataset comprimido\")\n",
    "    print(f\"   📄 preproceso_metadata.txt - Metadatos del procesamiento\")\n",
    "    print(f\"   📊 columnas_resumen.csv - Resumen de columnas\")\n",
    "    if len(df.select_dtypes(include=[np.number]).columns) > 0:\n",
    "        print(f\"   📈 estadistica_basica.csv - Estadísticas descriptivas\")\n",
    "    \n",
    "    print(f\"✅ Guardado completado: {exitosos}/{len(resultados)} formatos exitosos\")\n",
    "    \n",
    "    return resultados\n",
    "    \n",
    "exito_guardado = guardar_dataset_final(df_final, ruta_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Resumen del Preprocesamiento\n",
    "\n",
    "### ✅ Tareas Completadas\n",
    "\n",
    "1. **Carga y Consolidación Inteligente**\n",
    "   - ✅ Detección automática de estructura de archivos Excel\n",
    "   - ✅ Consolidación de 28 archivos en un dataset único\n",
    "   - ✅ Manejo robusto de diferentes formatos (.xls/.xlsx)\n",
    "\n",
    "2. **Limpieza de Estructura de Datos**\n",
    "   - ✅ Normalización de nombres de columnas a formato snake_case\n",
    "   - ✅ Conversión inteligente de tipos de datos\n",
    "   - ✅ Manejo de diferentes formatos temporales\n",
    "\n",
    "3. **Tratamiento de Calidad de Datos**\n",
    "   - ✅ Interpolación temporal de valores faltantes\n",
    "   - ✅ Clipping estadístico de outliers (percentiles 1-99)\n",
    "   - ✅ Preservación de información operacional relevante\n",
    "\n",
    "4. **Validación y Documentación**\n",
    "   - ✅ Métricas completas de calidad de datos\n",
    "   - ✅ Documentación exhaustiva del procesamiento\n",
    "   - ✅ Archivos de metadatos para trazabilidad\n",
    "\n",
    "### 🔄 Pipeline de Calidad Implementado\n",
    "\n",
    "```\n",
    "Datos Raw → Detección Automática → Limpieza → Conversión de Tipos → \n",
    "Interpolación → Clipping → Validación → Dataset Limpio\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**📊 Estado**: ✅ **Preprocesamiento Completado**  \n",
    "**➡️  Siguiente fase**: `03_feature_engineering.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
