{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de Características (Feature Engineering)\n",
    "## Proyecto de Mantenimiento Predictivo - Moto-Compresores\n",
    "\n",
    "Este notebook contiene la creación y selección de características para el modelo predictivo.\n",
    "\n",
    "### Objetivos:\n",
    "- Crear características derivadas\n",
    "- Generar indicadores de tendencia\n",
    "- Seleccionar características relevantes\n",
    "- Crear variables de degradación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.signal import detrend\nfrom scipy.fft import fft, fftfreq\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nimport warnings\nimport gc\n\n# Configuración\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\nprint(\"✅ Librerías importadas correctamente\")\nprint(f\"📅 Notebook ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Configurar rutas\ndata_processed_path = Path(\"data/processed\")\neventos_path = Path(\"eventos\")\noutput_path = Path(\"data/processed\")\n\n# Verificar archivos disponibles\nprint(\"🔍 Buscando archivos de datos preprocesados...\")\n\nparquet_files = list(data_processed_path.glob(\"*.parquet\"))\ncsv_files = list(data_processed_path.glob(\"*.csv\"))\n\nprint(f\"📦 Archivos Parquet encontrados: {len(parquet_files)}\")\nfor file in parquet_files:\n    print(f\"   • {file.name}\")\n    \nprint(f\"📄 Archivos CSV encontrados: {len(csv_files)}\")\nfor file in csv_files:\n    print(f\"   • {file.name}\")\n\n# Cargar el dataset principal\ntry:\n    if parquet_files and 'timeseries_data.parquet' in [f.name for f in parquet_files]:\n        print(\"\\n💾 Cargando timeseries_data.parquet...\")\n        df = pd.read_parquet(data_processed_path / \"timeseries_data.parquet\")\n        print(\"✅ Parquet cargado exitosamente\")\n    elif csv_files and 'clean_timeseries_data.csv' in [f.name for f in csv_files]:\n        print(\"\\n💾 Cargando clean_timeseries_data.csv...\")\n        df = pd.read_csv(data_processed_path / \"clean_timeseries_data.csv\", \n                        index_col=0, parse_dates=True)\n        print(\"✅ CSV cargado exitosamente\")\n    else:\n        print(\"❌ No se encontró archivo de datos procesados\")\n        print(\"   Buscando archivo más reciente...\")\n        all_files = parquet_files + csv_files\n        if all_files:\n            latest_file = max(all_files, key=lambda x: x.stat().st_mtime)\n            print(f\"   Cargando: {latest_file.name}\")\n            if latest_file.suffix == '.parquet':\n                df = pd.read_parquet(latest_file)\n            else:\n                df = pd.read_csv(latest_file, index_col=0, parse_dates=True)\n            print(\"✅ Archivo cargado exitosamente\")\n        else:\n            raise FileNotFoundError(\"No se encontraron archivos de datos\")\n            \nexcept Exception as e:\n    print(f\"❌ Error al cargar datos: {str(e)}\")\n    raise\n\n# Información del dataset cargado\nprint(f\"\\n📊 Dataset cargado:\")\nprint(f\"   📐 Dimensiones: {df.shape[0]:,} filas × {df.shape[1]} columnas\")\nprint(f\"   📅 Período: {df.index.min()} → {df.index.max()}\")\nprint(f\"   ⏱️  Frecuencia: {pd.infer_freq(df.index) if hasattr(df.index, 'freq') else 'No detectada'}\")\nprint(f\"   💾 Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n\n# Preview de los datos\nprint(\"\\n👀 Primeras 3 filas:\")\nprint(df.head(3))\n\nprint(\"\\n📋 Tipos de datos:\")\nprint(df.dtypes.value_counts())",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Selección Inteligente de Variables\n\nAlgoritmo automático para seleccionar las variables más relevantes para el mantenimiento predictivo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def seleccionar_variables_relevantes(df, min_completitud=0.7, min_variabilidad=0.01, \n                                   max_variables=50):\n    \"\"\"\n    Selecciona automáticamente las variables más relevantes para mantenimiento predictivo\n    \"\"\"\n    print(\"🎯 Ejecutando selección inteligente de variables...\")\n    \n    # Solo variables numéricas\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    print(f\"   📊 Variables numéricas disponibles: {len(numeric_cols)}\")\n    \n    scores = []\n    \n    for col in numeric_cols:\n        try:\n            serie = df[col].dropna()\n            \n            if len(serie) == 0:\n                continue\n                \n            # 1. Completitud de datos (peso: 0.3)\n            completitud = len(serie) / len(df)\n            score_completitud = completitud if completitud >= min_completitud else 0\n            \n            # 2. Variabilidad (peso: 0.25)\n            cv = serie.std() / serie.mean() if serie.mean() != 0 else 0\n            score_variabilidad = min(cv, 2.0) / 2.0 if cv >= min_variabilidad else 0\n            \n            # 3. Relevancia física para moto-compresores (peso: 0.45)\n            relevancia_fisica = 0.0\n            col_lower = col.lower()\n            \n            # Variables críticas (relevancia alta)\n            if any(term in col_lower for term in ['presion', 'pressure', 'temp', 'temperatura', 'vibr']):\n                relevancia_fisica = 1.0\n            # Variables importantes (relevancia media-alta)  \n            elif any(term in col_lower for term in ['motor', 'corriente', 'current', 'voltage', 'voltaje', 'power', 'potencia']):\n                relevancia_fisica = 0.8\n            # Variables útiles (relevancia media)\n            elif any(term in col_lower for term in ['flujo', 'flow', 'nivel', 'level', 'rpm', 'velocidad', 'speed']):\n                relevancia_fisica = 0.6\n            # Variables auxiliares (relevancia baja)\n            else:\n                relevancia_fisica = 0.3\n            \n            # Score compuesto\n            score_final = (0.3 * score_completitud + \n                          0.25 * score_variabilidad + \n                          0.45 * relevancia_fisica)\n            \n            scores.append({\n                'variable': col,\n                'score_final': score_final,\n                'completitud': completitud,\n                'variabilidad': cv,\n                'relevancia_fisica': relevancia_fisica,\n                'media': serie.mean(),\n                'std': serie.std()\n            })\n            \n        except Exception as e:\n            print(f\"      ⚠️  Error procesando {col}: {str(e)}\")\n            continue\n    \n    # Convertir a DataFrame y ordenar por score\n    df_scores = pd.DataFrame(scores)\n    df_scores = df_scores.sort_values('score_final', ascending=False)\n    \n    # Seleccionar mejores variables\n    variables_seleccionadas = df_scores.head(max_variables)['variable'].tolist()\n    \n    print(f\"✅ Variables seleccionadas: {len(variables_seleccionadas)}\")\n    print(\"\\n🏆 Top 10 variables por relevancia:\")\n    for i, row in variables_seleccionadas[:10].iterrows() if hasattr(variables_seleccionadas[:10], 'iterrows') else enumerate(df_scores.head(10).itertuples()):\n        if hasattr(variables_seleccionadas[:10], 'iterrows'):\n            print(f\"   {i+1:2d}. {row['variable']} (score: {row['score_final']:.3f})\")\n        else:\n            print(f\"   {i+1:2d}. {row.variable} (score: {row.score_final:.3f})\")\n    \n    return variables_seleccionadas, df_scores\n\n# Ejecutar selección\nvariables_seleccionadas, df_scores = seleccionar_variables_relevantes(df)\n\n# Crear dataset filtrado\ndf_selected = df[variables_seleccionadas].copy()\nprint(f\"\\n📐 Dataset filtrado: {df_selected.shape[0]:,} × {df_selected.shape[1]}\")\nprint(f\"💾 Reducción de memoria: {(df.memory_usage(deep=True).sum() - df_selected.memory_usage(deep=True).sum()) / 1024**2:.1f} MB\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Características de Ventanas Móviles (Rolling Features)\n\nCreación de estadísticas agregadas en diferentes ventanas temporales para capturar tendencias y patrones de degradación."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def crear_rolling_features(df, ventanas=['6H', '24H', '72H'], \n                          estadisticas=['mean', 'std', 'min', 'max'], \n                          variables_prioritarias=None):\n    \"\"\"\n    Crea características de ventanas móviles para múltiples estadísticas\n    \"\"\"\n    print(\"📈 Creando características de ventanas móviles...\")\n    \n    if variables_prioritarias is None:\n        variables_prioritarias = df.columns[:20]  # Primeras 20 por defecto\n    \n    print(f\"   📊 Variables a procesar: {len(variables_prioritarias)}\")\n    print(f\"   ⏱️  Ventanas temporales: {ventanas}\")\n    print(f\"   📋 Estadísticas: {estadisticas}\")\n    \n    rolling_features = pd.DataFrame(index=df.index)\n    contador_features = 0\n    \n    for ventana in ventanas:\n        print(f\"\\n   🔄 Procesando ventana {ventana}...\")\n        \n        for variable in variables_prioritarias:\n            try:\n                # Crear rolling window\n                rolling = df[variable].rolling(window=ventana, min_periods=1)\n                \n                for stat in estadisticas:\n                    nombre_feature = f\"{variable}_roll_{ventana}_{stat}\"\n                    \n                    if stat == 'mean':\n                        rolling_features[nombre_feature] = rolling.mean()\n                    elif stat == 'std':\n                        rolling_features[nombre_feature] = rolling.std()\n                    elif stat == 'min':\n                        rolling_features[nombre_feature] = rolling.min()\n                    elif stat == 'max':\n                        rolling_features[nombre_feature] = rolling.max()\n                    elif stat == 'median':\n                        rolling_features[nombre_feature] = rolling.median()\n                    elif stat == 'skew':\n                        rolling_features[nombre_feature] = rolling.skew()\n                    elif stat == 'kurt':\n                        rolling_features[nombre_feature] = rolling.kurt()\n                    \n                    contador_features += 1\n                    \n            except Exception as e:\n                print(f\"      ⚠️  Error con {variable}: {str(e)}\")\n                continue\n    \n    # Crear características adicionales de volatilidad y tendencia\n    print(f\"\\n   📊 Creando características de volatilidad y tendencia...\")\n    \n    for ventana in ['24H', '72H']:  # Solo ventanas más grandes\n        for variable in variables_prioritarias[:10]:  # Solo top variables\n            try:\n                rolling = df[variable].rolling(window=ventana, min_periods=1)\n                \n                # Coeficiente de variación\n                mean_val = rolling.mean()\n                std_val = rolling.std()\n                cv_name = f\"{variable}_roll_{ventana}_cv\"\n                rolling_features[cv_name] = std_val / (mean_val + 1e-6)  # Evitar división por 0\n                \n                # Tendencia (pendiente)\n                trend_name = f\"{variable}_roll_{ventana}_trend\"\n                rolling_features[trend_name] = rolling.apply(\n                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0\n                )\n                \n                contador_features += 2\n                \n            except Exception as e:\n                continue\n    \n    print(f\"✅ Rolling features creadas: {contador_features}\")\n    print(f\"📐 Dimensiones: {rolling_features.shape[0]:,} × {rolling_features.shape[1]}\")\n    \n    return rolling_features\n\n# Ejecutar creación de rolling features\nrolling_features = crear_rolling_features(df_selected, \n                                        ventanas=['6H', '24H', '72H'],\n                                        variables_prioritarias=df_selected.columns[:15])\n\nprint(f\"\\n💾 Memoria rolling features: {rolling_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Características de Lag (Retrasos Temporales)\n\nCreación de variables que capturan el comportamiento histórico del equipo para predecir comportamiento futuro."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def crear_lag_features(df, lags=['2H', '12H', '48H'], \n                      variables_prioritarias=None, max_features=150):\n    \"\"\"\n    Crea características de lag (retrasos temporales)\n    \"\"\"\n    print(\"⏪ Creando características de lag (retrasos temporales)...\")\n    \n    if variables_prioritarias is None:\n        variables_prioritarias = df.columns[:10]  # Primeras 10 por defecto\n    \n    print(f\"   📊 Variables a procesar: {len(variables_prioritarias)}\")\n    print(f\"   ⏱️  Lags temporales: {lags}\")\n    \n    lag_features = pd.DataFrame(index=df.index)\n    contador_features = 0\n    \n    for lag in lags:\n        print(f\"\\n   🔄 Procesando lag {lag}...\")\n        \n        for variable in variables_prioritarias:\n            if contador_features >= max_features:\n                print(f\"   ⚠️  Límite de {max_features} features alcanzado\")\n                break\n                \n            try:\n                # Convertir lag a frecuencia pandas\n                lag_periods = pd.Timedelta(lag)\n                \n                # Crear feature de lag\n                nombre_feature = f\"{variable}_lag_{lag}\"\n                lag_features[nombre_feature] = df[variable].shift(freq=lag_periods)\n                \n                contador_features += 1\n                \n            except Exception as e:\n                print(f\"      ⚠️  Error con {variable} lag {lag}: {str(e)}\")\n                continue\n        \n        if contador_features >= max_features:\n            break\n    \n    # Crear características de diferencias (cambios entre períodos)\n    print(f\"\\n   📊 Creando características de diferencias...\")\n    \n    for variable in variables_prioritarias[:8]:  # Solo top variables\n        if contador_features >= max_features:\n            break\n            \n        try:\n            # Diferencia de 12H\n            diff_name = f\"{variable}_diff_12H\"\n            lag_features[diff_name] = df[variable] - df[variable].shift(freq='12H')\n            \n            # Cambio porcentual de 24H\n            pct_name = f\"{variable}_pct_change_24H\"\n            lag_features[pct_name] = df[variable].pct_change(\n                periods=pd.Timedelta('24H') // pd.infer_freq(df.index) if pd.infer_freq(df.index) else 1\n            )\n            \n            contador_features += 2\n            \n        except Exception as e:\n            continue\n    \n    print(f\"✅ Lag features creadas: {contador_features}\")\n    print(f\"📐 Dimensiones: {lag_features.shape[0]:,} × {lag_features.shape[1]}\")\n    \n    return lag_features\n\n# Ejecutar creación de lag features\nlag_features = crear_lag_features(df_selected, \n                                lags=['2H', '12H', '48H'],\n                                variables_prioritarias=df_selected.columns[:12])\n\nprint(f\"\\n💾 Memoria lag features: {lag_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Dataset Final de Características y Guardado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Importar función de guardado completo\nexec(open('guardar_completo.py').read())\n\n# Consolidar todas las características\nprint(\"🔗 Consolidando dataset final de características...\")\n\n# Crear dataset base con variables originales seleccionadas\ndataset_final = df_selected.copy()\nprint(f\"📊 Variables originales: {dataset_final.shape[1]}\")\n\n# Agregar rolling features (seleccionar las más importantes para evitar overfitting)\nif not rolling_features.empty:\n    # Filtrar rolling features con variabilidad significativa\n    rolling_features_filtered = rolling_features.dropna(axis=1, thresh=int(0.7 * len(rolling_features)))\n    rolling_features_filtered = rolling_features_filtered.select_dtypes(include=[np.number])\n    \n    # Seleccionar solo features con variación suficiente\n    features_con_variacion = []\n    for col in rolling_features_filtered.columns:\n        if rolling_features_filtered[col].std() > 1e-6:  # Evitar features constantes\n            features_con_variacion.append(col)\n    \n    rolling_features_final = rolling_features_filtered[features_con_variacion[:100]]  # Top 100\n    dataset_final = pd.concat([dataset_final, rolling_features_final], axis=1)\n    print(f\"📈 Rolling features agregadas: {rolling_features_final.shape[1]}\")\n\n# Agregar lag features\nif not lag_features.empty:\n    # Filtrar lag features con variabilidad significativa\n    lag_features_filtered = lag_features.dropna(axis=1, thresh=int(0.5 * len(lag_features)))\n    lag_features_filtered = lag_features_filtered.select_dtypes(include=[np.number])\n    \n    # Seleccionar solo features con variación suficiente\n    features_con_variacion = []\n    for col in lag_features_filtered.columns:\n        if lag_features_filtered[col].std() > 1e-6:\n            features_con_variacion.append(col)\n    \n    lag_features_final = lag_features_filtered[features_con_variacion[:50]]  # Top 50\n    dataset_final = pd.concat([dataset_final, lag_features_final], axis=1)\n    print(f\"⏪ Lag features agregadas: {lag_features_final.shape[1]}\")\n\n# Limpieza final del dataset\nprint(\"\\n🧹 Limpieza final del dataset...\")\n\n# Eliminar columnas completamente nulas o constantes\ndataset_final = dataset_final.dropna(axis=1, how='all')\nfor col in dataset_final.select_dtypes(include=[np.number]).columns:\n    if dataset_final[col].std() < 1e-10:  # Prácticamente constante\n        dataset_final = dataset_final.drop(col, axis=1)\n\n# Convertir a float32 para optimizar memoria\nnumeric_columns = dataset_final.select_dtypes(include=[np.number]).columns\ndataset_final[numeric_columns] = dataset_final[numeric_columns].astype(np.float32)\n\n# Información final\nprint(f\"\\n📊 Dataset final consolidado:\")\nprint(f\"   📐 Dimensiones: {dataset_final.shape[0]:,} filas × {dataset_final.shape[1]} columnas\")\nprint(f\"   📅 Período: {dataset_final.index.min()} → {dataset_final.index.max()}\")\nprint(f\"   💾 Memoria: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\nprint(f\"   🔢 Variables numéricas: {len(dataset_final.select_dtypes(include=[np.number]).columns)}\")\n\n# Mostrar composición del dataset\nprint(f\"\\n📋 Composición del dataset final:\")\nprint(f\"   🔵 Variables originales: {len(df_selected.columns)}\")\nif not rolling_features.empty:\n    print(f\"   📈 Rolling features: {rolling_features_final.shape[1] if 'rolling_features_final' in locals() else 0}\")\nif not lag_features.empty:\n    print(f\"   ⏪ Lag features: {lag_features_final.shape[1] if 'lag_features_final' in locals() else 0}\")\n\n# Guardar dataset usando función completa\nprint(f\"\\n💾 Guardando dataset final de características...\")\n\ntry:\n    resultados_guardado = guardar_dataset_final(\n        df=dataset_final,\n        ruta_destino=output_path,\n        nombre_base='features_dataset',\n        archivos_procesados=['datos_procesados_exitosamente'],\n        archivos_fallidos=[],\n        valores_interpolados=0,\n        valores_clipped=0\n    )\n    \n    print(f\"\\n✅ FEATURE ENGINEERING COMPLETADO\")\n    print(f\"📁 Archivos generados en data/processed/:\")\n    for formato, resultado in resultados_guardado.items():\n        if resultado['exito']:\n            print(f\"   ✅ features_dataset.{formato} ({resultado['tamaño_mb']:.1f} MB)\")\n    \n    print(f\"\\n🎯 Dataset listo para entrenamiento de modelos\")\n    print(f\"➡️  Siguiente fase: 04_model_training.ipynb\")\n    \nexcept Exception as e:\n    print(f\"❌ Error al guardar: {str(e)}\")\n    # Guardar al menos en CSV como respaldo\n    backup_file = output_path / 'features_dataset_backup.csv'\n    dataset_final.to_csv(backup_file)\n    print(f\"💾 Guardado respaldo en: {backup_file}\")\n\n# Liberar memoria\ndel df, df_selected, rolling_features, lag_features\ngc.collect()\n\nprint(f\"\\n🏁 Feature Engineering finalizado exitosamente\")\nprint(f\"📊 Total de características creadas: {dataset_final.shape[1]}\")\nprint(f\"🔗 Dataset listo para la fase de modelado\")",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}