{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IngenierÃ­a de CaracterÃ­sticas (Feature Engineering)\n",
    "## Proyecto de Mantenimiento Predictivo - Moto-Compresores\n",
    "\n",
    "Este notebook contiene la creaciÃ³n y selecciÃ³n de caracterÃ­sticas para el modelo predictivo.\n",
    "\n",
    "### Objetivos:\n",
    "- Crear caracterÃ­sticas derivadas\n",
    "- Generar indicadores de tendencia\n",
    "- Seleccionar caracterÃ­sticas relevantes\n",
    "- Crear variables de degradaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.signal import detrend\nfrom scipy.fft import fft, fftfreq\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nimport warnings\nimport gc\n\n# ConfiguraciÃ³n\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\nprint(\"âœ… LibrerÃ­as importadas correctamente\")\nprint(f\"ğŸ“… Notebook ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Configurar rutas\ndata_processed_path = Path(\"data/processed\")\neventos_path = Path(\"eventos\")\noutput_path = Path(\"data/processed\")\n\n# Verificar archivos disponibles\nprint(\"ğŸ” Buscando archivos de datos preprocesados...\")\n\nparquet_files = list(data_processed_path.glob(\"*.parquet\"))\ncsv_files = list(data_processed_path.glob(\"*.csv\"))\n\nprint(f\"ğŸ“¦ Archivos Parquet encontrados: {len(parquet_files)}\")\nfor file in parquet_files:\n    print(f\"   â€¢ {file.name}\")\n    \nprint(f\"ğŸ“„ Archivos CSV encontrados: {len(csv_files)}\")\nfor file in csv_files:\n    print(f\"   â€¢ {file.name}\")\n\n# Cargar el dataset principal\ntry:\n    if parquet_files and 'timeseries_data.parquet' in [f.name for f in parquet_files]:\n        print(\"\\nğŸ’¾ Cargando timeseries_data.parquet...\")\n        df = pd.read_parquet(data_processed_path / \"timeseries_data.parquet\")\n        print(\"âœ… Parquet cargado exitosamente\")\n    elif csv_files and 'clean_timeseries_data.csv' in [f.name for f in csv_files]:\n        print(\"\\nğŸ’¾ Cargando clean_timeseries_data.csv...\")\n        df = pd.read_csv(data_processed_path / \"clean_timeseries_data.csv\", \n                        index_col=0, parse_dates=True)\n        print(\"âœ… CSV cargado exitosamente\")\n    else:\n        print(\"âŒ No se encontrÃ³ archivo de datos procesados\")\n        print(\"   Buscando archivo mÃ¡s reciente...\")\n        all_files = parquet_files + csv_files\n        if all_files:\n            latest_file = max(all_files, key=lambda x: x.stat().st_mtime)\n            print(f\"   Cargando: {latest_file.name}\")\n            if latest_file.suffix == '.parquet':\n                df = pd.read_parquet(latest_file)\n            else:\n                df = pd.read_csv(latest_file, index_col=0, parse_dates=True)\n            print(\"âœ… Archivo cargado exitosamente\")\n        else:\n            raise FileNotFoundError(\"No se encontraron archivos de datos\")\n            \nexcept Exception as e:\n    print(f\"âŒ Error al cargar datos: {str(e)}\")\n    raise\n\n# InformaciÃ³n del dataset cargado\nprint(f\"\\nğŸ“Š Dataset cargado:\")\nprint(f\"   ğŸ“ Dimensiones: {df.shape[0]:,} filas Ã— {df.shape[1]} columnas\")\nprint(f\"   ğŸ“… PerÃ­odo: {df.index.min()} â†’ {df.index.max()}\")\nprint(f\"   â±ï¸  Frecuencia: {pd.infer_freq(df.index) if hasattr(df.index, 'freq') else 'No detectada'}\")\nprint(f\"   ğŸ’¾ Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n\n# Preview de los datos\nprint(\"\\nğŸ‘€ Primeras 3 filas:\")\nprint(df.head(3))\n\nprint(\"\\nğŸ“‹ Tipos de datos:\")\nprint(df.dtypes.value_counts())",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. SelecciÃ³n Inteligente de Variables\n\nAlgoritmo automÃ¡tico para seleccionar las variables mÃ¡s relevantes para el mantenimiento predictivo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def seleccionar_variables_relevantes(df, min_completitud=0.7, min_variabilidad=0.01, \n                                   max_variables=50):\n    \"\"\"\n    Selecciona automÃ¡ticamente las variables mÃ¡s relevantes para mantenimiento predictivo\n    \"\"\"\n    print(\"ğŸ¯ Ejecutando selecciÃ³n inteligente de variables...\")\n    \n    # Solo variables numÃ©ricas\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n    print(f\"   ğŸ“Š Variables numÃ©ricas disponibles: {len(numeric_cols)}\")\n    \n    scores = []\n    \n    for col in numeric_cols:\n        try:\n            serie = df[col].dropna()\n            \n            if len(serie) == 0:\n                continue\n                \n            # 1. Completitud de datos (peso: 0.3)\n            completitud = len(serie) / len(df)\n            score_completitud = completitud if completitud >= min_completitud else 0\n            \n            # 2. Variabilidad (peso: 0.25)\n            cv = serie.std() / serie.mean() if serie.mean() != 0 else 0\n            score_variabilidad = min(cv, 2.0) / 2.0 if cv >= min_variabilidad else 0\n            \n            # 3. Relevancia fÃ­sica para moto-compresores (peso: 0.45)\n            relevancia_fisica = 0.0\n            col_lower = col.lower()\n            \n            # Variables crÃ­ticas (relevancia alta)\n            if any(term in col_lower for term in ['presion', 'pressure', 'temp', 'temperatura', 'vibr']):\n                relevancia_fisica = 1.0\n            # Variables importantes (relevancia media-alta)  \n            elif any(term in col_lower for term in ['motor', 'corriente', 'current', 'voltage', 'voltaje', 'power', 'potencia']):\n                relevancia_fisica = 0.8\n            # Variables Ãºtiles (relevancia media)\n            elif any(term in col_lower for term in ['flujo', 'flow', 'nivel', 'level', 'rpm', 'velocidad', 'speed']):\n                relevancia_fisica = 0.6\n            # Variables auxiliares (relevancia baja)\n            else:\n                relevancia_fisica = 0.3\n            \n            # Score compuesto\n            score_final = (0.3 * score_completitud + \n                          0.25 * score_variabilidad + \n                          0.45 * relevancia_fisica)\n            \n            scores.append({\n                'variable': col,\n                'score_final': score_final,\n                'completitud': completitud,\n                'variabilidad': cv,\n                'relevancia_fisica': relevancia_fisica,\n                'media': serie.mean(),\n                'std': serie.std()\n            })\n            \n        except Exception as e:\n            print(f\"      âš ï¸  Error procesando {col}: {str(e)}\")\n            continue\n    \n    # Convertir a DataFrame y ordenar por score\n    df_scores = pd.DataFrame(scores)\n    df_scores = df_scores.sort_values('score_final', ascending=False)\n    \n    # Seleccionar mejores variables\n    variables_seleccionadas = df_scores.head(max_variables)['variable'].tolist()\n    \n    print(f\"âœ… Variables seleccionadas: {len(variables_seleccionadas)}\")\n    print(\"\\nğŸ† Top 10 variables por relevancia:\")\n    for i, row in variables_seleccionadas[:10].iterrows() if hasattr(variables_seleccionadas[:10], 'iterrows') else enumerate(df_scores.head(10).itertuples()):\n        if hasattr(variables_seleccionadas[:10], 'iterrows'):\n            print(f\"   {i+1:2d}. {row['variable']} (score: {row['score_final']:.3f})\")\n        else:\n            print(f\"   {i+1:2d}. {row.variable} (score: {row.score_final:.3f})\")\n    \n    return variables_seleccionadas, df_scores\n\n# Ejecutar selecciÃ³n\nvariables_seleccionadas, df_scores = seleccionar_variables_relevantes(df)\n\n# Crear dataset filtrado\ndf_selected = df[variables_seleccionadas].copy()\nprint(f\"\\nğŸ“ Dataset filtrado: {df_selected.shape[0]:,} Ã— {df_selected.shape[1]}\")\nprint(f\"ğŸ’¾ ReducciÃ³n de memoria: {(df.memory_usage(deep=True).sum() - df_selected.memory_usage(deep=True).sum()) / 1024**2:.1f} MB\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. CaracterÃ­sticas de Ventanas MÃ³viles (Rolling Features)\n\nCreaciÃ³n de estadÃ­sticas agregadas en diferentes ventanas temporales para capturar tendencias y patrones de degradaciÃ³n."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def crear_rolling_features(df, ventanas=['6H', '24H', '72H'], \n                          estadisticas=['mean', 'std', 'min', 'max'], \n                          variables_prioritarias=None):\n    \"\"\"\n    Crea caracterÃ­sticas de ventanas mÃ³viles para mÃºltiples estadÃ­sticas\n    \"\"\"\n    print(\"ğŸ“ˆ Creando caracterÃ­sticas de ventanas mÃ³viles...\")\n    \n    if variables_prioritarias is None:\n        variables_prioritarias = df.columns[:20]  # Primeras 20 por defecto\n    \n    print(f\"   ğŸ“Š Variables a procesar: {len(variables_prioritarias)}\")\n    print(f\"   â±ï¸  Ventanas temporales: {ventanas}\")\n    print(f\"   ğŸ“‹ EstadÃ­sticas: {estadisticas}\")\n    \n    rolling_features = pd.DataFrame(index=df.index)\n    contador_features = 0\n    \n    for ventana in ventanas:\n        print(f\"\\n   ğŸ”„ Procesando ventana {ventana}...\")\n        \n        for variable in variables_prioritarias:\n            try:\n                # Crear rolling window\n                rolling = df[variable].rolling(window=ventana, min_periods=1)\n                \n                for stat in estadisticas:\n                    nombre_feature = f\"{variable}_roll_{ventana}_{stat}\"\n                    \n                    if stat == 'mean':\n                        rolling_features[nombre_feature] = rolling.mean()\n                    elif stat == 'std':\n                        rolling_features[nombre_feature] = rolling.std()\n                    elif stat == 'min':\n                        rolling_features[nombre_feature] = rolling.min()\n                    elif stat == 'max':\n                        rolling_features[nombre_feature] = rolling.max()\n                    elif stat == 'median':\n                        rolling_features[nombre_feature] = rolling.median()\n                    elif stat == 'skew':\n                        rolling_features[nombre_feature] = rolling.skew()\n                    elif stat == 'kurt':\n                        rolling_features[nombre_feature] = rolling.kurt()\n                    \n                    contador_features += 1\n                    \n            except Exception as e:\n                print(f\"      âš ï¸  Error con {variable}: {str(e)}\")\n                continue\n    \n    # Crear caracterÃ­sticas adicionales de volatilidad y tendencia\n    print(f\"\\n   ğŸ“Š Creando caracterÃ­sticas de volatilidad y tendencia...\")\n    \n    for ventana in ['24H', '72H']:  # Solo ventanas mÃ¡s grandes\n        for variable in variables_prioritarias[:10]:  # Solo top variables\n            try:\n                rolling = df[variable].rolling(window=ventana, min_periods=1)\n                \n                # Coeficiente de variaciÃ³n\n                mean_val = rolling.mean()\n                std_val = rolling.std()\n                cv_name = f\"{variable}_roll_{ventana}_cv\"\n                rolling_features[cv_name] = std_val / (mean_val + 1e-6)  # Evitar divisiÃ³n por 0\n                \n                # Tendencia (pendiente)\n                trend_name = f\"{variable}_roll_{ventana}_trend\"\n                rolling_features[trend_name] = rolling.apply(\n                    lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0\n                )\n                \n                contador_features += 2\n                \n            except Exception as e:\n                continue\n    \n    print(f\"âœ… Rolling features creadas: {contador_features}\")\n    print(f\"ğŸ“ Dimensiones: {rolling_features.shape[0]:,} Ã— {rolling_features.shape[1]}\")\n    \n    return rolling_features\n\n# Ejecutar creaciÃ³n de rolling features\nrolling_features = crear_rolling_features(df_selected, \n                                        ventanas=['6H', '24H', '72H'],\n                                        variables_prioritarias=df_selected.columns[:15])\n\nprint(f\"\\nğŸ’¾ Memoria rolling features: {rolling_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. CaracterÃ­sticas de Lag (Retrasos Temporales)\n\nCreaciÃ³n de variables que capturan el comportamiento histÃ³rico del equipo para predecir comportamiento futuro."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def crear_lag_features(df, lags=['2H', '12H', '48H'], \n                      variables_prioritarias=None, max_features=150):\n    \"\"\"\n    Crea caracterÃ­sticas de lag (retrasos temporales)\n    \"\"\"\n    print(\"âª Creando caracterÃ­sticas de lag (retrasos temporales)...\")\n    \n    if variables_prioritarias is None:\n        variables_prioritarias = df.columns[:10]  # Primeras 10 por defecto\n    \n    print(f\"   ğŸ“Š Variables a procesar: {len(variables_prioritarias)}\")\n    print(f\"   â±ï¸  Lags temporales: {lags}\")\n    \n    lag_features = pd.DataFrame(index=df.index)\n    contador_features = 0\n    \n    for lag in lags:\n        print(f\"\\n   ğŸ”„ Procesando lag {lag}...\")\n        \n        for variable in variables_prioritarias:\n            if contador_features >= max_features:\n                print(f\"   âš ï¸  LÃ­mite de {max_features} features alcanzado\")\n                break\n                \n            try:\n                # Convertir lag a frecuencia pandas\n                lag_periods = pd.Timedelta(lag)\n                \n                # Crear feature de lag\n                nombre_feature = f\"{variable}_lag_{lag}\"\n                lag_features[nombre_feature] = df[variable].shift(freq=lag_periods)\n                \n                contador_features += 1\n                \n            except Exception as e:\n                print(f\"      âš ï¸  Error con {variable} lag {lag}: {str(e)}\")\n                continue\n        \n        if contador_features >= max_features:\n            break\n    \n    # Crear caracterÃ­sticas de diferencias (cambios entre perÃ­odos)\n    print(f\"\\n   ğŸ“Š Creando caracterÃ­sticas de diferencias...\")\n    \n    for variable in variables_prioritarias[:8]:  # Solo top variables\n        if contador_features >= max_features:\n            break\n            \n        try:\n            # Diferencia de 12H\n            diff_name = f\"{variable}_diff_12H\"\n            lag_features[diff_name] = df[variable] - df[variable].shift(freq='12H')\n            \n            # Cambio porcentual de 24H\n            pct_name = f\"{variable}_pct_change_24H\"\n            lag_features[pct_name] = df[variable].pct_change(\n                periods=pd.Timedelta('24H') // pd.infer_freq(df.index) if pd.infer_freq(df.index) else 1\n            )\n            \n            contador_features += 2\n            \n        except Exception as e:\n            continue\n    \n    print(f\"âœ… Lag features creadas: {contador_features}\")\n    print(f\"ğŸ“ Dimensiones: {lag_features.shape[0]:,} Ã— {lag_features.shape[1]}\")\n    \n    return lag_features\n\n# Ejecutar creaciÃ³n de lag features\nlag_features = crear_lag_features(df_selected, \n                                lags=['2H', '12H', '48H'],\n                                variables_prioritarias=df_selected.columns[:12])\n\nprint(f\"\\nğŸ’¾ Memoria lag features: {lag_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Dataset Final de CaracterÃ­sticas y Guardado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Importar funciÃ³n de guardado completo\nexec(open('guardar_completo.py').read())\n\n# Consolidar todas las caracterÃ­sticas\nprint(\"ğŸ”— Consolidando dataset final de caracterÃ­sticas...\")\n\n# Crear dataset base con variables originales seleccionadas\ndataset_final = df_selected.copy()\nprint(f\"ğŸ“Š Variables originales: {dataset_final.shape[1]}\")\n\n# Agregar rolling features (seleccionar las mÃ¡s importantes para evitar overfitting)\nif not rolling_features.empty:\n    # Filtrar rolling features con variabilidad significativa\n    rolling_features_filtered = rolling_features.dropna(axis=1, thresh=int(0.7 * len(rolling_features)))\n    rolling_features_filtered = rolling_features_filtered.select_dtypes(include=[np.number])\n    \n    # Seleccionar solo features con variaciÃ³n suficiente\n    features_con_variacion = []\n    for col in rolling_features_filtered.columns:\n        if rolling_features_filtered[col].std() > 1e-6:  # Evitar features constantes\n            features_con_variacion.append(col)\n    \n    rolling_features_final = rolling_features_filtered[features_con_variacion[:100]]  # Top 100\n    dataset_final = pd.concat([dataset_final, rolling_features_final], axis=1)\n    print(f\"ğŸ“ˆ Rolling features agregadas: {rolling_features_final.shape[1]}\")\n\n# Agregar lag features\nif not lag_features.empty:\n    # Filtrar lag features con variabilidad significativa\n    lag_features_filtered = lag_features.dropna(axis=1, thresh=int(0.5 * len(lag_features)))\n    lag_features_filtered = lag_features_filtered.select_dtypes(include=[np.number])\n    \n    # Seleccionar solo features con variaciÃ³n suficiente\n    features_con_variacion = []\n    for col in lag_features_filtered.columns:\n        if lag_features_filtered[col].std() > 1e-6:\n            features_con_variacion.append(col)\n    \n    lag_features_final = lag_features_filtered[features_con_variacion[:50]]  # Top 50\n    dataset_final = pd.concat([dataset_final, lag_features_final], axis=1)\n    print(f\"âª Lag features agregadas: {lag_features_final.shape[1]}\")\n\n# Limpieza final del dataset\nprint(\"\\nğŸ§¹ Limpieza final del dataset...\")\n\n# Eliminar columnas completamente nulas o constantes\ndataset_final = dataset_final.dropna(axis=1, how='all')\nfor col in dataset_final.select_dtypes(include=[np.number]).columns:\n    if dataset_final[col].std() < 1e-10:  # PrÃ¡cticamente constante\n        dataset_final = dataset_final.drop(col, axis=1)\n\n# Convertir a float32 para optimizar memoria\nnumeric_columns = dataset_final.select_dtypes(include=[np.number]).columns\ndataset_final[numeric_columns] = dataset_final[numeric_columns].astype(np.float32)\n\n# InformaciÃ³n final\nprint(f\"\\nğŸ“Š Dataset final consolidado:\")\nprint(f\"   ğŸ“ Dimensiones: {dataset_final.shape[0]:,} filas Ã— {dataset_final.shape[1]} columnas\")\nprint(f\"   ğŸ“… PerÃ­odo: {dataset_final.index.min()} â†’ {dataset_final.index.max()}\")\nprint(f\"   ğŸ’¾ Memoria: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\nprint(f\"   ğŸ”¢ Variables numÃ©ricas: {len(dataset_final.select_dtypes(include=[np.number]).columns)}\")\n\n# Mostrar composiciÃ³n del dataset\nprint(f\"\\nğŸ“‹ ComposiciÃ³n del dataset final:\")\nprint(f\"   ğŸ”µ Variables originales: {len(df_selected.columns)}\")\nif not rolling_features.empty:\n    print(f\"   ğŸ“ˆ Rolling features: {rolling_features_final.shape[1] if 'rolling_features_final' in locals() else 0}\")\nif not lag_features.empty:\n    print(f\"   âª Lag features: {lag_features_final.shape[1] if 'lag_features_final' in locals() else 0}\")\n\n# Guardar dataset usando funciÃ³n completa\nprint(f\"\\nğŸ’¾ Guardando dataset final de caracterÃ­sticas...\")\n\ntry:\n    resultados_guardado = guardar_dataset_final(\n        df=dataset_final,\n        ruta_destino=output_path,\n        nombre_base='features_dataset',\n        archivos_procesados=['datos_procesados_exitosamente'],\n        archivos_fallidos=[],\n        valores_interpolados=0,\n        valores_clipped=0\n    )\n    \n    print(f\"\\nâœ… FEATURE ENGINEERING COMPLETADO\")\n    print(f\"ğŸ“ Archivos generados en data/processed/:\")\n    for formato, resultado in resultados_guardado.items():\n        if resultado['exito']:\n            print(f\"   âœ… features_dataset.{formato} ({resultado['tamaÃ±o_mb']:.1f} MB)\")\n    \n    print(f\"\\nğŸ¯ Dataset listo para entrenamiento de modelos\")\n    print(f\"â¡ï¸  Siguiente fase: 04_model_training.ipynb\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error al guardar: {str(e)}\")\n    # Guardar al menos en CSV como respaldo\n    backup_file = output_path / 'features_dataset_backup.csv'\n    dataset_final.to_csv(backup_file)\n    print(f\"ğŸ’¾ Guardado respaldo en: {backup_file}\")\n\n# Liberar memoria\ndel df, df_selected, rolling_features, lag_features\ngc.collect()\n\nprint(f\"\\nğŸ Feature Engineering finalizado exitosamente\")\nprint(f\"ğŸ“Š Total de caracterÃ­sticas creadas: {dataset_final.shape[1]}\")\nprint(f\"ğŸ”— Dataset listo para la fase de modelado\")",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}