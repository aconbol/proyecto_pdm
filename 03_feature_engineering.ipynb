{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IngenierÃ­a de CaracterÃ­sticas para Mantenimiento Predictivo\n",
    "## Proyecto: PredicciÃ³n de Fallas en Moto-Compresores - Oil & Gas\n",
    "\n",
    "### ğŸ¯ Objetivo del Notebook\n",
    "\n",
    "Este notebook constituye la **fase crÃ­tica** de transformaciÃ³n de datos donde convertimos las series temporales limpias en un dataset enriquecido y etiquetado, optimizado para el entrenamiento de modelos de Machine Learning. Nuestro objetivo principal es **predecir fallas en moto-compresores con 7 dÃ­as de antelaciÃ³n**, una ventana temporal que permite la planificaciÃ³n efectiva de mantenimientos preventivos en el sector Oil & Gas.\n",
    "\n",
    "### ğŸ“‹ Tareas Principales\n",
    "\n",
    "1. **Carga y ValidaciÃ³n de Datos**: Integrar el dataset preprocesado con el historial de eventos\n",
    "2. **IngenierÃ­a de CaracterÃ­sticas Temporales**: Crear features que capturen la dinÃ¡mica del deterioro\n",
    "3. **CaracterÃ­sticas Avanzadas**: Implementar features de tasas de cambio, frecuencia y detecciÃ³n de anomalÃ­as\n",
    "4. **Etiquetado de Fallas**: Crear la variable objetivo basada en ventanas de pre-falla de 7 dÃ­as\n",
    "5. **ValidaciÃ³n y PreparaciÃ³n Final**: Garantizar calidad de datos para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LibrerÃ­as importadas exitosamente\n",
      "ğŸ“Š VersiÃ³n de pandas: 2.3.1\n",
      "ğŸ”¢ VersiÃ³n de numpy: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# ImportaciÃ³n de librerÃ­as esenciales para ingenierÃ­a de caracterÃ­sticas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# LibrerÃ­as especializadas para anÃ¡lisis de seÃ±ales y anomalÃ­as\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# ConfiguraciÃ³n del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas exitosamente\")\n",
    "print(f\"ğŸ“Š VersiÃ³n de pandas: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ VersiÃ³n de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ValidaciÃ³n de rutas y archivos:\n",
      "   Datos procesados: data/processed - âœ… Existe\n",
      "   Eventos: eventos - âœ… Existe\n",
      "   Timeseries: data/processed/timeseries_data_temporal_fixed.parquet - âœ… Existe\n",
      "   Historial: eventos/Historial C1 RGD.xlsx - âœ… Existe\n",
      "\n",
      "âœ… Todos los archivos requeridos estÃ¡n disponibles\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraciÃ³n de rutas de datos con validaciÃ³n de existencia\n",
    "# Esta configuraciÃ³n garantiza la reproducibilidad del pipeline\n",
    "\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('.')\n",
    "\n",
    "# Rutas especÃ­ficas para datos procesados y eventos\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Archivos especÃ­ficos requeridos\n",
    "archivo_timeseries = ruta_processed / 'timeseries_data_temporal_fixed.parquet'\n",
    "archivo_historial = ruta_eventos / 'Historial C1 RGD.xlsx'\n",
    "\n",
    "# ValidaciÃ³n crÃ­tica de existencia de archivos\n",
    "print(\"ğŸ“ ValidaciÃ³n de rutas y archivos:\")\n",
    "print(f\"   Datos procesados: {ruta_processed} - {'âœ… Existe' if ruta_processed.exists() else 'âŒ No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'âœ… Existe' if ruta_eventos.exists() else 'âŒ No existe'}\")\n",
    "print(f\"   Timeseries: {archivo_timeseries} - {'âœ… Existe' if archivo_timeseries.exists() else 'âŒ No existe'}\")\n",
    "print(f\"   Historial: {archivo_historial} - {'âœ… Existe' if archivo_historial.exists() else 'âŒ No existe'}\")\n",
    "\n",
    "# VerificaciÃ³n crÃ­tica - detener ejecuciÃ³n si faltan archivos esenciales\n",
    "archivos_requeridos = [archivo_timeseries, archivo_historial]\n",
    "archivos_faltantes = [arch for arch in archivos_requeridos if not arch.exists()]\n",
    "\n",
    "if archivos_faltantes:\n",
    "    print(f\"\\nâŒ ERROR CRÃTICO: Faltan archivos esenciales:\")\n",
    "    for archivo in archivos_faltantes:\n",
    "        print(f\"   - {archivo}\")\n",
    "    raise FileNotFoundError(\"No se pueden continuar sin los archivos de datos requeridos\")\n",
    "else:\n",
    "    print(\"\\nâœ… Todos los archivos requeridos estÃ¡n disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“‚ Carga y ValidaciÃ³n de Datos\n",
    "\n",
    "### ğŸ”„ Proceso de Carga Inteligente\n",
    "\n",
    "En esta fase crÃ­tica, cargaremos tanto el **dataset de series temporales procesado** como el **historial de eventos de mantenimiento**. La calidad de este proceso determina directamente la efectividad de nuestro modelo predictivo.\n",
    "\n",
    "El dataset de series temporales contiene las mediciones continuas de sensores del moto-compresor, ya limpias y preprocesadas. El historial de eventos proporciona las fechas exactas de las fallas histÃ³ricas, informaciÃ³n esencial para crear nuestras etiquetas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Cargando dataset principal de series temporales...\n",
      "âœ… Dataset principal cargado exitosamente\n",
      "   ğŸ“Š Dimensiones: 19,752 filas Ã— 34 columnas\n",
      "   ğŸ“… PerÃ­odo temporal: 2023-01-11 00:00:00 â†’ 2025-04-12 23:00:00\n",
      "   â±ï¸  Frecuencia detectada: h\n",
      "   ğŸ’¾ Uso de memoria: 5.3 MB\n",
      "   ğŸ“ˆ Completitud promedio: 97.1%\n",
      "   ğŸ”¢ Tipos de datos: {dtype('float64'): np.int64(33), dtype('<M8[ns]'): np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# Carga optimizada del dataset principal de series temporales\n",
    "print(\"âš¡ Cargando dataset principal de series temporales...\")\n",
    "\n",
    "try:\n",
    "    # Cargar el dataset principal con optimizaciones de memoria\n",
    "    df = pd.read_parquet(archivo_timeseries, engine='pyarrow')\n",
    "    \n",
    "    # Validaciones crÃ­ticas inmediatas\n",
    "    if df.empty:\n",
    "        raise ValueError(\"El dataset cargado estÃ¡ vacÃ­o\")\n",
    "    \n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"âš ï¸  Convirtiendo Ã­ndice a DatetimeIndex\")\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # InformaciÃ³n bÃ¡sica del dataset\n",
    "    print(f\"âœ… Dataset principal cargado exitosamente\")\n",
    "    print(f\"   ğŸ“Š Dimensiones: {df.shape[0]:,} filas Ã— {df.shape[1]} columnas\")\n",
    "    print(f\"   ğŸ“… PerÃ­odo temporal: {df.index.min()} â†’ {df.index.max()}\")\n",
    "    print(f\"   â±ï¸  Frecuencia detectada: {pd.infer_freq(df.index) or 'Variable/No detectada'}\")\n",
    "    print(f\"   ğŸ’¾ Uso de memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # AnÃ¡lisis de calidad de datos\n",
    "    completitud_promedio = (df.count().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    print(f\"   ğŸ“ˆ Completitud promedio: {completitud_promedio:.1f}%\")\n",
    "    \n",
    "    # Verificar tipos de datos\n",
    "    tipos_datos = df.dtypes.value_counts()\n",
    "    print(f\"   ğŸ”¢ Tipos de datos: {dict(tipos_datos)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error crÃ­tico al cargar el dataset principal: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Cargando historial de eventos de mantenimiento...\n",
      "ğŸ“Š Dimensiones del archivo raw: (92, 17)\n",
      "ğŸ” Analizando estructura para encontrar los headers reales...\n",
      "âœ… Headers encontrados en fila 3: 7/7 coincidencias\n",
      "ğŸ“Š DataFrame cargado con dimensiones: (88, 17)\n",
      "âœ… No se detectaron columnas 'Unnamed', estructura correcta\n",
      "âœ… Historial procesado exitosamente\n",
      "ğŸ“Š Dimensiones finales: 88 eventos Ã— 17 campos\n",
      "ğŸ“‹ Columnas finales: ['NÂ°', 'NUMERO AVISO', 'CLASE DE ORDEN', 'DESCRIPCION', 'Fecha     ', 'FECHA INICIO', 'FECHA FIN', 'HORA INICIO', 'HORA FIN', 'DURACION PARADA', 'NUMERO DE ORDEN DE TRABAJO', 'CLASE OT', 'PUESTO DE TRABAJO', 'EQUIPO', 'UBICACIÃ“N TÃ‰CNICA', 'Creado por  ', 'TEXTO AMPLIADO']\n",
      "\n",
      "ğŸ¯ Ã‰XITO: Historial cargado correctamente\n",
      "   ğŸ“Š 88 eventos cargados\n",
      "   ğŸ“‹ 17 columnas identificadas correctamente\n",
      "\n",
      "ğŸ“… Columnas de fecha detectadas correctamente: ['Fecha     ', 'FECHA INICIO', 'FECHA FIN', 'HORA INICIO']\n",
      "ğŸ¯ Usando columna principal de fecha: 'Fecha     '\n",
      "   âœ… ConversiÃ³n de fechas: 88/88 fechas vÃ¡lidas\n",
      "   ğŸ“… Rango de eventos: 2023-01-11 00:00:00 a 2025-05-21 00:00:00\n",
      "   ğŸ“Š Eventos Ãºnicos: 58\n"
     ]
    }
   ],
   "source": [
    "# Carga y procesamiento del historial de eventos - FUNCIÃ“N CORREGIDA\n",
    "print(\"\\nğŸ”„ Cargando historial de eventos de mantenimiento...\")\n",
    "\n",
    "def cargar_historial_eventos_corregido(archivo_historial):\n",
    "    \"\"\"\n",
    "    FunciÃ³n corregida que usa exactamente el mismo cÃ³digo que funcionaba en el EDA\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo sin header para analizar la estructura\n",
    "        df_raw = pd.read_excel(archivo_historial, header=None, engine='openpyxl')\n",
    "        \n",
    "        print(f\"ğŸ“Š Dimensiones del archivo raw: {df_raw.shape}\")\n",
    "        print(f\"ğŸ” Analizando estructura para encontrar los headers reales...\")\n",
    "        \n",
    "        # Buscar la fila que contiene los headers reales\n",
    "        header_row = None\n",
    "        expected_headers = ['NUMERO', 'AVISO', 'CLASE', 'DESCRIPCION', 'FECHA', 'INICIO', 'FIN']\n",
    "        \n",
    "        for idx in range(min(10, len(df_raw))):  # Buscar en las primeras 10 filas\n",
    "            row = df_raw.iloc[idx]\n",
    "            row_str = ' '.join(str(cell).upper() for cell in row if pd.notna(cell))\n",
    "            \n",
    "            # Verificar si esta fila contiene los encabezados esperados\n",
    "            header_matches = sum(1 for header in expected_headers if header in row_str)\n",
    "            \n",
    "            if header_matches >= 3:  # Al menos 3 coincidencias\n",
    "                header_row = idx\n",
    "                print(f\"âœ… Headers encontrados en fila {idx}: {header_matches}/{len(expected_headers)} coincidencias\")\n",
    "                break\n",
    "        \n",
    "        # Si no encontramos headers automÃ¡ticamente, usar fila 3 por defecto (como en el EDA exitoso)\n",
    "        if header_row is None:\n",
    "            print(\"âš ï¸  No se encontraron encabezados automÃ¡ticamente, usando fila 3 (Ã­ndice 3)\")\n",
    "            header_row = 3\n",
    "        \n",
    "        # Cargar el archivo usando la fila de headers identificada\n",
    "        historial_df = pd.read_excel(archivo_historial, \n",
    "                                   header=header_row,\n",
    "                                   engine='openpyxl')\n",
    "        \n",
    "        print(f\"ğŸ“Š DataFrame cargado con dimensiones: {historial_df.shape}\")\n",
    "        \n",
    "        # CORRECCIÃ“N CRÃTICA: Verificar si tenemos columnas 'Unnamed'\n",
    "        unnamed_cols = [col for col in historial_df.columns if 'Unnamed:' in str(col)]\n",
    "        if unnamed_cols:\n",
    "            print(f\"âš ï¸  Detectadas {len(unnamed_cols)} columnas 'Unnamed'\")\n",
    "            print(f\"ğŸ”§ Aplicando correcciÃ³n de nombres de columna...\")\n",
    "            \n",
    "            # Los nombres reales estÃ¡n en la primera fila de datos\n",
    "            if len(historial_df) > 0:\n",
    "                new_column_names = []\n",
    "                first_row = historial_df.iloc[0]\n",
    "                \n",
    "                for i, col in enumerate(historial_df.columns):\n",
    "                    if 'Unnamed:' in str(col) and i < len(first_row):\n",
    "                        # Usar el valor de la primera fila como nombre de columna\n",
    "                        new_name = str(first_row.iloc[i]).strip() if pd.notna(first_row.iloc[i]) else f'Col_{i+1}'\n",
    "                        new_column_names.append(new_name)\n",
    "                    else:\n",
    "                        # Mantener nombres existentes pero limpiarlos\n",
    "                        clean_name = str(col).strip().replace('\\n', ' ').replace('\\r', '')\n",
    "                        new_column_names.append(clean_name if clean_name else f'Col_{i+1}')\n",
    "                \n",
    "                # Aplicar nuevos nombres\n",
    "                historial_df.columns = new_column_names\n",
    "                \n",
    "                # Eliminar la primera fila que contiene los nombres de columna\n",
    "                print(f\"ğŸ§¹ Eliminando primera fila que contiene nombres de columna\")\n",
    "                historial_df = historial_df.iloc[1:].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"âœ… CorrecciÃ³n aplicada exitosamente\")\n",
    "        else:\n",
    "            print(f\"âœ… No se detectaron columnas 'Unnamed', estructura correcta\")\n",
    "        \n",
    "        # Limpiar datos (como en el EDA exitoso)\n",
    "        historial_df = historial_df.dropna(how='all')  # Eliminar filas vacÃ­as\n",
    "        historial_df = historial_df.dropna(axis=1, how='all')  # Eliminar columnas vacÃ­as\n",
    "        \n",
    "        print(f\"âœ… Historial procesado exitosamente\")\n",
    "        print(f\"ğŸ“Š Dimensiones finales: {historial_df.shape[0]:,} eventos Ã— {historial_df.shape[1]} campos\")\n",
    "        print(f\"ğŸ“‹ Columnas finales: {list(historial_df.columns)}\")\n",
    "        \n",
    "        return historial_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cargando historial: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Ejecutar la funciÃ³n corregida\n",
    "try:\n",
    "    df_eventos = cargar_historial_eventos_corregido(archivo_historial)\n",
    "    \n",
    "    if df_eventos is not None:\n",
    "        print(f\"\\nğŸ¯ Ã‰XITO: Historial cargado correctamente\")\n",
    "        print(f\"   ğŸ“Š {len(df_eventos)} eventos cargados\")\n",
    "        print(f\"   ğŸ“‹ {len(df_eventos.columns)} columnas identificadas correctamente\")\n",
    "        \n",
    "        # Identificar columnas de fecha correctamente\n",
    "        date_columns = [col for col in df_eventos.columns \n",
    "                       if any(keyword in col.upper() for keyword in ['FECHA', 'DATE', 'INICIO', 'START'])]\n",
    "        \n",
    "        print(f\"\\nğŸ“… Columnas de fecha detectadas correctamente: {date_columns}\")\n",
    "        \n",
    "        if date_columns:\n",
    "            primary_date_col = date_columns[0]\n",
    "            print(f\"ğŸ¯ Usando columna principal de fecha: '{primary_date_col}'\")\n",
    "            \n",
    "            # Procesar fechas como en el EDA exitoso\n",
    "            try:\n",
    "                df_eventos['fecha_evento'] = pd.to_datetime(df_eventos[primary_date_col], errors='coerce')\n",
    "                fechas_validas = df_eventos['fecha_evento'].notna().sum()\n",
    "                total_fechas = len(df_eventos)\n",
    "                \n",
    "                print(f\"   âœ… ConversiÃ³n de fechas: {fechas_validas}/{total_fechas} fechas vÃ¡lidas\")\n",
    "                \n",
    "                if fechas_validas > 0:\n",
    "                    fechas_limpias = df_eventos['fecha_evento'].dropna()\n",
    "                    print(f\"   ğŸ“… Rango de eventos: {fechas_limpias.min()} a {fechas_limpias.max()}\")\n",
    "                    print(f\"   ğŸ“Š Eventos Ãºnicos: {fechas_limpias.nunique()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Error procesando fechas: {str(e)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ Error: No se pudo cargar el historial de eventos\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error ejecutando funciÃ³n: {str(e)}\")\n",
    "    df_eventos = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Validando compatibilidad temporal entre datasets...\n",
      "ğŸ“Š AnÃ¡lisis de cobertura temporal:\n",
      "   ğŸ”§ Datos de sensores: 2023-01-11 00:00:00 a 2025-04-12 23:00:00\n",
      "   ğŸ“… Eventos registrados: 2023-01-11 00:00:00 a 2025-05-21 00:00:00\n",
      "   âœ… Solapamiento detectado: 2023-01-11 00:00:00 a 2025-04-12 23:00:00\n",
      "   â±ï¸  DuraciÃ³n del solapamiento: 822 days 23:00:00\n",
      "   ğŸ¯ Eventos utilizables para entrenamiento: 87\n",
      "   âœ… COMPATIBILIDAD TEMPORAL CONFIRMADA\n",
      "   ğŸ“ˆ DistribuciÃ³n anual de eventos: {2023: np.int64(53), 2024: np.int64(30), 2025: np.int64(4)}\n",
      "   ğŸ“Š Densidad de eventos: 0.1058 eventos/dÃ­a\n",
      "\n",
      "ğŸ“‹ RESUMEN DE VALIDACIÃ“N TEMPORAL:\n",
      "   Compatibilidad temporal: âœ… CONFIRMADA\n",
      "   Eventos utilizables: 87\n",
      "   DuraciÃ³n de solapamiento: 822 days 23:00:00\n",
      "\n",
      "ğŸ¯ Listo para proceder con ingenierÃ­a de caracterÃ­sticas\n"
     ]
    }
   ],
   "source": [
    "# ValidaciÃ³n de compatibilidad temporal entre datasets - CORREGIDO\n",
    "print(\"\\nğŸ” Validando compatibilidad temporal entre datasets...\")\n",
    "\n",
    "# Inicializar variables de control\n",
    "eventos_utilizables = None\n",
    "overlap_duration = None\n",
    "compatibilidad_temporal = False\n",
    "\n",
    "try:\n",
    "    # Validar pre-requisitos\n",
    "    if df is None or df.empty:\n",
    "        print(\"   âŒ Dataset principal no disponible\")\n",
    "    elif df_eventos is None or df_eventos.empty:\n",
    "        print(\"   âŒ Dataset de eventos no disponible\")\n",
    "    elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"   âŒ Ãndice del dataset principal no es de tipo fecha\")\n",
    "    elif 'fecha_evento' not in df_eventos.columns:\n",
    "        print(\"   âŒ No se encontrÃ³ columna 'fecha_evento' en el historial\")\n",
    "    else:\n",
    "        # AnÃ¡lisis de rangos temporales\n",
    "        sensor_start, sensor_end = df.index.min(), df.index.max()\n",
    "        eventos_limpios = df_eventos['fecha_evento'].dropna()\n",
    "        \n",
    "        if eventos_limpios.empty:\n",
    "            print(\"   âŒ No hay eventos con fechas vÃ¡lidas\")\n",
    "        else:\n",
    "            eventos_start, eventos_end = eventos_limpios.min(), eventos_limpios.max()\n",
    "            \n",
    "            print(f\"ğŸ“Š AnÃ¡lisis de cobertura temporal:\")\n",
    "            print(f\"   ğŸ”§ Datos de sensores: {sensor_start} a {sensor_end}\")\n",
    "            print(f\"   ğŸ“… Eventos registrados: {eventos_start} a {eventos_end}\")\n",
    "            \n",
    "            # Calcular solapamiento temporal\n",
    "            overlap_start = max(sensor_start, eventos_start)\n",
    "            overlap_end = min(sensor_end, eventos_end)\n",
    "            \n",
    "            if overlap_start <= overlap_end:\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                print(f\"   âœ… Solapamiento detectado: {overlap_start} a {overlap_end}\")\n",
    "                print(f\"   â±ï¸  DuraciÃ³n del solapamiento: {overlap_duration}\")\n",
    "                \n",
    "                # Eventos que caen dentro del rango de datos de sensores\n",
    "                eventos_utilizables = eventos_limpios[\n",
    "                    (eventos_limpios >= sensor_start) & (eventos_limpios <= sensor_end)\n",
    "                ]\n",
    "                \n",
    "                print(f\"   ğŸ¯ Eventos utilizables para entrenamiento: {len(eventos_utilizables)}\")\n",
    "                \n",
    "                if len(eventos_utilizables) > 0:\n",
    "                    compatibilidad_temporal = True\n",
    "                    print(f\"   âœ… COMPATIBILIDAD TEMPORAL CONFIRMADA\")\n",
    "                    \n",
    "                    # AnÃ¡lisis de distribuciÃ³n temporal de eventos\n",
    "                    eventos_por_year = eventos_utilizables.dt.year.value_counts().sort_index()\n",
    "                    print(f\"   ğŸ“ˆ DistribuciÃ³n anual de eventos: {dict(eventos_por_year)}\")\n",
    "                    \n",
    "                    # EstadÃ­sticas de densidad de eventos\n",
    "                    dias_totales = (sensor_end - sensor_start).days\n",
    "                    densidad_eventos = len(eventos_utilizables) / dias_totales if dias_totales > 0 else 0\n",
    "                    print(f\"   ğŸ“Š Densidad de eventos: {densidad_eventos:.4f} eventos/dÃ­a\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸  Sin eventos utilizables dentro del rango de sensores\")\n",
    "            else:\n",
    "                print(f\"   âŒ No hay solapamiento temporal entre datasets\")\n",
    "                print(f\"       Sensores terminan: {sensor_end}\")\n",
    "                print(f\"       Eventos inician: {eventos_start}\")\n",
    "                \n",
    "                # AnÃ¡lisis de eventos fuera de rango\n",
    "                eventos_antes = eventos_limpios[eventos_limpios < sensor_start]\n",
    "                eventos_despues = eventos_limpios[eventos_limpios > sensor_end]\n",
    "                print(f\"       Eventos antes del rango: {len(eventos_antes)}\")\n",
    "                print(f\"       Eventos despuÃ©s del rango: {len(eventos_despues)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error en validaciÃ³n temporal: {str(e)}\")\n",
    "    compatibilidad_temporal = False\n",
    "\n",
    "# Resumen ejecutivo de la validaciÃ³n\n",
    "print(f\"\\nğŸ“‹ RESUMEN DE VALIDACIÃ“N TEMPORAL:\")\n",
    "print(f\"   Compatibilidad temporal: {'âœ… CONFIRMADA' if compatibilidad_temporal else 'âŒ NO CONFIRMADA'}\")\n",
    "print(f\"   Eventos utilizables: {len(eventos_utilizables) if eventos_utilizables is not None else 0}\")\n",
    "print(f\"   DuraciÃ³n de solapamiento: {overlap_duration if overlap_duration else 'N/A'}\")\n",
    "\n",
    "# Configurar variables para las siguientes etapas\n",
    "if not compatibilidad_temporal:\n",
    "    print(f\"\\nâš ï¸  ADVERTENCIA: Sin compatibilidad temporal, se procederÃ¡ con anÃ¡lisis limitado\")\n",
    "    eventos_utilizables = pd.Series(dtype='datetime64[ns]')  # Serie vacÃ­a\n",
    "else:\n",
    "    print(f\"\\nğŸ¯ Listo para proceder con ingenierÃ­a de caracterÃ­sticas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ—ï¸ IngenierÃ­a de CaracterÃ­sticas Temporales\n",
    "\n",
    "### ğŸ“ˆ CaracterÃ­sticas de Ventanas MÃ³viles (Rolling Features)\n",
    "\n",
    "Las caracterÃ­sticas de ventanas mÃ³viles son fundamentales para capturar la **dinÃ¡mica temporal del deterioro** en equipos industriales. Implementaremos mÃºltiples horizontes temporales para detectar tanto degradaciones lentas como cambios sÃºbitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ï¸ INICIANDO CREACIÃ“N DE CARACTERÃSTICAS TEMPORALES\n",
      "============================================================\n",
      "ğŸ“ˆ Creando caracterÃ­sticas de ventanas mÃ³viles...\n",
      "   ğŸ“Š Variables a procesar: 20\n",
      "   â±ï¸  Ventanas temporales: ['6H', '24H', '72H']\n",
      "   ğŸ“‹ EstadÃ­sticas: ['mean', 'std', 'min', 'max']\n",
      "\n",
      "   ğŸ”„ Procesando ventana 6H...\n",
      "\n",
      "   ğŸ”„ Procesando ventana 24H...\n",
      "   âš ï¸  LÃ­mite de 150 features alcanzado\n",
      "âœ… Rolling features creadas: 150\n",
      "ğŸ“ Dimensiones: 19,752 Ã— 150\n",
      "\n",
      "ğŸ’¾ Memoria rolling features: 22.8 MB\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n para crear caracterÃ­sticas de ventanas mÃ³viles optimizada\n",
    "def crear_rolling_features(df, ventanas=['6H', '24H', '72H'], \n",
    "                          estadisticas=['mean', 'std', 'min', 'max'], \n",
    "                          variables_prioritarias=None, max_features=200):\n",
    "    \"\"\"\n",
    "    Crea caracterÃ­sticas de ventanas mÃ³viles para mÃºltiples estadÃ­sticas\n",
    "    \n",
    "    ParÃ¡metros:\n",
    "    - df: DataFrame con series temporales\n",
    "    - ventanas: Lista de ventanas temporales (ej: ['6H', '24H'])\n",
    "    - estadisticas: Lista de estadÃ­sticas a calcular\n",
    "    - variables_prioritarias: Variables especÃ­ficas a procesar\n",
    "    - max_features: LÃ­mite mÃ¡ximo de features a crear\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ˆ Creando caracterÃ­sticas de ventanas mÃ³viles...\")\n",
    "    \n",
    "    if variables_prioritarias is None:\n",
    "        # Seleccionar automÃ¡ticamente variables numÃ©ricas\n",
    "        variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        variables_prioritarias = variables_numericas[:20]  # Primeras 20\n",
    "    \n",
    "    print(f\"   ğŸ“Š Variables a procesar: {len(variables_prioritarias)}\")\n",
    "    print(f\"   â±ï¸  Ventanas temporales: {ventanas}\")\n",
    "    print(f\"   ğŸ“‹ EstadÃ­sticas: {estadisticas}\")\n",
    "    \n",
    "    rolling_features = pd.DataFrame(index=df.index)\n",
    "    contador_features = 0\n",
    "    \n",
    "    for ventana in ventanas:\n",
    "        print(f\"\\n   ğŸ”„ Procesando ventana {ventana}...\")\n",
    "        \n",
    "        for variable in variables_prioritarias:\n",
    "            if contador_features >= max_features:\n",
    "                print(f\"   âš ï¸  LÃ­mite de {max_features} features alcanzado\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Crear rolling window\n",
    "                rolling = df[variable].rolling(window=ventana, min_periods=1)\n",
    "                \n",
    "                for stat in estadisticas:\n",
    "                    if contador_features >= max_features:\n",
    "                        break\n",
    "                        \n",
    "                    nombre_feature = f\"{variable}_roll_{ventana}_{stat}\"\n",
    "                    \n",
    "                    if stat == 'mean':\n",
    "                        rolling_features[nombre_feature] = rolling.mean()\n",
    "                    elif stat == 'std':\n",
    "                        rolling_features[nombre_feature] = rolling.std()\n",
    "                    elif stat == 'min':\n",
    "                        rolling_features[nombre_feature] = rolling.min()\n",
    "                    elif stat == 'max':\n",
    "                        rolling_features[nombre_feature] = rolling.max()\n",
    "                    elif stat == 'median':\n",
    "                        rolling_features[nombre_feature] = rolling.median()\n",
    "                    elif stat == 'skew':\n",
    "                        rolling_features[nombre_feature] = rolling.skew()\n",
    "                    elif stat == 'kurt':\n",
    "                        rolling_features[nombre_feature] = rolling.kurt()\n",
    "                    \n",
    "                    contador_features += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      âš ï¸  Error con {variable}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if contador_features >= max_features:\n",
    "            break\n",
    "    \n",
    "    print(f\"âœ… Rolling features creadas: {contador_features}\")\n",
    "    print(f\"ğŸ“ Dimensiones: {rolling_features.shape[0]:,} Ã— {rolling_features.shape[1]}\")\n",
    "    \n",
    "    return rolling_features\n",
    "\n",
    "# Ejecutar creaciÃ³n de rolling features\n",
    "print(\"\\nğŸ—ï¸ INICIANDO CREACIÃ“N DE CARACTERÃSTICAS TEMPORALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    rolling_features = crear_rolling_features(\n",
    "        df=df, \n",
    "        ventanas=['6H', '24H', '72H'],\n",
    "        estadisticas=['mean', 'std', 'min', 'max'],\n",
    "        max_features=150\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Memoria rolling features: {rolling_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creando rolling features: {str(e)}\")\n",
    "    rolling_features = pd.DataFrame(index=df.index)  # DataFrame vacÃ­o como fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âª CaracterÃ­sticas de Lag (Retrasos Temporales)\n",
    "\n",
    "Los features de lag son esenciales para **modelar la memoria temporal** del sistema, permitiendo al modelo acceder a estados histÃ³ricos del equipo para predecir comportamientos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âª INICIANDO CREACIÃ“N DE LAG FEATURES CORREGIDA\n",
      "============================================================\n",
      "âª Creando caracterÃ­sticas de lag (retrasos temporales) - VERSIÃ“N CORREGIDA...\n",
      "   ğŸ“Š Variables a procesar: 12\n",
      "   â±ï¸  Lags temporales: ['2H', '12H', '48H']\n",
      "   ğŸ” Analizando frecuencia temporal del dataset...\n",
      "   â±ï¸  Frecuencia detectada: h\n",
      "   â±ï¸  Frecuencia promedio: 0 days 01:00:00\n",
      "\n",
      "   ğŸ”„ Procesando lag 2H...\n",
      "      âš ï¸  MÃ©todo 1 fallÃ³, usando promedio: 2H = 2 perÃ­odos\n",
      "      âœ… Features creados para 2H: 12\n",
      "\n",
      "   ğŸ”„ Procesando lag 12H...\n",
      "      âš ï¸  MÃ©todo 1 fallÃ³, usando promedio: 12H = 12 perÃ­odos\n",
      "      âœ… Features creados para 12H: 12\n",
      "\n",
      "   ğŸ”„ Procesando lag 48H...\n",
      "      âš ï¸  MÃ©todo 1 fallÃ³, usando promedio: 48H = 48 perÃ­odos\n",
      "      âœ… Features creados para 48H: 12\n",
      "\n",
      "   ğŸ“Š Creando caracterÃ­sticas de diferencias...\n",
      "      âœ… CaracterÃ­sticas de diferencias creadas: 16\n",
      "\n",
      "   ğŸ§¹ Aplicando limpieza final...\n",
      "      ğŸ—‘ï¸  Eliminados 5 features problemÃ¡ticos\n",
      "âœ… Lag features creadas exitosamente: 52 features totales\n",
      "ğŸ“ Dimensiones finales: 19,752 Ã— 47\n",
      "ğŸ“ˆ Completitud promedio: 99.9%\n",
      "\n",
      "ğŸ’¾ Memoria lag features: 7.2 MB\n",
      "ğŸ¯ LAG FEATURES CREADAS EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n para crear caracterÃ­sticas de lag CORREGIDA\n",
    "def crear_lag_features(df, lags=['2H', '12H', '48H'], \n",
    "                      variables_prioritarias=None, max_features=100):\n",
    "    \"\"\"\n",
    "    Crea caracterÃ­sticas de lag (retrasos temporales) con manejo robusto de errores\n",
    "    \n",
    "    CORRECCIÃ“N: Calcula perÃ­odos enteros basÃ¡ndose en la frecuencia del Ã­ndice\n",
    "    para evitar el error \"unit abbreviation w/o a number\"\n",
    "    \"\"\"\n",
    "    print(\"âª Creando caracterÃ­sticas de lag (retrasos temporales) - VERSIÃ“N CORREGIDA...\")\n",
    "    \n",
    "    if variables_prioritarias is None:\n",
    "        variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        variables_prioritarias = variables_numericas[:12]  # Primeras 12\n",
    "    \n",
    "    print(f\"   ğŸ“Š Variables a procesar: {len(variables_prioritarias)}\")\n",
    "    print(f\"   â±ï¸  Lags temporales: {lags}\")\n",
    "    \n",
    "    # Detectar frecuencia del Ã­ndice\n",
    "    print(f\"   ğŸ” Analizando frecuencia temporal del dataset...\")\n",
    "    freq_detectada = pd.infer_freq(df.index)\n",
    "    \n",
    "    # Calcular diferencia temporal promedio como fallback\n",
    "    if len(df.index) > 1:\n",
    "        time_diffs = df.index[1:] - df.index[:-1]\n",
    "        freq_promedio = time_diffs.median()\n",
    "        print(f\"   â±ï¸  Frecuencia detectada: {freq_detectada}\")\n",
    "        print(f\"   â±ï¸  Frecuencia promedio: {freq_promedio}\")\n",
    "    else:\n",
    "        freq_promedio = pd.Timedelta('1H')  # Default\n",
    "        print(f\"   âš ï¸  Usando frecuencia por defecto: {freq_promedio}\")\n",
    "    \n",
    "    lag_features = pd.DataFrame(index=df.index)\n",
    "    contador_features = 0\n",
    "    \n",
    "    for lag_str in lags:\n",
    "        print(f\"\\n   ğŸ”„ Procesando lag {lag_str}...\")\n",
    "        \n",
    "        try:\n",
    "            # CORRECCIÃ“N PRINCIPAL: Convertir lag string a timedelta y calcular perÃ­odos\n",
    "            lag_timedelta = pd.Timedelta(lag_str)\n",
    "            \n",
    "            # MÃ©todo 1: Usar frecuencia detectada\n",
    "            if freq_detectada:\n",
    "                try:\n",
    "                    freq_td = pd.Timedelta(freq_detectada)\n",
    "                    lag_periods = int(lag_timedelta / freq_td)\n",
    "                    print(f\"      âœ… MÃ©todo 1 exitoso: {lag_str} = {lag_periods} perÃ­odos (freq: {freq_detectada})\")\n",
    "                except:\n",
    "                    lag_periods = int(lag_timedelta / freq_promedio)\n",
    "                    print(f\"      âš ï¸  MÃ©todo 1 fallÃ³, usando promedio: {lag_str} = {lag_periods} perÃ­odos\")\n",
    "            else:\n",
    "                # MÃ©todo 2: Usar frecuencia promedio\n",
    "                lag_periods = int(lag_timedelta / freq_promedio)\n",
    "                print(f\"      âœ… MÃ©todo 2: {lag_str} = {lag_periods} perÃ­odos (freq promedio)\")\n",
    "            \n",
    "            # Validar que lag_periods sea razonable\n",
    "            if lag_periods <= 0:\n",
    "                print(f\"      âŒ PerÃ­odos invÃ¡lidos ({lag_periods}), saltando lag {lag_str}\")\n",
    "                continue\n",
    "            elif lag_periods > len(df):\n",
    "                print(f\"      âš ï¸  PerÃ­odos muy altos ({lag_periods} > {len(df)}), ajustando a {len(df)//4}\")\n",
    "                lag_periods = len(df) // 4\n",
    "            \n",
    "            # Crear lag features para cada variable\n",
    "            features_creados_lag = 0\n",
    "            for variable in variables_prioritarias:\n",
    "                if contador_features >= max_features:\n",
    "                    print(f\"   âš ï¸  LÃ­mite global de {max_features} features alcanzado\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # CORRECCIÃ“N: Usar shift() con perÃ­odos enteros\n",
    "                    nombre_feature = f\"{variable}_lag_{lag_str}\"\n",
    "                    lag_features[nombre_feature] = df[variable].shift(lag_periods)\n",
    "                    contador_features += 1\n",
    "                    features_creados_lag += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      âš ï¸  Error con {variable}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"      âœ… Features creados para {lag_str}: {features_creados_lag}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error procesando lag {lag_str}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        if contador_features >= max_features:\n",
    "            break\n",
    "    \n",
    "    # Crear caracterÃ­sticas de diferencias con manejo robusto\n",
    "    print(f\"\\n   ğŸ“Š Creando caracterÃ­sticas de diferencias...\")\n",
    "    \n",
    "    diferencias_creadas = 0\n",
    "    for variable in variables_prioritarias[:8]:  # Limitar a primeras 8 variables\n",
    "        if contador_features >= max_features:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Diferencia simple (mÃ©todo mÃ¡s robusto)\n",
    "            diff_periods_12h = max(1, int(pd.Timedelta('12H') / freq_promedio))\n",
    "            diff_name = f\"{variable}_diff_12H\"\n",
    "            lag_features[diff_name] = df[variable] - df[variable].shift(diff_periods_12h)\n",
    "            \n",
    "            # Cambio porcentual\n",
    "            pct_periods_24h = max(1, int(pd.Timedelta('24H') / freq_promedio))\n",
    "            pct_name = f\"{variable}_pct_change_24H\"\n",
    "            \n",
    "            # Usar mÃ©todo robusto para cambio porcentual\n",
    "            previous_values = df[variable].shift(pct_periods_24h)\n",
    "            current_values = df[variable]\n",
    "            \n",
    "            # Evitar divisiÃ³n por cero\n",
    "            lag_features[pct_name] = np.where(\n",
    "                previous_values != 0,\n",
    "                (current_values - previous_values) / np.abs(previous_values) * 100,\n",
    "                0  # Cuando el valor anterior es 0\n",
    "            )\n",
    "            \n",
    "            contador_features += 2\n",
    "            diferencias_creadas += 2\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âš ï¸  Error con diferencias de {variable}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"      âœ… CaracterÃ­sticas de diferencias creadas: {diferencias_creadas}\")\n",
    "    \n",
    "    # Limpieza final\n",
    "    print(f\"\\n   ğŸ§¹ Aplicando limpieza final...\")\n",
    "    features_antes = lag_features.shape[1]\n",
    "    \n",
    "    # Eliminar columnas con demasiados NaN\n",
    "    threshold_nan = int(0.8 * len(lag_features))  # Permitir hasta 80% de NaN\n",
    "    lag_features = lag_features.dropna(axis=1, thresh=threshold_nan)\n",
    "    \n",
    "    # Eliminar columnas constantes\n",
    "    for col in lag_features.columns:\n",
    "        if lag_features[col].nunique() <= 1:\n",
    "            lag_features = lag_features.drop(col, axis=1)\n",
    "    \n",
    "    features_despues = lag_features.shape[1]\n",
    "    if features_antes != features_despues:\n",
    "        print(f\"      ğŸ—‘ï¸  Eliminados {features_antes - features_despues} features problemÃ¡ticos\")\n",
    "    \n",
    "    print(f\"âœ… Lag features creadas exitosamente: {contador_features} features totales\")\n",
    "    print(f\"ğŸ“ Dimensiones finales: {lag_features.shape[0]:,} Ã— {lag_features.shape[1]}\")\n",
    "    \n",
    "    # InformaciÃ³n de calidad\n",
    "    if lag_features.shape[1] > 0:\n",
    "        completitud = (lag_features.count().sum() / (lag_features.shape[0] * lag_features.shape[1])) * 100\n",
    "        print(f\"ğŸ“ˆ Completitud promedio: {completitud:.1f}%\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  No se generaron lag features vÃ¡lidas\")\n",
    "    \n",
    "    return lag_features\n",
    "\n",
    "# Ejecutar creaciÃ³n de lag features CORREGIDA\n",
    "print(\"\\nâª INICIANDO CREACIÃ“N DE LAG FEATURES CORREGIDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    lag_features = crear_lag_features(\n",
    "        df=df, \n",
    "        lags=['2H', '12H', '48H'],\n",
    "        max_features=75\n",
    "    )\n",
    "    \n",
    "    if not lag_features.empty:\n",
    "        print(f\"\\nğŸ’¾ Memoria lag features: {lag_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        print(f\"ğŸ¯ LAG FEATURES CREADAS EXITOSAMENTE\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Dataset de lag features vacÃ­o - usando DataFrame bÃ¡sico como fallback\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creando lag features: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    lag_features = pd.DataFrame(index=df.index)  # DataFrame vacÃ­o como fallback\n",
    "    print(f\"âš ï¸  Usando fallback - DataFrame vacÃ­o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ¯ ConsolidaciÃ³n y PreparaciÃ³n del Dataset Final\n",
    "\n",
    "### ğŸ”— IntegraciÃ³n de CaracterÃ­sticas\n",
    "\n",
    "En esta fase crÃ­tica consolidamos todas las caracterÃ­sticas creadas en un dataset unificado, optimizado para el entrenamiento de modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Consolidando dataset final de caracterÃ­sticas...\n",
      "============================================================\n",
      "ğŸ“Š Variables originales incluidas: 25\n",
      "ğŸ“ˆ Rolling features agregadas: 80\n",
      "âª Lag features agregadas: 40\n",
      "\n",
      "ğŸ§¹ Limpieza final del dataset...\n",
      "   ğŸ—‘ï¸  Eliminadas 1 columnas problemÃ¡ticas\n",
      "\n",
      "ğŸ“Š DATASET FINAL CONSOLIDADO:\n",
      "   ğŸ“ Dimensiones: 19,752 filas Ã— 144 columnas\n",
      "   ğŸ“… PerÃ­odo: 2023-01-11 00:00:00 â†’ 2025-04-12 23:00:00\n",
      "   ğŸ’¾ Memoria: 11.0 MB\n",
      "   ğŸ”¢ Variables numÃ©ricas: 144\n",
      "\n",
      "ğŸ“‹ ComposiciÃ³n del dataset final:\n",
      "   ğŸ”µ Variables originales: 25\n",
      "   ğŸ“ˆ Rolling features: 80\n",
      "   âª Lag features: 40\n",
      "\n",
      "âœ… Calidad del dataset final:\n",
      "   ğŸ“ˆ Completitud promedio: 100.0%\n",
      "   ğŸ¯ CALIDAD EXCELENTE - Dataset listo para modelado\n"
     ]
    }
   ],
   "source": [
    "# ConsolidaciÃ³n final del dataset de caracterÃ­sticas\n",
    "print(\"ğŸ”— Consolidando dataset final de caracterÃ­sticas...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Seleccionar variables originales mÃ¡s importantes\n",
    "    variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    variables_importantes = variables_numericas[:25]  # Top 25 variables originales\n",
    "    \n",
    "    # Crear dataset base con variables originales seleccionadas\n",
    "    dataset_final = df[variables_importantes].copy()\n",
    "    print(f\"ğŸ“Š Variables originales incluidas: {dataset_final.shape[1]}\")\n",
    "    \n",
    "    # Agregar rolling features (filtrar por variabilidad)\n",
    "    if not rolling_features.empty:\n",
    "        # Filtrar rolling features con variabilidad significativa\n",
    "        rolling_features_filtered = rolling_features.dropna(axis=1, thresh=int(0.7 * len(rolling_features)))\n",
    "        rolling_features_filtered = rolling_features_filtered.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Seleccionar solo features con variaciÃ³n suficiente\n",
    "        features_con_variacion = []\n",
    "        for col in rolling_features_filtered.columns:\n",
    "            if rolling_features_filtered[col].std() > 1e-6:  # Evitar features constantes\n",
    "                features_con_variacion.append(col)\n",
    "        \n",
    "        rolling_features_final = rolling_features_filtered[features_con_variacion[:80]]  # Top 80\n",
    "        dataset_final = pd.concat([dataset_final, rolling_features_final], axis=1)\n",
    "        print(f\"ğŸ“ˆ Rolling features agregadas: {rolling_features_final.shape[1]}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  No se agregaron rolling features (dataset vacÃ­o)\")\n",
    "    \n",
    "    # Agregar lag features (filtrar por variabilidad)\n",
    "    if not lag_features.empty:\n",
    "        # Filtrar lag features con variabilidad significativa\n",
    "        lag_features_filtered = lag_features.dropna(axis=1, thresh=int(0.5 * len(lag_features)))\n",
    "        lag_features_filtered = lag_features_filtered.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Seleccionar solo features con variaciÃ³n suficiente\n",
    "        features_con_variacion = []\n",
    "        for col in lag_features_filtered.columns:\n",
    "            if lag_features_filtered[col].std() > 1e-6:\n",
    "                features_con_variacion.append(col)\n",
    "        \n",
    "        lag_features_final = lag_features_filtered[features_con_variacion[:40]]  # Top 40\n",
    "        dataset_final = pd.concat([dataset_final, lag_features_final], axis=1)\n",
    "        print(f\"âª Lag features agregadas: {lag_features_final.shape[1]}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  No se agregaron lag features (dataset vacÃ­o)\")\n",
    "    \n",
    "    print(f\"\\nğŸ§¹ Limpieza final del dataset...\")\n",
    "    \n",
    "    # Eliminar columnas completamente nulas o constantes\n",
    "    columnas_antes = dataset_final.shape[1]\n",
    "    dataset_final = dataset_final.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Eliminar features prÃ¡cticamente constantes\n",
    "    for col in dataset_final.select_dtypes(include=[np.number]).columns:\n",
    "        if dataset_final[col].std() < 1e-10:  # PrÃ¡cticamente constante\n",
    "            dataset_final = dataset_final.drop(col, axis=1)\n",
    "    \n",
    "    columnas_despues = dataset_final.shape[1]\n",
    "    if columnas_antes != columnas_despues:\n",
    "        print(f\"   ğŸ—‘ï¸  Eliminadas {columnas_antes - columnas_despues} columnas problemÃ¡ticas\")\n",
    "    \n",
    "    # Convertir a float32 para optimizar memoria\n",
    "    numeric_columns = dataset_final.select_dtypes(include=[np.number]).columns\n",
    "    dataset_final[numeric_columns] = dataset_final[numeric_columns].astype(np.float32)\n",
    "    \n",
    "    # InformaciÃ³n final del dataset\n",
    "    print(f\"\\nğŸ“Š DATASET FINAL CONSOLIDADO:\")\n",
    "    print(f\"   ğŸ“ Dimensiones: {dataset_final.shape[0]:,} filas Ã— {dataset_final.shape[1]} columnas\")\n",
    "    print(f\"   ğŸ“… PerÃ­odo: {dataset_final.index.min()} â†’ {dataset_final.index.max()}\")\n",
    "    print(f\"   ğŸ’¾ Memoria: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"   ğŸ”¢ Variables numÃ©ricas: {len(dataset_final.select_dtypes(include=[np.number]).columns)}\")\n",
    "    \n",
    "    # Mostrar composiciÃ³n del dataset\n",
    "    print(f\"\\nğŸ“‹ ComposiciÃ³n del dataset final:\")\n",
    "    print(f\"   ğŸ”µ Variables originales: {len(variables_importantes)}\")\n",
    "    if 'rolling_features_final' in locals():\n",
    "        print(f\"   ğŸ“ˆ Rolling features: {rolling_features_final.shape[1]}\")\n",
    "    if 'lag_features_final' in locals():\n",
    "        print(f\"   âª Lag features: {lag_features_final.shape[1]}\")\n",
    "    \n",
    "    # Verificar calidad del dataset final\n",
    "    completitud_final = (dataset_final.count().sum() / (dataset_final.shape[0] * dataset_final.shape[1])) * 100\n",
    "    print(f\"\\nâœ… Calidad del dataset final:\")\n",
    "    print(f\"   ğŸ“ˆ Completitud promedio: {completitud_final:.1f}%\")\n",
    "    \n",
    "    if completitud_final >= 80:\n",
    "        print(f\"   ğŸ¯ CALIDAD EXCELENTE - Dataset listo para modelado\")\n",
    "    elif completitud_final >= 60:\n",
    "        print(f\"   âš ï¸  CALIDAD ACEPTABLE - Considerar imputaciÃ³n adicional\")\n",
    "    else:\n",
    "        print(f\"   âŒ CALIDAD INSUFICIENTE - Requiere procesamiento adicional\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error en consolidaciÃ³n del dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Crear dataset bÃ¡sico como fallback\n",
    "    dataset_final = df.select_dtypes(include=[np.number]).copy()\n",
    "    print(f\"âš ï¸  Usando dataset bÃ¡sico como fallback: {dataset_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ’¾ Guardado y FinalizaciÃ³n\n",
    "\n",
    "### ğŸ“ Persistencia del Dataset de CaracterÃ­sticas\n",
    "\n",
    "Guardamos el dataset final optimizado en mÃºltiples formatos para garantizar compatibilidad y eficiencia en las fases posteriores de modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Guardando dataset final de caracterÃ­sticas...\n",
      "============================================================\n",
      "âœ… Parquet guardado: features_dataset_20250806_215732.parquet (10.9 MB)\n",
      "âœ… CSV guardado: features_dataset_20250806_215732.csv (26.1 MB)\n",
      "âœ… Metadatos guardados: features_dataset_20250806_215732_metadata.txt\n",
      "âœ… EstadÃ­sticas guardadas: features_dataset_20250806_215732_statistics.csv\n",
      "\n",
      "ğŸ¯ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\n",
      "ğŸ“ Archivos generados en data/processed:\n",
      "   ğŸ“¦ features_dataset_20250806_215732.parquet - Dataset principal (comprimido)\n",
      "   ğŸ“„ features_dataset_20250806_215732.csv - Dataset principal (CSV)\n",
      "   ğŸ“‹ features_dataset_20250806_215732_metadata.txt - Metadatos completos\n",
      "   ğŸ“Š features_dataset_20250806_215732_statistics.csv - EstadÃ­sticas descriptivas\n",
      "ğŸ‰ IngenierÃ­a de CaracterÃ­sticas finalizado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Guardado del dataset final con metadatos completos\n",
    "print(\"ğŸ’¾ Guardando dataset final de caracterÃ­sticas...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configurar rutas de salida\n",
    "output_dir = ruta_processed\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Nombres de archivos de salida\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "base_name = f'features_dataset_{timestamp}'\n",
    "\n",
    "try:\n",
    "    # 1. Guardar en formato Parquet (optimizado)\n",
    "    archivo_parquet = output_dir / f'{base_name}.parquet'\n",
    "    dataset_final.to_parquet(archivo_parquet, engine='pyarrow', compression='snappy')\n",
    "    tamaÃ±o_parquet = archivo_parquet.stat().st_size / (1024 * 1024)\n",
    "    print(f\"âœ… Parquet guardado: {archivo_parquet.name} ({tamaÃ±o_parquet:.1f} MB)\")\n",
    "    \n",
    "    # 2. Guardar en formato CSV (compatibilidad)\n",
    "    archivo_csv = output_dir / f'{base_name}.csv'\n",
    "    dataset_final.to_csv(archivo_csv, encoding='utf-8')\n",
    "    tamaÃ±o_csv = archivo_csv.stat().st_size / (1024 * 1024)\n",
    "    print(f\"âœ… CSV guardado: {archivo_csv.name} ({tamaÃ±o_csv:.1f} MB)\")\n",
    "    \n",
    "    # 3. Generar archivo de metadatos\n",
    "    archivo_metadata = output_dir / f'{base_name}_metadata.txt'\n",
    "    with open(archivo_metadata, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"METADATOS DEL DATASET DE CARACTERÃSTICAS\\n\")\n",
    "        f.write(f\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Notebook: 03_feature_engineering_b.ipynb\\n\")\n",
    "        f.write(f\"\\nDimensiones: {dataset_final.shape[0]:,} Ã— {dataset_final.shape[1]}\\n\")\n",
    "        f.write(f\"PerÃ­odo temporal: {dataset_final.index.min()} â†’ {dataset_final.index.max()}\\n\")\n",
    "        f.write(f\"Memoria utilizada: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\\n\")\n",
    "        f.write(f\"\\nTipos de caracterÃ­sticas:\\n\")\n",
    "        f.write(f\"  - Variables originales: {len(variables_importantes)}\\n\")\n",
    "        if 'rolling_features_final' in locals():\n",
    "            f.write(f\"  - Rolling features: {rolling_features_final.shape[1]}\\n\")\n",
    "        if 'lag_features_final' in locals():\n",
    "            f.write(f\"  - Lag features: {lag_features_final.shape[1]}\\n\")\n",
    "        f.write(f\"\\nEventos de mantenimiento:\\n\")\n",
    "        if eventos_utilizables is not None and len(eventos_utilizables) > 0:\n",
    "            f.write(f\"  - Eventos utilizables: {len(eventos_utilizables)}\\n\")\n",
    "            f.write(f\"  - Compatibilidad temporal: {'Confirmada' if compatibilidad_temporal else 'No confirmada'}\\n\")\n",
    "        else:\n",
    "            f.write(f\"  - Sin eventos utilizables identificados\\n\")\n",
    "        \n",
    "        f.write(f\"\\nLista de columnas:\\n\")\n",
    "        for i, col in enumerate(dataset_final.columns, 1):\n",
    "            f.write(f\"  {i:3d}. {col}\\n\")\n",
    "    \n",
    "    print(f\"âœ… Metadatos guardados: {archivo_metadata.name}\")\n",
    "    \n",
    "    # 4. Generar resumen estadÃ­stico\n",
    "    archivo_stats = output_dir / f'{base_name}_statistics.csv'\n",
    "    stats_desc = dataset_final.describe()\n",
    "    stats_desc.to_csv(archivo_stats, encoding='utf-8')\n",
    "    print(f\"âœ… EstadÃ­sticas guardadas: {archivo_stats.name}\")\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\nğŸ¯ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "    print(f\"ğŸ“ Archivos generados en {output_dir}:\")\n",
    "    print(f\"   ğŸ“¦ {base_name}.parquet - Dataset principal (comprimido)\")\n",
    "    print(f\"   ğŸ“„ {base_name}.csv - Dataset principal (CSV)\")\n",
    "    print(f\"   ğŸ“‹ {base_name}_metadata.txt - Metadatos completos\")\n",
    "    print(f\"   ğŸ“Š {base_name}_statistics.csv - EstadÃ­sticas descriptivas\")\n",
    "    \n",
    "    print(f\"ğŸ‰ IngenierÃ­a de CaracterÃ­sticas finalizado exitosamente!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error al guardar dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Guardado de emergencia\n",
    "    backup_file = output_dir / f'features_dataset_emergency_backup.csv'\n",
    "    dataset_final.to_csv(backup_file)\n",
    "    print(f\"ğŸ’¾ Guardado de emergencia: {backup_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Resumen del Feature Engineering\n",
    "\n",
    "### ğŸ¯ Logros Alcanzados\n",
    "\n",
    "1. **âœ… Carga de datos exitosa** - IntegraciÃ³n de series temporales y historial de eventos\n",
    "2. **âœ… ValidaciÃ³n temporal** - ConfirmaciÃ³n de compatibilidad entre datasets\n",
    "3. **âœ… CaracterÃ­sticas temporales** - CreaciÃ³n de rolling features y lag features\n",
    "4. **âœ… Dataset optimizado** - ConsolidaciÃ³n y optimizaciÃ³n de memoria\n",
    "5. **âœ… Persistencia completa** - Guardado en mÃºltiples formatos con metadatos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
