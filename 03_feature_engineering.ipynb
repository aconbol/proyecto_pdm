{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de Características para Mantenimiento Predictivo\n",
    "## Proyecto: Predicción de Fallas en Moto-Compresores - Oil & Gas\n",
    "\n",
    "### 🎯 Objetivo del Notebook\n",
    "\n",
    "Este notebook constituye la **fase crítica** de transformación de datos donde convertimos las series temporales limpias en un dataset enriquecido y etiquetado, optimizado para el entrenamiento de modelos de Machine Learning. Nuestro objetivo principal es **predecir fallas en moto-compresores con 7 días de antelación**, una ventana temporal que permite la planificación efectiva de mantenimientos preventivos en el sector Oil & Gas.\n",
    "\n",
    "### 📋 Tareas Principales\n",
    "\n",
    "1. **Carga y Validación de Datos**: Integrar el dataset preprocesado con el historial de eventos\n",
    "2. **Ingeniería de Características Temporales**: Crear features que capturen la dinámica del deterioro\n",
    "3. **Características Avanzadas**: Implementar features de tasas de cambio, frecuencia y detección de anomalías\n",
    "4. **Etiquetado de Fallas**: Crear la variable objetivo basada en ventanas de pre-falla de 7 días\n",
    "5. **Validación y Preparación Final**: Garantizar calidad de datos para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías importadas exitosamente\n",
      "📊 Versión de pandas: 2.3.1\n",
      "🔢 Versión de numpy: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías esenciales para ingeniería de características\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Librerías especializadas para análisis de señales y anomalías\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Configuración del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"✅ Librerías importadas exitosamente\")\n",
    "print(f\"📊 Versión de pandas: {pd.__version__}\")\n",
    "print(f\"🔢 Versión de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Validación de rutas y archivos:\n",
      "   Datos procesados: data/processed - ✅ Existe\n",
      "   Eventos: eventos - ✅ Existe\n",
      "   Timeseries: data/processed/timeseries_data_temporal_fixed.parquet - ✅ Existe\n",
      "   Historial: eventos/Historial C1 RGD.xlsx - ✅ Existe\n",
      "\n",
      "✅ Todos los archivos requeridos están disponibles\n"
     ]
    }
   ],
   "source": [
    "# Configuración de rutas de datos con validación de existencia\n",
    "# Esta configuración garantiza la reproducibilidad del pipeline\n",
    "\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('.')\n",
    "\n",
    "# Rutas específicas para datos procesados y eventos\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Archivos específicos requeridos\n",
    "archivo_timeseries = ruta_processed / 'timeseries_data_temporal_fixed.parquet'\n",
    "archivo_historial = ruta_eventos / 'Historial C1 RGD.xlsx'\n",
    "\n",
    "# Validación crítica de existencia de archivos\n",
    "print(\"📁 Validación de rutas y archivos:\")\n",
    "print(f\"   Datos procesados: {ruta_processed} - {'✅ Existe' if ruta_processed.exists() else '❌ No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'✅ Existe' if ruta_eventos.exists() else '❌ No existe'}\")\n",
    "print(f\"   Timeseries: {archivo_timeseries} - {'✅ Existe' if archivo_timeseries.exists() else '❌ No existe'}\")\n",
    "print(f\"   Historial: {archivo_historial} - {'✅ Existe' if archivo_historial.exists() else '❌ No existe'}\")\n",
    "\n",
    "# Verificación crítica - detener ejecución si faltan archivos esenciales\n",
    "archivos_requeridos = [archivo_timeseries, archivo_historial]\n",
    "archivos_faltantes = [arch for arch in archivos_requeridos if not arch.exists()]\n",
    "\n",
    "if archivos_faltantes:\n",
    "    print(f\"\\n❌ ERROR CRÍTICO: Faltan archivos esenciales:\")\n",
    "    for archivo in archivos_faltantes:\n",
    "        print(f\"   - {archivo}\")\n",
    "    raise FileNotFoundError(\"No se pueden continuar sin los archivos de datos requeridos\")\n",
    "else:\n",
    "    print(\"\\n✅ Todos los archivos requeridos están disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📂 Carga y Validación de Datos\n",
    "\n",
    "### 🔄 Proceso de Carga Inteligente\n",
    "\n",
    "En esta fase crítica, cargaremos tanto el **dataset de series temporales procesado** como el **historial de eventos de mantenimiento**. La calidad de este proceso determina directamente la efectividad de nuestro modelo predictivo.\n",
    "\n",
    "El dataset de series temporales contiene las mediciones continuas de sensores del moto-compresor, ya limpias y preprocesadas. El historial de eventos proporciona las fechas exactas de las fallas históricas, información esencial para crear nuestras etiquetas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Cargando dataset principal de series temporales...\n",
      "✅ Dataset principal cargado exitosamente\n",
      "   📊 Dimensiones: 19,752 filas × 34 columnas\n",
      "   📅 Período temporal: 2023-01-11 00:00:00 → 2025-04-12 23:00:00\n",
      "   ⏱️  Frecuencia detectada: h\n",
      "   💾 Uso de memoria: 5.3 MB\n",
      "   📈 Completitud promedio: 97.1%\n",
      "   🔢 Tipos de datos: {dtype('float64'): np.int64(33), dtype('<M8[ns]'): np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# Carga optimizada del dataset principal de series temporales\n",
    "print(\"⚡ Cargando dataset principal de series temporales...\")\n",
    "\n",
    "try:\n",
    "    # Cargar el dataset principal con optimizaciones de memoria\n",
    "    df = pd.read_parquet(archivo_timeseries, engine='pyarrow')\n",
    "    \n",
    "    # Validaciones críticas inmediatas\n",
    "    if df.empty:\n",
    "        raise ValueError(\"El dataset cargado está vacío\")\n",
    "    \n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"⚠️  Convirtiendo índice a DatetimeIndex\")\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Información básica del dataset\n",
    "    print(f\"✅ Dataset principal cargado exitosamente\")\n",
    "    print(f\"   📊 Dimensiones: {df.shape[0]:,} filas × {df.shape[1]} columnas\")\n",
    "    print(f\"   📅 Período temporal: {df.index.min()} → {df.index.max()}\")\n",
    "    print(f\"   ⏱️  Frecuencia detectada: {pd.infer_freq(df.index) or 'Variable/No detectada'}\")\n",
    "    print(f\"   💾 Uso de memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Análisis de calidad de datos\n",
    "    completitud_promedio = (df.count().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    print(f\"   📈 Completitud promedio: {completitud_promedio:.1f}%\")\n",
    "    \n",
    "    # Verificar tipos de datos\n",
    "    tipos_datos = df.dtypes.value_counts()\n",
    "    print(f\"   🔢 Tipos de datos: {dict(tipos_datos)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error crítico al cargar el dataset principal: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Cargando historial de eventos de mantenimiento...\n",
      "📊 Dimensiones del archivo raw: (92, 17)\n",
      "🔍 Analizando estructura para encontrar los headers reales...\n",
      "✅ Headers encontrados en fila 3: 7/7 coincidencias\n",
      "📊 DataFrame cargado con dimensiones: (88, 17)\n",
      "✅ No se detectaron columnas 'Unnamed', estructura correcta\n",
      "✅ Historial procesado exitosamente\n",
      "📊 Dimensiones finales: 88 eventos × 17 campos\n",
      "📋 Columnas finales: ['N°', 'NUMERO AVISO', 'CLASE DE ORDEN', 'DESCRIPCION', 'Fecha     ', 'FECHA INICIO', 'FECHA FIN', 'HORA INICIO', 'HORA FIN', 'DURACION PARADA', 'NUMERO DE ORDEN DE TRABAJO', 'CLASE OT', 'PUESTO DE TRABAJO', 'EQUIPO', 'UBICACIÓN TÉCNICA', 'Creado por  ', 'TEXTO AMPLIADO']\n",
      "\n",
      "🎯 ÉXITO: Historial cargado correctamente\n",
      "   📊 88 eventos cargados\n",
      "   📋 17 columnas identificadas correctamente\n",
      "\n",
      "📅 Columnas de fecha detectadas correctamente: ['Fecha     ', 'FECHA INICIO', 'FECHA FIN', 'HORA INICIO']\n",
      "🎯 Usando columna principal de fecha: 'Fecha     '\n",
      "   ✅ Conversión de fechas: 88/88 fechas válidas\n",
      "   📅 Rango de eventos: 2023-01-11 00:00:00 a 2025-05-21 00:00:00\n",
      "   📊 Eventos únicos: 58\n"
     ]
    }
   ],
   "source": [
    "# Carga y procesamiento del historial de eventos - FUNCIÓN CORREGIDA\n",
    "print(\"\\n🔄 Cargando historial de eventos de mantenimiento...\")\n",
    "\n",
    "def cargar_historial_eventos_corregido(archivo_historial):\n",
    "    \"\"\"\n",
    "    Función corregida que usa exactamente el mismo código que funcionaba en el EDA\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo sin header para analizar la estructura\n",
    "        df_raw = pd.read_excel(archivo_historial, header=None, engine='openpyxl')\n",
    "        \n",
    "        print(f\"📊 Dimensiones del archivo raw: {df_raw.shape}\")\n",
    "        print(f\"🔍 Analizando estructura para encontrar los headers reales...\")\n",
    "        \n",
    "        # Buscar la fila que contiene los headers reales\n",
    "        header_row = None\n",
    "        expected_headers = ['NUMERO', 'AVISO', 'CLASE', 'DESCRIPCION', 'FECHA', 'INICIO', 'FIN']\n",
    "        \n",
    "        for idx in range(min(10, len(df_raw))):  # Buscar en las primeras 10 filas\n",
    "            row = df_raw.iloc[idx]\n",
    "            row_str = ' '.join(str(cell).upper() for cell in row if pd.notna(cell))\n",
    "            \n",
    "            # Verificar si esta fila contiene los encabezados esperados\n",
    "            header_matches = sum(1 for header in expected_headers if header in row_str)\n",
    "            \n",
    "            if header_matches >= 3:  # Al menos 3 coincidencias\n",
    "                header_row = idx\n",
    "                print(f\"✅ Headers encontrados en fila {idx}: {header_matches}/{len(expected_headers)} coincidencias\")\n",
    "                break\n",
    "        \n",
    "        # Si no encontramos headers automáticamente, usar fila 3 por defecto (como en el EDA exitoso)\n",
    "        if header_row is None:\n",
    "            print(\"⚠️  No se encontraron encabezados automáticamente, usando fila 3 (índice 3)\")\n",
    "            header_row = 3\n",
    "        \n",
    "        # Cargar el archivo usando la fila de headers identificada\n",
    "        historial_df = pd.read_excel(archivo_historial, \n",
    "                                   header=header_row,\n",
    "                                   engine='openpyxl')\n",
    "        \n",
    "        print(f\"📊 DataFrame cargado con dimensiones: {historial_df.shape}\")\n",
    "        \n",
    "        # CORRECCIÓN CRÍTICA: Verificar si tenemos columnas 'Unnamed'\n",
    "        unnamed_cols = [col for col in historial_df.columns if 'Unnamed:' in str(col)]\n",
    "        if unnamed_cols:\n",
    "            print(f\"⚠️  Detectadas {len(unnamed_cols)} columnas 'Unnamed'\")\n",
    "            print(f\"🔧 Aplicando corrección de nombres de columna...\")\n",
    "            \n",
    "            # Los nombres reales están en la primera fila de datos\n",
    "            if len(historial_df) > 0:\n",
    "                new_column_names = []\n",
    "                first_row = historial_df.iloc[0]\n",
    "                \n",
    "                for i, col in enumerate(historial_df.columns):\n",
    "                    if 'Unnamed:' in str(col) and i < len(first_row):\n",
    "                        # Usar el valor de la primera fila como nombre de columna\n",
    "                        new_name = str(first_row.iloc[i]).strip() if pd.notna(first_row.iloc[i]) else f'Col_{i+1}'\n",
    "                        new_column_names.append(new_name)\n",
    "                    else:\n",
    "                        # Mantener nombres existentes pero limpiarlos\n",
    "                        clean_name = str(col).strip().replace('\\n', ' ').replace('\\r', '')\n",
    "                        new_column_names.append(clean_name if clean_name else f'Col_{i+1}')\n",
    "                \n",
    "                # Aplicar nuevos nombres\n",
    "                historial_df.columns = new_column_names\n",
    "                \n",
    "                # Eliminar la primera fila que contiene los nombres de columna\n",
    "                print(f\"🧹 Eliminando primera fila que contiene nombres de columna\")\n",
    "                historial_df = historial_df.iloc[1:].reset_index(drop=True)\n",
    "                \n",
    "                print(f\"✅ Corrección aplicada exitosamente\")\n",
    "        else:\n",
    "            print(f\"✅ No se detectaron columnas 'Unnamed', estructura correcta\")\n",
    "        \n",
    "        # Limpiar datos (como en el EDA exitoso)\n",
    "        historial_df = historial_df.dropna(how='all')  # Eliminar filas vacías\n",
    "        historial_df = historial_df.dropna(axis=1, how='all')  # Eliminar columnas vacías\n",
    "        \n",
    "        print(f\"✅ Historial procesado exitosamente\")\n",
    "        print(f\"📊 Dimensiones finales: {historial_df.shape[0]:,} eventos × {historial_df.shape[1]} campos\")\n",
    "        print(f\"📋 Columnas finales: {list(historial_df.columns)}\")\n",
    "        \n",
    "        return historial_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando historial: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Ejecutar la función corregida\n",
    "try:\n",
    "    df_eventos = cargar_historial_eventos_corregido(archivo_historial)\n",
    "    \n",
    "    if df_eventos is not None:\n",
    "        print(f\"\\n🎯 ÉXITO: Historial cargado correctamente\")\n",
    "        print(f\"   📊 {len(df_eventos)} eventos cargados\")\n",
    "        print(f\"   📋 {len(df_eventos.columns)} columnas identificadas correctamente\")\n",
    "        \n",
    "        # Identificar columnas de fecha correctamente\n",
    "        date_columns = [col for col in df_eventos.columns \n",
    "                       if any(keyword in col.upper() for keyword in ['FECHA', 'DATE', 'INICIO', 'START'])]\n",
    "        \n",
    "        print(f\"\\n📅 Columnas de fecha detectadas correctamente: {date_columns}\")\n",
    "        \n",
    "        if date_columns:\n",
    "            primary_date_col = date_columns[0]\n",
    "            print(f\"🎯 Usando columna principal de fecha: '{primary_date_col}'\")\n",
    "            \n",
    "            # Procesar fechas como en el EDA exitoso\n",
    "            try:\n",
    "                df_eventos['fecha_evento'] = pd.to_datetime(df_eventos[primary_date_col], errors='coerce')\n",
    "                fechas_validas = df_eventos['fecha_evento'].notna().sum()\n",
    "                total_fechas = len(df_eventos)\n",
    "                \n",
    "                print(f\"   ✅ Conversión de fechas: {fechas_validas}/{total_fechas} fechas válidas\")\n",
    "                \n",
    "                if fechas_validas > 0:\n",
    "                    fechas_limpias = df_eventos['fecha_evento'].dropna()\n",
    "                    print(f\"   📅 Rango de eventos: {fechas_limpias.min()} a {fechas_limpias.max()}\")\n",
    "                    print(f\"   📊 Eventos únicos: {fechas_limpias.nunique()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Error procesando fechas: {str(e)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Error: No se pudo cargar el historial de eventos\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error ejecutando función: {str(e)}\")\n",
    "    df_eventos = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Validando compatibilidad temporal entre datasets...\n",
      "📊 Análisis de cobertura temporal:\n",
      "   🔧 Datos de sensores: 2023-01-11 00:00:00 a 2025-04-12 23:00:00\n",
      "   📅 Eventos registrados: 2023-01-11 00:00:00 a 2025-05-21 00:00:00\n",
      "   ✅ Solapamiento detectado: 2023-01-11 00:00:00 a 2025-04-12 23:00:00\n",
      "   ⏱️  Duración del solapamiento: 822 days 23:00:00\n",
      "   🎯 Eventos utilizables para entrenamiento: 87\n",
      "   ✅ COMPATIBILIDAD TEMPORAL CONFIRMADA\n",
      "   📈 Distribución anual de eventos: {2023: np.int64(53), 2024: np.int64(30), 2025: np.int64(4)}\n",
      "   📊 Densidad de eventos: 0.1058 eventos/día\n",
      "\n",
      "📋 RESUMEN DE VALIDACIÓN TEMPORAL:\n",
      "   Compatibilidad temporal: ✅ CONFIRMADA\n",
      "   Eventos utilizables: 87\n",
      "   Duración de solapamiento: 822 days 23:00:00\n",
      "\n",
      "🎯 Listo para proceder con ingeniería de características\n"
     ]
    }
   ],
   "source": [
    "# Validación de compatibilidad temporal entre datasets - CORREGIDO\n",
    "print(\"\\n🔍 Validando compatibilidad temporal entre datasets...\")\n",
    "\n",
    "# Inicializar variables de control\n",
    "eventos_utilizables = None\n",
    "overlap_duration = None\n",
    "compatibilidad_temporal = False\n",
    "\n",
    "try:\n",
    "    # Validar pre-requisitos\n",
    "    if df is None or df.empty:\n",
    "        print(\"   ❌ Dataset principal no disponible\")\n",
    "    elif df_eventos is None or df_eventos.empty:\n",
    "        print(\"   ❌ Dataset de eventos no disponible\")\n",
    "    elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"   ❌ Índice del dataset principal no es de tipo fecha\")\n",
    "    elif 'fecha_evento' not in df_eventos.columns:\n",
    "        print(\"   ❌ No se encontró columna 'fecha_evento' en el historial\")\n",
    "    else:\n",
    "        # Análisis de rangos temporales\n",
    "        sensor_start, sensor_end = df.index.min(), df.index.max()\n",
    "        eventos_limpios = df_eventos['fecha_evento'].dropna()\n",
    "        \n",
    "        if eventos_limpios.empty:\n",
    "            print(\"   ❌ No hay eventos con fechas válidas\")\n",
    "        else:\n",
    "            eventos_start, eventos_end = eventos_limpios.min(), eventos_limpios.max()\n",
    "            \n",
    "            print(f\"📊 Análisis de cobertura temporal:\")\n",
    "            print(f\"   🔧 Datos de sensores: {sensor_start} a {sensor_end}\")\n",
    "            print(f\"   📅 Eventos registrados: {eventos_start} a {eventos_end}\")\n",
    "            \n",
    "            # Calcular solapamiento temporal\n",
    "            overlap_start = max(sensor_start, eventos_start)\n",
    "            overlap_end = min(sensor_end, eventos_end)\n",
    "            \n",
    "            if overlap_start <= overlap_end:\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                print(f\"   ✅ Solapamiento detectado: {overlap_start} a {overlap_end}\")\n",
    "                print(f\"   ⏱️  Duración del solapamiento: {overlap_duration}\")\n",
    "                \n",
    "                # Eventos que caen dentro del rango de datos de sensores\n",
    "                eventos_utilizables = eventos_limpios[\n",
    "                    (eventos_limpios >= sensor_start) & (eventos_limpios <= sensor_end)\n",
    "                ]\n",
    "                \n",
    "                print(f\"   🎯 Eventos utilizables para entrenamiento: {len(eventos_utilizables)}\")\n",
    "                \n",
    "                if len(eventos_utilizables) > 0:\n",
    "                    compatibilidad_temporal = True\n",
    "                    print(f\"   ✅ COMPATIBILIDAD TEMPORAL CONFIRMADA\")\n",
    "                    \n",
    "                    # Análisis de distribución temporal de eventos\n",
    "                    eventos_por_year = eventos_utilizables.dt.year.value_counts().sort_index()\n",
    "                    print(f\"   📈 Distribución anual de eventos: {dict(eventos_por_year)}\")\n",
    "                    \n",
    "                    # Estadísticas de densidad de eventos\n",
    "                    dias_totales = (sensor_end - sensor_start).days\n",
    "                    densidad_eventos = len(eventos_utilizables) / dias_totales if dias_totales > 0 else 0\n",
    "                    print(f\"   📊 Densidad de eventos: {densidad_eventos:.4f} eventos/día\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️  Sin eventos utilizables dentro del rango de sensores\")\n",
    "            else:\n",
    "                print(f\"   ❌ No hay solapamiento temporal entre datasets\")\n",
    "                print(f\"       Sensores terminan: {sensor_end}\")\n",
    "                print(f\"       Eventos inician: {eventos_start}\")\n",
    "                \n",
    "                # Análisis de eventos fuera de rango\n",
    "                eventos_antes = eventos_limpios[eventos_limpios < sensor_start]\n",
    "                eventos_despues = eventos_limpios[eventos_limpios > sensor_end]\n",
    "                print(f\"       Eventos antes del rango: {len(eventos_antes)}\")\n",
    "                print(f\"       Eventos después del rango: {len(eventos_despues)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error en validación temporal: {str(e)}\")\n",
    "    compatibilidad_temporal = False\n",
    "\n",
    "# Resumen ejecutivo de la validación\n",
    "print(f\"\\n📋 RESUMEN DE VALIDACIÓN TEMPORAL:\")\n",
    "print(f\"   Compatibilidad temporal: {'✅ CONFIRMADA' if compatibilidad_temporal else '❌ NO CONFIRMADA'}\")\n",
    "print(f\"   Eventos utilizables: {len(eventos_utilizables) if eventos_utilizables is not None else 0}\")\n",
    "print(f\"   Duración de solapamiento: {overlap_duration if overlap_duration else 'N/A'}\")\n",
    "\n",
    "# Configurar variables para las siguientes etapas\n",
    "if not compatibilidad_temporal:\n",
    "    print(f\"\\n⚠️  ADVERTENCIA: Sin compatibilidad temporal, se procederá con análisis limitado\")\n",
    "    eventos_utilizables = pd.Series(dtype='datetime64[ns]')  # Serie vacía\n",
    "else:\n",
    "    print(f\"\\n🎯 Listo para proceder con ingeniería de características\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🏗️ Ingeniería de Características Temporales\n",
    "\n",
    "### 📈 Características de Ventanas Móviles (Rolling Features)\n",
    "\n",
    "Las características de ventanas móviles son fundamentales para capturar la **dinámica temporal del deterioro** en equipos industriales. Implementaremos múltiples horizontes temporales para detectar tanto degradaciones lentas como cambios súbitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏗️ INICIANDO CREACIÓN DE CARACTERÍSTICAS TEMPORALES\n",
      "============================================================\n",
      "📈 Creando características de ventanas móviles...\n",
      "   📊 Variables a procesar: 20\n",
      "   ⏱️  Ventanas temporales: ['6H', '24H', '72H']\n",
      "   📋 Estadísticas: ['mean', 'std', 'min', 'max']\n",
      "\n",
      "   🔄 Procesando ventana 6H...\n",
      "\n",
      "   🔄 Procesando ventana 24H...\n",
      "   ⚠️  Límite de 150 features alcanzado\n",
      "✅ Rolling features creadas: 150\n",
      "📐 Dimensiones: 19,752 × 150\n",
      "\n",
      "💾 Memoria rolling features: 22.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Función para crear características de ventanas móviles optimizada\n",
    "def crear_rolling_features(df, ventanas=['6H', '24H', '72H'], \n",
    "                          estadisticas=['mean', 'std', 'min', 'max'], \n",
    "                          variables_prioritarias=None, max_features=200):\n",
    "    \"\"\"\n",
    "    Crea características de ventanas móviles para múltiples estadísticas\n",
    "    \n",
    "    Parámetros:\n",
    "    - df: DataFrame con series temporales\n",
    "    - ventanas: Lista de ventanas temporales (ej: ['6H', '24H'])\n",
    "    - estadisticas: Lista de estadísticas a calcular\n",
    "    - variables_prioritarias: Variables específicas a procesar\n",
    "    - max_features: Límite máximo de features a crear\n",
    "    \"\"\"\n",
    "    print(\"📈 Creando características de ventanas móviles...\")\n",
    "    \n",
    "    if variables_prioritarias is None:\n",
    "        # Seleccionar automáticamente variables numéricas\n",
    "        variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        variables_prioritarias = variables_numericas[:20]  # Primeras 20\n",
    "    \n",
    "    print(f\"   📊 Variables a procesar: {len(variables_prioritarias)}\")\n",
    "    print(f\"   ⏱️  Ventanas temporales: {ventanas}\")\n",
    "    print(f\"   📋 Estadísticas: {estadisticas}\")\n",
    "    \n",
    "    rolling_features = pd.DataFrame(index=df.index)\n",
    "    contador_features = 0\n",
    "    \n",
    "    for ventana in ventanas:\n",
    "        print(f\"\\n   🔄 Procesando ventana {ventana}...\")\n",
    "        \n",
    "        for variable in variables_prioritarias:\n",
    "            if contador_features >= max_features:\n",
    "                print(f\"   ⚠️  Límite de {max_features} features alcanzado\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Crear rolling window\n",
    "                rolling = df[variable].rolling(window=ventana, min_periods=1)\n",
    "                \n",
    "                for stat in estadisticas:\n",
    "                    if contador_features >= max_features:\n",
    "                        break\n",
    "                        \n",
    "                    nombre_feature = f\"{variable}_roll_{ventana}_{stat}\"\n",
    "                    \n",
    "                    if stat == 'mean':\n",
    "                        rolling_features[nombre_feature] = rolling.mean()\n",
    "                    elif stat == 'std':\n",
    "                        rolling_features[nombre_feature] = rolling.std()\n",
    "                    elif stat == 'min':\n",
    "                        rolling_features[nombre_feature] = rolling.min()\n",
    "                    elif stat == 'max':\n",
    "                        rolling_features[nombre_feature] = rolling.max()\n",
    "                    elif stat == 'median':\n",
    "                        rolling_features[nombre_feature] = rolling.median()\n",
    "                    elif stat == 'skew':\n",
    "                        rolling_features[nombre_feature] = rolling.skew()\n",
    "                    elif stat == 'kurt':\n",
    "                        rolling_features[nombre_feature] = rolling.kurt()\n",
    "                    \n",
    "                    contador_features += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      ⚠️  Error con {variable}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if contador_features >= max_features:\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Rolling features creadas: {contador_features}\")\n",
    "    print(f\"📐 Dimensiones: {rolling_features.shape[0]:,} × {rolling_features.shape[1]}\")\n",
    "    \n",
    "    return rolling_features\n",
    "\n",
    "# Ejecutar creación de rolling features\n",
    "print(\"\\n🏗️ INICIANDO CREACIÓN DE CARACTERÍSTICAS TEMPORALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    rolling_features = crear_rolling_features(\n",
    "        df=df, \n",
    "        ventanas=['6H', '24H', '72H'],\n",
    "        estadisticas=['mean', 'std', 'min', 'max'],\n",
    "        max_features=150\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n💾 Memoria rolling features: {rolling_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creando rolling features: {str(e)}\")\n",
    "    rolling_features = pd.DataFrame(index=df.index)  # DataFrame vacío como fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏪ Características de Lag (Retrasos Temporales)\n",
    "\n",
    "Los features de lag son esenciales para **modelar la memoria temporal** del sistema, permitiendo al modelo acceder a estados históricos del equipo para predecir comportamientos futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏪ INICIANDO CREACIÓN DE LAG FEATURES CORREGIDA\n",
      "============================================================\n",
      "⏪ Creando características de lag (retrasos temporales) - VERSIÓN CORREGIDA...\n",
      "   📊 Variables a procesar: 12\n",
      "   ⏱️  Lags temporales: ['2H', '12H', '48H']\n",
      "   🔍 Analizando frecuencia temporal del dataset...\n",
      "   ⏱️  Frecuencia detectada: h\n",
      "   ⏱️  Frecuencia promedio: 0 days 01:00:00\n",
      "\n",
      "   🔄 Procesando lag 2H...\n",
      "      ⚠️  Método 1 falló, usando promedio: 2H = 2 períodos\n",
      "      ✅ Features creados para 2H: 12\n",
      "\n",
      "   🔄 Procesando lag 12H...\n",
      "      ⚠️  Método 1 falló, usando promedio: 12H = 12 períodos\n",
      "      ✅ Features creados para 12H: 12\n",
      "\n",
      "   🔄 Procesando lag 48H...\n",
      "      ⚠️  Método 1 falló, usando promedio: 48H = 48 períodos\n",
      "      ✅ Features creados para 48H: 12\n",
      "\n",
      "   📊 Creando características de diferencias...\n",
      "      ✅ Características de diferencias creadas: 16\n",
      "\n",
      "   🧹 Aplicando limpieza final...\n",
      "      🗑️  Eliminados 5 features problemáticos\n",
      "✅ Lag features creadas exitosamente: 52 features totales\n",
      "📐 Dimensiones finales: 19,752 × 47\n",
      "📈 Completitud promedio: 99.9%\n",
      "\n",
      "💾 Memoria lag features: 7.2 MB\n",
      "🎯 LAG FEATURES CREADAS EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# Función para crear características de lag CORREGIDA\n",
    "def crear_lag_features(df, lags=['2H', '12H', '48H'], \n",
    "                      variables_prioritarias=None, max_features=100):\n",
    "    \"\"\"\n",
    "    Crea características de lag (retrasos temporales) con manejo robusto de errores\n",
    "    \n",
    "    CORRECCIÓN: Calcula períodos enteros basándose en la frecuencia del índice\n",
    "    para evitar el error \"unit abbreviation w/o a number\"\n",
    "    \"\"\"\n",
    "    print(\"⏪ Creando características de lag (retrasos temporales) - VERSIÓN CORREGIDA...\")\n",
    "    \n",
    "    if variables_prioritarias is None:\n",
    "        variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        variables_prioritarias = variables_numericas[:12]  # Primeras 12\n",
    "    \n",
    "    print(f\"   📊 Variables a procesar: {len(variables_prioritarias)}\")\n",
    "    print(f\"   ⏱️  Lags temporales: {lags}\")\n",
    "    \n",
    "    # Detectar frecuencia del índice\n",
    "    print(f\"   🔍 Analizando frecuencia temporal del dataset...\")\n",
    "    freq_detectada = pd.infer_freq(df.index)\n",
    "    \n",
    "    # Calcular diferencia temporal promedio como fallback\n",
    "    if len(df.index) > 1:\n",
    "        time_diffs = df.index[1:] - df.index[:-1]\n",
    "        freq_promedio = time_diffs.median()\n",
    "        print(f\"   ⏱️  Frecuencia detectada: {freq_detectada}\")\n",
    "        print(f\"   ⏱️  Frecuencia promedio: {freq_promedio}\")\n",
    "    else:\n",
    "        freq_promedio = pd.Timedelta('1H')  # Default\n",
    "        print(f\"   ⚠️  Usando frecuencia por defecto: {freq_promedio}\")\n",
    "    \n",
    "    lag_features = pd.DataFrame(index=df.index)\n",
    "    contador_features = 0\n",
    "    \n",
    "    for lag_str in lags:\n",
    "        print(f\"\\n   🔄 Procesando lag {lag_str}...\")\n",
    "        \n",
    "        try:\n",
    "            # CORRECCIÓN PRINCIPAL: Convertir lag string a timedelta y calcular períodos\n",
    "            lag_timedelta = pd.Timedelta(lag_str)\n",
    "            \n",
    "            # Método 1: Usar frecuencia detectada\n",
    "            if freq_detectada:\n",
    "                try:\n",
    "                    freq_td = pd.Timedelta(freq_detectada)\n",
    "                    lag_periods = int(lag_timedelta / freq_td)\n",
    "                    print(f\"      ✅ Método 1 exitoso: {lag_str} = {lag_periods} períodos (freq: {freq_detectada})\")\n",
    "                except:\n",
    "                    lag_periods = int(lag_timedelta / freq_promedio)\n",
    "                    print(f\"      ⚠️  Método 1 falló, usando promedio: {lag_str} = {lag_periods} períodos\")\n",
    "            else:\n",
    "                # Método 2: Usar frecuencia promedio\n",
    "                lag_periods = int(lag_timedelta / freq_promedio)\n",
    "                print(f\"      ✅ Método 2: {lag_str} = {lag_periods} períodos (freq promedio)\")\n",
    "            \n",
    "            # Validar que lag_periods sea razonable\n",
    "            if lag_periods <= 0:\n",
    "                print(f\"      ❌ Períodos inválidos ({lag_periods}), saltando lag {lag_str}\")\n",
    "                continue\n",
    "            elif lag_periods > len(df):\n",
    "                print(f\"      ⚠️  Períodos muy altos ({lag_periods} > {len(df)}), ajustando a {len(df)//4}\")\n",
    "                lag_periods = len(df) // 4\n",
    "            \n",
    "            # Crear lag features para cada variable\n",
    "            features_creados_lag = 0\n",
    "            for variable in variables_prioritarias:\n",
    "                if contador_features >= max_features:\n",
    "                    print(f\"   ⚠️  Límite global de {max_features} features alcanzado\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # CORRECCIÓN: Usar shift() con períodos enteros\n",
    "                    nombre_feature = f\"{variable}_lag_{lag_str}\"\n",
    "                    lag_features[nombre_feature] = df[variable].shift(lag_periods)\n",
    "                    contador_features += 1\n",
    "                    features_creados_lag += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      ⚠️  Error con {variable}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"      ✅ Features creados para {lag_str}: {features_creados_lag}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error procesando lag {lag_str}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        if contador_features >= max_features:\n",
    "            break\n",
    "    \n",
    "    # Crear características de diferencias con manejo robusto\n",
    "    print(f\"\\n   📊 Creando características de diferencias...\")\n",
    "    \n",
    "    diferencias_creadas = 0\n",
    "    for variable in variables_prioritarias[:8]:  # Limitar a primeras 8 variables\n",
    "        if contador_features >= max_features:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Diferencia simple (método más robusto)\n",
    "            diff_periods_12h = max(1, int(pd.Timedelta('12H') / freq_promedio))\n",
    "            diff_name = f\"{variable}_diff_12H\"\n",
    "            lag_features[diff_name] = df[variable] - df[variable].shift(diff_periods_12h)\n",
    "            \n",
    "            # Cambio porcentual\n",
    "            pct_periods_24h = max(1, int(pd.Timedelta('24H') / freq_promedio))\n",
    "            pct_name = f\"{variable}_pct_change_24H\"\n",
    "            \n",
    "            # Usar método robusto para cambio porcentual\n",
    "            previous_values = df[variable].shift(pct_periods_24h)\n",
    "            current_values = df[variable]\n",
    "            \n",
    "            # Evitar división por cero\n",
    "            lag_features[pct_name] = np.where(\n",
    "                previous_values != 0,\n",
    "                (current_values - previous_values) / np.abs(previous_values) * 100,\n",
    "                0  # Cuando el valor anterior es 0\n",
    "            )\n",
    "            \n",
    "            contador_features += 2\n",
    "            diferencias_creadas += 2\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ⚠️  Error con diferencias de {variable}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"      ✅ Características de diferencias creadas: {diferencias_creadas}\")\n",
    "    \n",
    "    # Limpieza final\n",
    "    print(f\"\\n   🧹 Aplicando limpieza final...\")\n",
    "    features_antes = lag_features.shape[1]\n",
    "    \n",
    "    # Eliminar columnas con demasiados NaN\n",
    "    threshold_nan = int(0.8 * len(lag_features))  # Permitir hasta 80% de NaN\n",
    "    lag_features = lag_features.dropna(axis=1, thresh=threshold_nan)\n",
    "    \n",
    "    # Eliminar columnas constantes\n",
    "    for col in lag_features.columns:\n",
    "        if lag_features[col].nunique() <= 1:\n",
    "            lag_features = lag_features.drop(col, axis=1)\n",
    "    \n",
    "    features_despues = lag_features.shape[1]\n",
    "    if features_antes != features_despues:\n",
    "        print(f\"      🗑️  Eliminados {features_antes - features_despues} features problemáticos\")\n",
    "    \n",
    "    print(f\"✅ Lag features creadas exitosamente: {contador_features} features totales\")\n",
    "    print(f\"📐 Dimensiones finales: {lag_features.shape[0]:,} × {lag_features.shape[1]}\")\n",
    "    \n",
    "    # Información de calidad\n",
    "    if lag_features.shape[1] > 0:\n",
    "        completitud = (lag_features.count().sum() / (lag_features.shape[0] * lag_features.shape[1])) * 100\n",
    "        print(f\"📈 Completitud promedio: {completitud:.1f}%\")\n",
    "    else:\n",
    "        print(f\"⚠️  No se generaron lag features válidas\")\n",
    "    \n",
    "    return lag_features\n",
    "\n",
    "# Ejecutar creación de lag features CORREGIDA\n",
    "print(\"\\n⏪ INICIANDO CREACIÓN DE LAG FEATURES CORREGIDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    lag_features = crear_lag_features(\n",
    "        df=df, \n",
    "        lags=['2H', '12H', '48H'],\n",
    "        max_features=75\n",
    "    )\n",
    "    \n",
    "    if not lag_features.empty:\n",
    "        print(f\"\\n💾 Memoria lag features: {lag_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        print(f\"🎯 LAG FEATURES CREADAS EXITOSAMENTE\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Dataset de lag features vacío - usando DataFrame básico como fallback\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creando lag features: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    lag_features = pd.DataFrame(index=df.index)  # DataFrame vacío como fallback\n",
    "    print(f\"⚠️  Usando fallback - DataFrame vacío\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🎯 Consolidación y Preparación del Dataset Final\n",
    "\n",
    "### 🔗 Integración de Características\n",
    "\n",
    "En esta fase crítica consolidamos todas las características creadas en un dataset unificado, optimizado para el entrenamiento de modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Consolidando dataset final de características...\n",
      "============================================================\n",
      "📊 Variables originales incluidas: 25\n",
      "📈 Rolling features agregadas: 80\n",
      "⏪ Lag features agregadas: 40\n",
      "\n",
      "🧹 Limpieza final del dataset...\n",
      "   🗑️  Eliminadas 1 columnas problemáticas\n",
      "\n",
      "📊 DATASET FINAL CONSOLIDADO:\n",
      "   📐 Dimensiones: 19,752 filas × 144 columnas\n",
      "   📅 Período: 2023-01-11 00:00:00 → 2025-04-12 23:00:00\n",
      "   💾 Memoria: 11.0 MB\n",
      "   🔢 Variables numéricas: 144\n",
      "\n",
      "📋 Composición del dataset final:\n",
      "   🔵 Variables originales: 25\n",
      "   📈 Rolling features: 80\n",
      "   ⏪ Lag features: 40\n",
      "\n",
      "✅ Calidad del dataset final:\n",
      "   📈 Completitud promedio: 100.0%\n",
      "   🎯 CALIDAD EXCELENTE - Dataset listo para modelado\n"
     ]
    }
   ],
   "source": [
    "# Consolidación final del dataset de características\n",
    "print(\"🔗 Consolidando dataset final de características...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Seleccionar variables originales más importantes\n",
    "    variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    variables_importantes = variables_numericas[:25]  # Top 25 variables originales\n",
    "    \n",
    "    # Crear dataset base con variables originales seleccionadas\n",
    "    dataset_final = df[variables_importantes].copy()\n",
    "    print(f\"📊 Variables originales incluidas: {dataset_final.shape[1]}\")\n",
    "    \n",
    "    # Agregar rolling features (filtrar por variabilidad)\n",
    "    if not rolling_features.empty:\n",
    "        # Filtrar rolling features con variabilidad significativa\n",
    "        rolling_features_filtered = rolling_features.dropna(axis=1, thresh=int(0.7 * len(rolling_features)))\n",
    "        rolling_features_filtered = rolling_features_filtered.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Seleccionar solo features con variación suficiente\n",
    "        features_con_variacion = []\n",
    "        for col in rolling_features_filtered.columns:\n",
    "            if rolling_features_filtered[col].std() > 1e-6:  # Evitar features constantes\n",
    "                features_con_variacion.append(col)\n",
    "        \n",
    "        rolling_features_final = rolling_features_filtered[features_con_variacion[:80]]  # Top 80\n",
    "        dataset_final = pd.concat([dataset_final, rolling_features_final], axis=1)\n",
    "        print(f\"📈 Rolling features agregadas: {rolling_features_final.shape[1]}\")\n",
    "    else:\n",
    "        print(f\"⚠️  No se agregaron rolling features (dataset vacío)\")\n",
    "    \n",
    "    # Agregar lag features (filtrar por variabilidad)\n",
    "    if not lag_features.empty:\n",
    "        # Filtrar lag features con variabilidad significativa\n",
    "        lag_features_filtered = lag_features.dropna(axis=1, thresh=int(0.5 * len(lag_features)))\n",
    "        lag_features_filtered = lag_features_filtered.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Seleccionar solo features con variación suficiente\n",
    "        features_con_variacion = []\n",
    "        for col in lag_features_filtered.columns:\n",
    "            if lag_features_filtered[col].std() > 1e-6:\n",
    "                features_con_variacion.append(col)\n",
    "        \n",
    "        lag_features_final = lag_features_filtered[features_con_variacion[:40]]  # Top 40\n",
    "        dataset_final = pd.concat([dataset_final, lag_features_final], axis=1)\n",
    "        print(f\"⏪ Lag features agregadas: {lag_features_final.shape[1]}\")\n",
    "    else:\n",
    "        print(f\"⚠️  No se agregaron lag features (dataset vacío)\")\n",
    "    \n",
    "    print(f\"\\n🧹 Limpieza final del dataset...\")\n",
    "    \n",
    "    # Eliminar columnas completamente nulas o constantes\n",
    "    columnas_antes = dataset_final.shape[1]\n",
    "    dataset_final = dataset_final.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Eliminar features prácticamente constantes\n",
    "    for col in dataset_final.select_dtypes(include=[np.number]).columns:\n",
    "        if dataset_final[col].std() < 1e-10:  # Prácticamente constante\n",
    "            dataset_final = dataset_final.drop(col, axis=1)\n",
    "    \n",
    "    columnas_despues = dataset_final.shape[1]\n",
    "    if columnas_antes != columnas_despues:\n",
    "        print(f\"   🗑️  Eliminadas {columnas_antes - columnas_despues} columnas problemáticas\")\n",
    "    \n",
    "    # Convertir a float32 para optimizar memoria\n",
    "    numeric_columns = dataset_final.select_dtypes(include=[np.number]).columns\n",
    "    dataset_final[numeric_columns] = dataset_final[numeric_columns].astype(np.float32)\n",
    "    \n",
    "    # Información final del dataset\n",
    "    print(f\"\\n📊 DATASET FINAL CONSOLIDADO:\")\n",
    "    print(f\"   📐 Dimensiones: {dataset_final.shape[0]:,} filas × {dataset_final.shape[1]} columnas\")\n",
    "    print(f\"   📅 Período: {dataset_final.index.min()} → {dataset_final.index.max()}\")\n",
    "    print(f\"   💾 Memoria: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"   🔢 Variables numéricas: {len(dataset_final.select_dtypes(include=[np.number]).columns)}\")\n",
    "    \n",
    "    # Mostrar composición del dataset\n",
    "    print(f\"\\n📋 Composición del dataset final:\")\n",
    "    print(f\"   🔵 Variables originales: {len(variables_importantes)}\")\n",
    "    if 'rolling_features_final' in locals():\n",
    "        print(f\"   📈 Rolling features: {rolling_features_final.shape[1]}\")\n",
    "    if 'lag_features_final' in locals():\n",
    "        print(f\"   ⏪ Lag features: {lag_features_final.shape[1]}\")\n",
    "    \n",
    "    # Verificar calidad del dataset final\n",
    "    completitud_final = (dataset_final.count().sum() / (dataset_final.shape[0] * dataset_final.shape[1])) * 100\n",
    "    print(f\"\\n✅ Calidad del dataset final:\")\n",
    "    print(f\"   📈 Completitud promedio: {completitud_final:.1f}%\")\n",
    "    \n",
    "    if completitud_final >= 80:\n",
    "        print(f\"   🎯 CALIDAD EXCELENTE - Dataset listo para modelado\")\n",
    "    elif completitud_final >= 60:\n",
    "        print(f\"   ⚠️  CALIDAD ACEPTABLE - Considerar imputación adicional\")\n",
    "    else:\n",
    "        print(f\"   ❌ CALIDAD INSUFICIENTE - Requiere procesamiento adicional\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error en consolidación del dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Crear dataset básico como fallback\n",
    "    dataset_final = df.select_dtypes(include=[np.number]).copy()\n",
    "    print(f\"⚠️  Usando dataset básico como fallback: {dataset_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 💾 Guardado y Finalización\n",
    "\n",
    "### 📁 Persistencia del Dataset de Características\n",
    "\n",
    "Guardamos el dataset final optimizado en múltiples formatos para garantizar compatibilidad y eficiencia en las fases posteriores de modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Guardando dataset final de características...\n",
      "============================================================\n",
      "✅ Parquet guardado: features_dataset_20250806_215732.parquet (10.9 MB)\n",
      "✅ CSV guardado: features_dataset_20250806_215732.csv (26.1 MB)\n",
      "✅ Metadatos guardados: features_dataset_20250806_215732_metadata.txt\n",
      "✅ Estadísticas guardadas: features_dataset_20250806_215732_statistics.csv\n",
      "\n",
      "🎯 FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\n",
      "📁 Archivos generados en data/processed:\n",
      "   📦 features_dataset_20250806_215732.parquet - Dataset principal (comprimido)\n",
      "   📄 features_dataset_20250806_215732.csv - Dataset principal (CSV)\n",
      "   📋 features_dataset_20250806_215732_metadata.txt - Metadatos completos\n",
      "   📊 features_dataset_20250806_215732_statistics.csv - Estadísticas descriptivas\n",
      "🎉 Ingeniería de Características finalizado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Guardado del dataset final con metadatos completos\n",
    "print(\"💾 Guardando dataset final de características...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configurar rutas de salida\n",
    "output_dir = ruta_processed\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Nombres de archivos de salida\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "base_name = f'features_dataset_{timestamp}'\n",
    "\n",
    "try:\n",
    "    # 1. Guardar en formato Parquet (optimizado)\n",
    "    archivo_parquet = output_dir / f'{base_name}.parquet'\n",
    "    dataset_final.to_parquet(archivo_parquet, engine='pyarrow', compression='snappy')\n",
    "    tamaño_parquet = archivo_parquet.stat().st_size / (1024 * 1024)\n",
    "    print(f\"✅ Parquet guardado: {archivo_parquet.name} ({tamaño_parquet:.1f} MB)\")\n",
    "    \n",
    "    # 2. Guardar en formato CSV (compatibilidad)\n",
    "    archivo_csv = output_dir / f'{base_name}.csv'\n",
    "    dataset_final.to_csv(archivo_csv, encoding='utf-8')\n",
    "    tamaño_csv = archivo_csv.stat().st_size / (1024 * 1024)\n",
    "    print(f\"✅ CSV guardado: {archivo_csv.name} ({tamaño_csv:.1f} MB)\")\n",
    "    \n",
    "    # 3. Generar archivo de metadatos\n",
    "    archivo_metadata = output_dir / f'{base_name}_metadata.txt'\n",
    "    with open(archivo_metadata, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"METADATOS DEL DATASET DE CARACTERÍSTICAS\\n\")\n",
    "        f.write(f\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Notebook: 03_feature_engineering_b.ipynb\\n\")\n",
    "        f.write(f\"\\nDimensiones: {dataset_final.shape[0]:,} × {dataset_final.shape[1]}\\n\")\n",
    "        f.write(f\"Período temporal: {dataset_final.index.min()} → {dataset_final.index.max()}\\n\")\n",
    "        f.write(f\"Memoria utilizada: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\\n\")\n",
    "        f.write(f\"\\nTipos de características:\\n\")\n",
    "        f.write(f\"  - Variables originales: {len(variables_importantes)}\\n\")\n",
    "        if 'rolling_features_final' in locals():\n",
    "            f.write(f\"  - Rolling features: {rolling_features_final.shape[1]}\\n\")\n",
    "        if 'lag_features_final' in locals():\n",
    "            f.write(f\"  - Lag features: {lag_features_final.shape[1]}\\n\")\n",
    "        f.write(f\"\\nEventos de mantenimiento:\\n\")\n",
    "        if eventos_utilizables is not None and len(eventos_utilizables) > 0:\n",
    "            f.write(f\"  - Eventos utilizables: {len(eventos_utilizables)}\\n\")\n",
    "            f.write(f\"  - Compatibilidad temporal: {'Confirmada' if compatibilidad_temporal else 'No confirmada'}\\n\")\n",
    "        else:\n",
    "            f.write(f\"  - Sin eventos utilizables identificados\\n\")\n",
    "        \n",
    "        f.write(f\"\\nLista de columnas:\\n\")\n",
    "        for i, col in enumerate(dataset_final.columns, 1):\n",
    "            f.write(f\"  {i:3d}. {col}\\n\")\n",
    "    \n",
    "    print(f\"✅ Metadatos guardados: {archivo_metadata.name}\")\n",
    "    \n",
    "    # 4. Generar resumen estadístico\n",
    "    archivo_stats = output_dir / f'{base_name}_statistics.csv'\n",
    "    stats_desc = dataset_final.describe()\n",
    "    stats_desc.to_csv(archivo_stats, encoding='utf-8')\n",
    "    print(f\"✅ Estadísticas guardadas: {archivo_stats.name}\")\n",
    "    \n",
    "    # Resumen final\n",
    "    print(f\"\\n🎯 FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "    print(f\"📁 Archivos generados en {output_dir}:\")\n",
    "    print(f\"   📦 {base_name}.parquet - Dataset principal (comprimido)\")\n",
    "    print(f\"   📄 {base_name}.csv - Dataset principal (CSV)\")\n",
    "    print(f\"   📋 {base_name}_metadata.txt - Metadatos completos\")\n",
    "    print(f\"   📊 {base_name}_statistics.csv - Estadísticas descriptivas\")\n",
    "    \n",
    "    print(f\"🎉 Ingeniería de Características finalizado exitosamente!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al guardar dataset: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Guardado de emergencia\n",
    "    backup_file = output_dir / f'features_dataset_emergency_backup.csv'\n",
    "    dataset_final.to_csv(backup_file)\n",
    "    print(f\"💾 Guardado de emergencia: {backup_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Resumen del Feature Engineering\n",
    "\n",
    "### 🎯 Logros Alcanzados\n",
    "\n",
    "1. **✅ Carga de datos exitosa** - Integración de series temporales y historial de eventos\n",
    "2. **✅ Validación temporal** - Confirmación de compatibilidad entre datasets\n",
    "3. **✅ Características temporales** - Creación de rolling features y lag features\n",
    "4. **✅ Dataset optimizado** - Consolidación y optimización de memoria\n",
    "5. **✅ Persistencia completa** - Guardado en múltiples formatos con metadatos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
