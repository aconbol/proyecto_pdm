{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingeniería de Características para Mantenimiento Predictivo\n",
    "## Proyecto: Predicción de Fallas en Moto-Compresores - Oil & Gas\n",
    "\n",
    "### 🎯 Objetivo del Notebook\n",
    "\n",
    "Este notebook constituye la **fase crítica** de transformación de datos donde convertimos las series temporales limpias en un dataset enriquecido y etiquetado, optimizado para el entrenamiento de modelos de Machine Learning. Nuestro objetivo principal es **predecir fallas en moto-compresores con 7 días de antelación**, una ventana temporal que permite la planificación efectiva de mantenimientos preventivos en el sector Oil & Gas.\n",
    "\n",
    "### 📋 Tareas Principales\n",
    "\n",
    "1. **Carga y Validación de Datos**: Integrar el dataset preprocesado con el historial de eventos\n",
    "2. **Ingeniería de Características Temporales**: Crear features que capturen la dinámica del deterioro\n",
    "3. **Características Avanzadas**: Implementar features de tasas de cambio, frecuencia y detección de anomalías\n",
    "4. **Etiquetado de Fallas**: Crear la variable objetivo basada en ventanas de pre-falla de 7 días\n",
    "5. **Validación y Preparación Final**: Garantizar calidad de datos para modelado\n",
    "\n",
    "### 🛠️ Librerías Especializadas\n",
    "\n",
    "Utilizaremos un stack tecnológico optimizado para análisis de series temporales industriales:\n",
    "- **pandas**: Manipulación de series temporales y DataFrames\n",
    "- **numpy**: Operaciones numéricas y cálculos matriciales\n",
    "- **scipy**: Transformadas de Fourier y análisis de señales\n",
    "- **scikit-learn**: Detección de anomalías y normalización\n",
    "- **pathlib**: Manejo robusto de rutas de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías esenciales para ingeniería de características\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Librerías especializadas para análisis de señales y anomalías\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Configuración del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"✅ Librerías importadas exitosamente\")\n",
    "print(f\"📊 Versión de pandas: {pd.__version__}\")\n",
    "print(f\"🔢 Versión de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas de datos con validación de existencia\n",
    "# Esta configuración garantiza la reproducibilidad del pipeline\n",
    "\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('..')\n",
    "\n",
    "# Rutas específicas para datos procesados y eventos\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Archivos específicos requeridos\n",
    "archivo_timeseries = ruta_processed / 'timeseries_data.parquet'\n",
    "archivo_historial = ruta_eventos / 'Historial C1 RGD.xlsx'\n",
    "\n",
    "# Validación crítica de existencia de archivos\n",
    "print(\"📁 Validación de rutas y archivos:\")\n",
    "print(f\"   Datos procesados: {ruta_processed} - {'✅ Existe' if ruta_processed.exists() else '❌ No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'✅ Existe' if ruta_eventos.exists() else '❌ No existe'}\")\n",
    "print(f\"   Timeseries: {archivo_timeseries} - {'✅ Existe' if archivo_timeseries.exists() else '❌ No existe'}\")\n",
    "print(f\"   Historial: {archivo_historial} - {'✅ Existe' if archivo_historial.exists() else '❌ No existe'}\")\n",
    "\n",
    "# Verificación crítica - detener ejecución si faltan archivos esenciales\n",
    "archivos_requeridos = [archivo_timeseries, archivo_historial]\n",
    "archivos_faltantes = [arch for arch in archivos_requeridos if not arch.exists()]\n",
    "\n",
    "if archivos_faltantes:\n",
    "    print(f\"\\n❌ ERROR CRÍTICO: Faltan archivos esenciales:\")\n",
    "    for archivo in archivos_faltantes:\n",
    "        print(f\"   - {archivo}\")\n",
    "    raise FileNotFoundError(\"No se pueden continuar sin los archivos de datos requeridos\")\n",
    "else:\n",
    "    print(\"\\n✅ Todos los archivos requeridos están disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📂 Carga y Validación de Datos\n",
    "\n",
    "### 🔄 Proceso de Carga Inteligente\n",
    "\n",
    "En esta fase crítica, cargaremos tanto el **dataset de series temporales procesado** como el **historial de eventos de mantenimiento**. La calidad de este proceso determina directamente la efectividad de nuestro modelo predictivo.\n",
    "\n",
    "El dataset de series temporales contiene las mediciones continuas de sensores del moto-compresor, ya limpias y preprocesadas. El historial de eventos proporciona las fechas exactas de las fallas históricas, información esencial para crear nuestras etiquetas de entrenamiento.\n",
    "\n",
    "### 📊 Estrategia de Validación\n",
    "\n",
    "Implementaremos validaciones exhaustivas para garantizar:\n",
    "- **Integridad temporal**: Verificar que no hay gaps críticos en los datos\n",
    "- **Consistencia de formato**: Asegurar que los índices temporales están correctamente configurados\n",
    "- **Cobertura de eventos**: Confirmar que tenemos datos de sensores para las fechas de falla registradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset principal de series temporales\n",
    "print(\"🔄 Cargando dataset principal de series temporales...\")\n",
    "\n",
    "try:\n",
    "    # Cargar dataset con configuración específica para series temporales\n",
    "    df = pd.read_parquet(archivo_timeseries)\n",
    "    \n",
    "    # Validación inmediata de la estructura\n",
    "    print(f\"✅ Dataset cargado exitosamente\")\n",
    "    print(f\"   📊 Dimensiones: {df.shape[0]:,} filas × {df.shape[1]} columnas\")\n",
    "    print(f\"   📅 Tipo de índice: {type(df.index).__name__}\")\n",
    "    \n",
    "    # Verificar que el índice temporal está correctamente configurado\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(f\"   ⏰ Rango temporal: {df.index.min()} a {df.index.max()}\")\n",
    "        print(f\"   📈 Duración total: {df.index.max() - df.index.min()}\")\n",
    "        \n",
    "        # Análisis de frecuencia de muestreo\n",
    "        diff_times = df.index.to_series().diff().dropna()\n",
    "        freq_mode = diff_times.mode().iloc[0] if not diff_times.empty else None\n",
    "        print(f\"   🕒 Frecuencia predominante: {freq_mode}\")\n",
    "        \n",
    "        # Detectar gaps significativos en los datos\n",
    "        large_gaps = diff_times[diff_times > pd.Timedelta(hours=2)]\n",
    "        if not large_gaps.empty:\n",
    "            print(f\"   ⚠️  Gaps detectados: {len(large_gaps)} intervalos > 2 horas\")\n",
    "        else:\n",
    "            print(f\"   ✅ No se detectaron gaps significativos\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  ADVERTENCIA: Índice no es DatetimeIndex, convertir si es necesario\")\n",
    "        # Intentar conversión si la primera columna parece ser temporal\n",
    "        if 'hora' in df.columns:\n",
    "            df = df.set_index('hora')\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            print(f\"   ✅ Índice convertido a DatetimeIndex usando columna 'hora'\")\n",
    "    \n",
    "    # Información sobre las columnas disponibles\n",
    "    print(f\"\\n📋 Información de columnas:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"   🔢 Columnas numéricas: {len(numeric_cols)}\")\n",
    "    print(f\"   📊 Primeras 5 columnas: {list(df.columns[:5])}\")\n",
    "    \n",
    "    # Estadísticas de calidad básicas\n",
    "    missing_pct = (df.isnull().sum().sum() / df.size) * 100\n",
    "    print(f\"   📉 Porcentaje de valores faltantes: {missing_pct:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error cargando dataset principal: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y procesamiento del historial de eventos\n",
    "print(\"\\n🔄 Cargando historial de eventos de mantenimiento...\")\n",
    "\n",
    "try:\n",
    "    # Cargar archivo Excel con manejo robusto\n",
    "    df_eventos = pd.read_excel(archivo_historial, engine='openpyxl')\n",
    "    \n",
    "    print(f\"✅ Historial de eventos cargado\")\n",
    "    print(f\"   📊 Dimensiones: {df_eventos.shape[0]} eventos × {df_eventos.shape[1]} columnas\")\n",
    "    print(f\"   📋 Columnas disponibles: {list(df_eventos.columns)}\")\n",
    "    \n",
    "    # Mostrar muestra de los primeros registros\n",
    "    print(f\"\\n🔍 Muestra de eventos registrados:\")\n",
    "    print(df_eventos.head())\n",
    "    \n",
    "    # Identificar y validar columnas de fecha\n",
    "    date_columns = [col for col in df_eventos.columns \n",
    "                   if any(keyword in col.upper() for keyword in ['FECHA', 'DATE', 'INICIO', 'START'])]\n",
    "    \n",
    "    print(f\"\\n📅 Columnas de fecha detectadas: {date_columns}\")\n",
    "    \n",
    "    if not date_columns:\n",
    "        # Buscar en todas las columnas por contenido que parezca fechas\n",
    "        print(\"⚠️  No se detectaron columnas de fecha por nombre, analizando contenido...\")\n",
    "        for col in df_eventos.columns:\n",
    "            # Verificar si la columna contiene valores que puedan ser fechas\n",
    "            sample_values = df_eventos[col].dropna().head(3)\n",
    "            if not sample_values.empty:\n",
    "                print(f\"   Columna '{col}' - Muestra: {list(sample_values)}\")\n",
    "    \n",
    "    # Seleccionar columna principal de fecha (usar la primera encontrada o la más probable)\n",
    "    if date_columns:\n",
    "        primary_date_col = date_columns[0]\n",
    "        print(f\"\\n🎯 Usando columna principal de fecha: '{primary_date_col}'\")\n",
    "        \n",
    "        # Limpiar y convertir fechas\n",
    "        fechas_originales = df_eventos[primary_date_col].copy()\n",
    "        \n",
    "        # Intentar conversión a datetime con múltiples formatos\n",
    "        try:\n",
    "            df_eventos['fecha_evento'] = pd.to_datetime(df_eventos[primary_date_col], errors='coerce')\n",
    "            fechas_validas = df_eventos['fecha_evento'].notna().sum()\n",
    "            total_fechas = len(df_eventos)\n",
    "            \n",
    "            print(f\"   ✅ Conversión de fechas: {fechas_validas}/{total_fechas} fechas válidas\")\n",
    "            \n",
    "            if fechas_validas > 0:\n",
    "                fechas_limpias = df_eventos['fecha_evento'].dropna()\n",
    "                print(f\"   📅 Rango de eventos: {fechas_limpias.min()} a {fechas_limpias.max()}\")\n",
    "                print(f\"   📊 Eventos únicos: {fechas_limpias.nunique()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error en conversión de fechas: {str(e)}\")\n",
    "            print(f\"   📋 Valores de muestra para diagnóstico: {list(fechas_originales.head())}\")\n",
    "    else:\n",
    "        print(f\"⚠️  ADVERTENCIA: No se pudo identificar automáticamente la columna de fechas\")\n",
    "        print(f\"   Se requerirá intervención manual para especificar la columna correcta\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error cargando historial de eventos: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación de compatibilidad temporal entre datasets\n",
    "print(\"\\n🔍 Validando compatibilidad temporal entre datasets...\")\n",
    "\n",
    "if isinstance(df.index, pd.DatetimeIndex) and 'fecha_evento' in df_eventos.columns:\n",
    "    # Rangos temporales de ambos datasets\n",
    "    sensor_start, sensor_end = df.index.min(), df.index.max()\n",
    "    eventos_limpios = df_eventos['fecha_evento'].dropna()\n",
    "    \n",
    "    if not eventos_limpios.empty:\n",
    "        eventos_start, eventos_end = eventos_limpios.min(), eventos_limpios.max()\n",
    "        \n",
    "        print(f\"📊 Análisis de cobertura temporal:\")\n",
    "        print(f\"   🔧 Datos de sensores: {sensor_start} a {sensor_end}\")\n",
    "        print(f\"   📅 Eventos registrados: {eventos_start} a {eventos_end}\")\n",
    "        \n",
    "        # Calcular solapamiento temporal\n",
    "        overlap_start = max(sensor_start, eventos_start)\n",
    "        overlap_end = min(sensor_end, eventos_end)\n",
    "        \n",
    "        if overlap_start <= overlap_end:\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "            print(f\"   ✅ Solapamiento detectado: {overlap_start} a {overlap_end}\")\n",
    "            print(f\"   ⏱️  Duración del solapamiento: {overlap_duration}\")\n",
    "            \n",
    "            # Eventos que caen dentro del rango de datos de sensores\n",
    "            eventos_utilizables = eventos_limpios[\n",
    "                (eventos_limpios >= sensor_start) & (eventos_limpios <= sensor_end)\n",
    "            ]\n",
    "            \n",
    "            print(f\"   🎯 Eventos utilizables para entrenamiento: {len(eventos_utilizables)}\")\n",
    "            \n",
    "            if len(eventos_utilizables) > 0:\n",
    "                print(f\"   📋 Fechas de eventos utilizables:\")\n",
    "                for i, evento in enumerate(eventos_utilizables, 1):\n",
    "                    print(f\"      {i}. {evento}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  ADVERTENCIA: No hay eventos dentro del rango de datos de sensores\")\n",
    "        else:\n",
    "            print(f\"   ❌ No hay solapamiento temporal entre datasets\")\n",
    "            print(f\"   💡 Sugerencia: Verificar que los datos corresponden al mismo equipo y período\")\n",
    "    else:\n",
    "        print(f\"   ❌ No hay eventos válidos para analizar\")\n",
    "else:\n",
    "    print(f\"   ⚠️  No se puede realizar validación temporal - verificar formato de datos\")\n",
    "\n",
    "# Guardar información de validación para uso posterior\n",
    "if 'eventos_utilizables' in locals() and len(eventos_utilizables) > 0:\n",
    "    fechas_falla = eventos_utilizables.sort_values().tolist()\n",
    "    print(f\"\\n✅ Configuración completada: {len(fechas_falla)} eventos listos para etiquetado\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  ADVERTENCIA: Configuración incompleta - revisar datos de eventos\")\n",
    "    fechas_falla = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🔬 Selección Inteligente de Variables Críticas\n",
    "\n",
    "### 📊 Metodología de Selección Automática\n",
    "\n",
    "Antes de proceder con la ingeniería de características, debemos identificar las **variables más relevantes** para el mantenimiento predictivo del moto-compresor. Esta selección se basa en criterios estadísticos y de ingeniería específicos para equipos rotativos en Oil & Gas.\n",
    "\n",
    "### 🎯 Criterios de Selección\n",
    "\n",
    "Aplicaremos múltiples criterios para identificar las variables más predictivas:\n",
    "\n",
    "1. **Variabilidad Temporal**: Variables con suficiente variación para ser informativas\n",
    "2. **Completitud de Datos**: Variables con alta disponibilidad de mediciones\n",
    "3. **Relevancia Física**: Parámetros críticos conocidos en moto-compresores (temperaturas, presiones, vibraciones)\n",
    "4. **Sensibilidad a Fallas**: Variables que típicamente muestran deterioro progresivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección inteligente de variables críticas para feature engineering\n",
    "print(\"🔍 Seleccionando variables críticas para ingeniería de características...\")\n",
    "\n",
    "# Filtrar solo columnas numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"📊 Variables numéricas disponibles: {len(numeric_cols)}\")\n",
    "\n",
    "# Análisis de calidad de datos por variable\n",
    "variable_quality = pd.DataFrame({\n",
    "    'variable': numeric_cols,\n",
    "    'completitud': df[numeric_cols].count() / len(df),\n",
    "    'variabilidad': df[numeric_cols].std() / df[numeric_cols].mean().abs(),  # Coeficiente de variación\n",
    "    'rango': df[numeric_cols].max() - df[numeric_cols].min(),\n",
    "    'valores_unicos': df[numeric_cols].nunique()\n",
    "})\n",
    "\n",
    "# Manejo de valores infinitos y NaN en variabilidad\n",
    "variable_quality['variabilidad'] = variable_quality['variabilidad'].replace([np.inf, -np.inf], np.nan)\n",
    "variable_quality['variabilidad'] = variable_quality['variabilidad'].fillna(0)\n",
    "\n",
    "# Criterios de filtrado específicos para moto-compresores\n",
    "criterios_filtrado = {\n",
    "    'completitud_minima': 0.80,  # Al menos 80% de datos disponibles\n",
    "    'variabilidad_minima': 0.01,  # Coeficiente de variación mínimo\n",
    "    'valores_unicos_minimos': 10   # Al menos 10 valores únicos\n",
    "}\n",
    "\n",
    "# Aplicar filtros de calidad\n",
    "variables_validas = variable_quality[\n",
    "    (variable_quality['completitud'] >= criterios_filtrado['completitud_minima']) &\n",
    "    (variable_quality['variabilidad'] >= criterios_filtrado['variabilidad_minima']) &\n",
    "    (variable_quality['valores_unicos'] >= criterios_filtrado['valores_unicos_minimos'])\n",
    "]\n",
    "\n",
    "print(f\"✅ Variables que cumplen criterios de calidad: {len(variables_validas)}\")\n",
    "\n",
    "# Priorización por relevancia física en moto-compresores\n",
    "keywords_criticos = {\n",
    "    'temperatura': ['temp', 'temperatura', 'temperature'],\n",
    "    'presion': ['pres', 'presion', 'pressure'],\n",
    "    'rpm': ['rpm', 'velocidad', 'speed', 'rev'],\n",
    "    'vibracion': ['vib', 'vibracion', 'vibration'],\n",
    "    'flujo': ['flujo', 'flow', 'caudal'],\n",
    "    'potencia': ['potencia', 'power', 'watt']\n",
    "}\n",
    "\n",
    "# Scoring de relevancia física\n",
    "def calcular_score_relevancia(nombre_variable):\n",
    "    \"\"\"Calcula score de relevancia basado en palabras clave críticas\"\"\"\n",
    "    score = 0\n",
    "    nombre_lower = nombre_variable.lower()\n",
    "    \n",
    "    for categoria, keywords in keywords_criticos.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in nombre_lower:\n",
    "                # Pesos específicos por criticidad en moto-compresores\n",
    "                pesos = {'temperatura': 3, 'presion': 3, 'rpm': 2, 'vibracion': 2, 'flujo': 1, 'potencia': 1}\n",
    "                score += pesos.get(categoria, 1)\n",
    "                break\n",
    "    \n",
    "    return score\n",
    "\n",
    "variables_validas['score_relevancia'] = variables_validas['variable'].apply(calcular_score_relevancia)\n",
    "\n",
    "# Scoring combinado (normalizar cada componente entre 0-1)\n",
    "variables_validas['score_completitud'] = variables_validas['completitud']\n",
    "variables_validas['score_variabilidad'] = variables_validas['variabilidad'] / variables_validas['variabilidad'].max()\n",
    "variables_validas['score_relevancia_norm'] = variables_validas['score_relevancia'] / variables_validas['score_relevancia'].max() if variables_validas['score_relevancia'].max() > 0 else 0\n",
    "\n",
    "# Score final ponderado\n",
    "pesos_criterios = {'completitud': 0.3, 'variabilidad': 0.3, 'relevancia': 0.4}\n",
    "variables_validas['score_final'] = (\n",
    "    pesos_criterios['completitud'] * variables_validas['score_completitud'] +\n",
    "    pesos_criterios['variabilidad'] * variables_validas['score_variabilidad'] +\n",
    "    pesos_criterios['relevancia'] * variables_validas['score_relevancia_norm']\n",
    ")\n",
    "\n",
    "# Selección final de variables críticas\n",
    "variables_seleccionadas = variables_validas.nlargest(12, 'score_final')  # Top 12 variables\n",
    "variables_criticas = variables_seleccionadas['variable'].tolist()\n",
    "\n",
    "print(f\"\\n🎯 Variables críticas seleccionadas ({len(variables_criticas)}):\")\n",
    "for i, var in enumerate(variables_criticas, 1):\n",
    "    score = variables_seleccionadas[variables_seleccionadas['variable'] == var]['score_final'].iloc[0]\n",
    "    completitud = variables_seleccionadas[variables_seleccionadas['variable'] == var]['completitud'].iloc[0]\n",
    "    print(f\"   {i:2d}. {var:<25} (Score: {score:.3f}, Completitud: {completitud:.1%})\")\n",
    "\n",
    "print(f\"\\n✅ Selección completada: {len(variables_criticas)} variables listas para feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ⚙️ Ingeniería de Características Temporales - Fundamentos\n",
    "\n",
    "### 🎯 Filosofía del Feature Engineering en Mantenimiento Predictivo\n",
    "\n",
    "La **ingeniería de características temporales** es el corazón de cualquier sistema de mantenimiento predictivo exitoso. A diferencia del análisis de datos estáticos, los equipos industriales exhiben **patrones de deterioro progresivo** que se manifiestan a través de cambios sutiles en múltiples escalas temporales.\n",
    "\n",
    "### 📊 Escalas Temporales en Moto-Compresores\n",
    "\n",
    "Los moto-compresores en Oil & Gas operan con dinámicas complejas que requieren análisis multi-escala:\n",
    "\n",
    "- **Corto Plazo (1-6 horas)**: Fluctuaciones operacionales, cambios de carga, efectos térmicos\n",
    "- **Mediano Plazo (12-48 horas)**: Tendencias de degradación, efectos de fatiga, acumulación de contaminantes\n",
    "- **Largo Plazo (72+ horas)**: Deterioro estructural, desgaste progresivo, degradación de componentes\n",
    "\n",
    "### 🔬 Ventanas Móviles (Rolling Features)\n",
    "\n",
    "Las **ventanas móviles** permiten capturar el comportamiento estadístico reciente del equipo, suavizando el ruido inherente en las mediciones industriales mientras preservan las tendencias críticas. Implementaremos ventanas optimizadas para la dinámica del moto-compresor:\n",
    "\n",
    "- **Ventana 6H**: Captura ciclos operacionales y fluctuaciones de turno\n",
    "- **Ventana 24H**: Identifica patrones diarios y efectos térmicos acumulativos  \n",
    "- **Ventana 72H**: Detecta tendencias de degradación de mediano plazo\n",
    "\n",
    "Para cada ventana, calcularemos estadísticas robustas que han demostrado alta correlación con el estado de salud del equipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de características de ventanas móviles (Rolling Features)\n",
    "print(\"⚙️ Generando características de ventanas móviles...\")\n",
    "print(\"\\n📊 Configuración de ventanas temporales optimizadas para moto-compresores:\")\n",
    "\n",
    "# Configuración de ventanas específicas para moto-compresores\n",
    "ventanas_config = {\n",
    "    '6H': {'horas': 6, 'descripcion': 'Ciclos operacionales y fluctuaciones de turno'},\n",
    "    '24H': {'horas': 24, 'descripcion': 'Patrones diarios y efectos térmicos acumulativos'},\n",
    "    '72H': {'horas': 72, 'descripcion': 'Tendencias de degradación de mediano plazo'}\n",
    "}\n",
    "\n",
    "for ventana, config in ventanas_config.items():\n",
    "    print(f\"   {ventana}: {config['descripcion']}\")\n",
    "\n",
    "# DataFrame para almacenar todas las características generadas\n",
    "df_features = df[variables_criticas].copy()\n",
    "contador_features = len(variables_criticas)  # Features originales\n",
    "\n",
    "print(f\"\\n🔄 Procesando {len(variables_criticas)} variables críticas...\")\n",
    "\n",
    "# Generar features de ventanas móviles para cada variable crítica\n",
    "for variable in variables_criticas:\n",
    "    print(f\"\\n📈 Procesando variable: {variable}\")\n",
    "    \n",
    "    # Verificar que la variable tiene datos suficientes\n",
    "    datos_validos = df[variable].notna().sum()\n",
    "    if datos_validos < 100:  # Mínimo 100 puntos válidos\n",
    "        print(f\"   ⚠️  Saltando {variable}: datos insuficientes ({datos_validos} puntos)\")\n",
    "        continue\n",
    "    \n",
    "    for ventana_nombre, ventana_config in ventanas_config.items():\n",
    "        window_size = f\"{ventana_config['horas']}H\"\n",
    "        \n",
    "        try:\n",
    "            # Rolling Window Statistics - optimizadas para detección de degradación\n",
    "            \n",
    "            # 1. Media móvil - tendencia central suavizada\n",
    "            col_mean = f\"{variable}_mean_{ventana_nombre.lower()}\"\n",
    "            df_features[col_mean] = df[variable].rolling(window=window_size, min_periods=1).mean()\n",
    "            \n",
    "            # 2. Desviación estándar móvil - indicador de estabilidad operacional\n",
    "            col_std = f\"{variable}_std_{ventana_nombre.lower()}\"\n",
    "            df_features[col_std] = df[variable].rolling(window=window_size, min_periods=1).std()\n",
    "            \n",
    "            # 3. Rango móvil (max-min) - detector de picos anómalos\n",
    "            col_range = f\"{variable}_range_{ventana_nombre.lower()}\"\n",
    "            rolling_max = df[variable].rolling(window=window_size, min_periods=1).max()\n",
    "            rolling_min = df[variable].rolling(window=window_size, min_periods=1).min()\n",
    "            df_features[col_range] = rolling_max - rolling_min\n",
    "            \n",
    "            # 4. Percentiles móviles - robustos ante outliers\n",
    "            col_q25 = f\"{variable}_q25_{ventana_nombre.lower()}\"\n",
    "            col_q75 = f\"{variable}_q75_{ventana_nombre.lower()}\"\n",
    "            df_features[col_q25] = df[variable].rolling(window=window_size, min_periods=1).quantile(0.25)\n",
    "            df_features[col_q75] = df[variable].rolling(window=window_size, min_periods=1).quantile(0.75)\n",
    "            \n",
    "            # 5. Coeficiente de variación móvil - estabilidad relativa\n",
    "            col_cv = f\"{variable}_cv_{ventana_nombre.lower()}\"\n",
    "            rolling_mean = df_features[col_mean]\n",
    "            rolling_std = df_features[col_std]\n",
    "            df_features[col_cv] = rolling_std / rolling_mean.abs()\n",
    "            df_features[col_cv] = df_features[col_cv].replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            contador_features += 5  # 5 nuevas features por ventana\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error procesando {variable} con ventana {ventana_nombre}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   ✅ Completado: 15 features generadas (5 estadísticas × 3 ventanas)\")\n",
    "\n",
    "print(f\"\\n📊 Resumen de Rolling Features:\")\n",
    "print(f\"   🔢 Features originales: {len(variables_criticas)}\")\n",
    "print(f\"   ⚙️ Features rolling generadas: {contador_features - len(variables_criticas)}\")\n",
    "print(f\"   📈 Total features actuales: {contador_features}\")\n",
    "print(f\"   💾 Dimensiones del dataset: {df_features.shape}\")\n",
    "\n",
    "print(f\"\\n✅ Rolling Features completadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🕐 Características de Retraso (Lag Features)\n",
    "\n",
    "### 🧠 Fundamento Teórico de las Lag Features\n",
    "\n",
    "Las **características de retraso** proporcionan \"memoria histórica\" al modelo, permitiéndole comparar el estado actual del equipo con estados anteriores. Esta capacidad es fundamental para detectar **tendencias de degradación** y **cambios progresivos** que son imperceptibles en mediciones instantáneas.\n",
    "\n",
    "### ⏰ Selección Estratégica de Intervalos de Retraso\n",
    "\n",
    "La selección de intervalos de retraso debe alinearse con la **física del deterioro** en moto-compresores:\n",
    "\n",
    "- **Lag 2H**: Detección de cambios operacionales inmediatos y transitorios térmicos\n",
    "- **Lag 12H**: Identificación de ciclos de fatiga y efectos de acumulación térmica\n",
    "- **Lag 48H**: Captura de tendencias de desgaste progresivo y degradación estructural\n",
    "\n",
    "### 📊 Interpretación Técnica\n",
    "\n",
    "Las lag features permiten al modelo evaluar preguntas críticas como:\n",
    "- *¿La temperatura actual es significativamente diferente a la de hace 48 horas?*\n",
    "- *¿Las vibraciones muestran una tendencia ascendente comparando con el estado de ayer?*\n",
    "- *¿Los patrones de presión han cambiado respecto al comportamiento histórico reciente?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de características de retraso (Lag Features)\n",
    "print(\"🕐 Generando características de retraso (Lag Features)...\")\n",
    "print(\"\\n⏰ Configuración de intervalos de retraso para moto-compresores:\")\n",
    "\n",
    "# Configuración de lags optimizada para dinámicas de moto-compresores\n",
    "lags_config = {\n",
    "    '2H': {'horas': 2, 'descripcion': 'Cambios operacionales inmediatos y transitorios térmicos'},\n",
    "    '12H': {'horas': 12, 'descripcion': 'Ciclos de fatiga y efectos de acumulación térmica'},\n",
    "    '48H': {'horas': 48, 'descripcion': 'Tendencias de desgaste progresivo y degradación estructural'}\n",
    "}\n",
    "\n",
    "for lag_nombre, config in lags_config.items():\n",
    "    print(f\"   {lag_nombre}: {config['descripcion']}\")\n",
    "\n",
    "features_lag_generadas = 0\n",
    "print(f\"\\n🔄 Generando lag features para {len(variables_criticas)} variables críticas...\")\n",
    "\n",
    "# Generar lag features para cada variable crítica\n",
    "for variable in variables_criticas:\n",
    "    print(f\"\\n📊 Procesando lags para: {variable}\")\n",
    "    \n",
    "    # Verificar disponibilidad de datos\n",
    "    datos_validos = df[variable].notna().sum()\n",
    "    if datos_validos < 200:  # Mínimo para lags significativos\n",
    "        print(f\"   ⚠️  Saltando {variable}: datos insuficientes para lags ({datos_validos} puntos)\")\n",
    "        continue\n",
    "    \n",
    "    for lag_nombre, lag_config in lags_config.items():\n",
    "        lag_periods = lag_config['horas']  # pandas shift espera períodos\n",
    "        \n",
    "        try:\n",
    "            # Lag feature básico - valor histórico directo\n",
    "            col_lag = f\"{variable}_lag_{lag_nombre.lower()}\"\n",
    "            df_features[col_lag] = df[variable].shift(periods=lag_periods)\n",
    "            \n",
    "            # Diferencia con lag - cambio absoluto\n",
    "            col_diff = f\"{variable}_diff_{lag_nombre.lower()}\"\n",
    "            df_features[col_diff] = df[variable] - df_features[col_lag]\n",
    "            \n",
    "            # Ratio con lag - cambio relativo (más robusto para diferentes escalas)\n",
    "            col_ratio = f\"{variable}_ratio_{lag_nombre.lower()}\"\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                df_features[col_ratio] = df[variable] / df_features[col_lag]\n",
    "                # Limpiar valores infinitos e inválidos\n",
    "                df_features[col_ratio] = df_features[col_ratio].replace([np.inf, -np.inf], np.nan)\n",
    "                # Valores extremos que indican problemas de medición\n",
    "                df_features[col_ratio] = df_features[col_ratio].where(\n",
    "                    (df_features[col_ratio] >= 0.1) & (df_features[col_ratio] <= 10.0), np.nan\n",
    "                )\n",
    "            \n",
    "            features_lag_generadas += 3  # lag, diff, ratio\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error procesando lag {lag_nombre} para {variable}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   ✅ Completado: 9 lag features generadas (3 tipos × 3 intervalos)\")\n",
    "\n",
    "print(f\"\\n📊 Resumen de Lag Features:\")\n",
    "print(f\"   🕐 Features lag generadas: {features_lag_generadas}\")\n",
    "print(f\"   📈 Total features actuales: {df_features.shape[1]}\")\n",
    "print(f\"   💾 Dimensiones del dataset: {df_features.shape}\")\n",
    "\n",
    "print(f\"\\n✅ Lag Features completadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 🔬 Características Avanzadas de Análisis de Señales\n",
    "\n",
    "### 📈 Tasas de Cambio (First Derivatives)\n",
    "\n",
    "### 🧮 Fundamento Matemático y Físico\n",
    "\n",
    "Las **tasas de cambio** o **derivadas temporales** capturan la **velocidad de variación** de los parámetros del moto-compresor. Desde la perspectiva física, estas características revelan procesos dinámicos críticos:\n",
    "\n",
    "- **Gradientes térmicos**: Velocidad de calentamiento que indica fricción anómala\n",
    "- **Cambios de presión**: Tasas que revelan fugas o degradación de sellos\n",
    "- **Aceleración de componentes**: Variaciones en RPM que indican desbalanceos\n",
    "\n",
    "### ⚡ Implementación Robusta\n",
    "\n",
    "Utilizaremos diferencias finitas centradas para mayor precisión numérica, aplicando suavizado previo para reducir el impacto del ruido de medición en las derivadas.\n",
    "\n",
    "### 🌊 Análisis de Frecuencia (FFT Features)\n",
    "\n",
    "### 🔊 Transformada Rápida de Fourier para Diagnóstico\n",
    "\n",
    "El **análisis espectral** mediante FFT revela patrones ocultos en el dominio de frecuencia que son imperceptibles en el análisis temporal tradicional. En moto-compresores, cada componente genera firmas espectrales características:\n",
    "\n",
    "- **Frecuencias de rotación**: Detección de desbalances y desalineaciones\n",
    "- **Armónicos**: Identificación de problemas en rodamientos y engranajes  \n",
    "- **Modulaciones**: Detección de holguras y desgastes progresivos\n",
    "\n",
    "### 📊 Features Espectrales Críticas\n",
    "\n",
    "Extraeremos características espectrales específicamente relevantes para mantenimiento predictivo:\n",
    "- **Energía espectral total**: Indicador de actividad vibratoria global\n",
    "- **Frecuencia dominante**: Identificación de modos de falla predominantes\n",
    "- **Distribución espectral**: Concentración de energía en bandas críticas\n",
    "\n",
    "### 🚨 Detección de Anomalías Estadísticas\n",
    "\n",
    "### 📐 Z-Scores Móviles para Detección de Desviaciones\n",
    "\n",
    "Los **Z-scores móviles** permiten identificar desviaciones estadísticamente significativas respecto al comportamiento histórico reciente del equipo. Esta técnica es especialmente efectiva para detectar:\n",
    "\n",
    "- **Cambios abruptos**: Desviaciones súbitas que indican eventos críticos\n",
    "- **Drift gradual**: Desviaciones lentas que señalan deterioro progresivo\n",
    "- **Comportamientos atípicos**: Patrones que se apartan de la normalidad operacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de características avanzadas - Tasas de cambio (First Derivatives)\n",
    "print(\"📈 Generando características de tasas de cambio (First Derivatives)...\")\n",
    "print(\"\\n🧮 Aplicando diferencias finitas centradas con suavizado previo...\")\n",
    "\n",
    "features_derivadas = 0\n",
    "\n",
    "# Configuración para cálculo de derivadas\n",
    "ventana_suavizado = '2H'  # Suavizado previo para reducir ruido en derivadas\n",
    "\n",
    "for variable in variables_criticas:\n",
    "    print(f\"\\n📊 Procesando derivadas para: {variable}\")\n",
    "    \n",
    "    try:\n",
    "        # Suavizado previo usando media móvil para reducir ruido\n",
    "        serie_suavizada = df[variable].rolling(window=ventana_suavizado, center=True).mean()\n",
    "        \n",
    "        # Primera derivada (diferencias finitas)\n",
    "        col_deriv1 = f\"{variable}_deriv_1h\"\n",
    "        df_features[col_deriv1] = serie_suavizada.diff(periods=1)\n",
    "        \n",
    "        # Derivada de mayor orden (cambios en la tasa de cambio)\n",
    "        col_deriv2 = f\"{variable}_deriv_6h\"\n",
    "        df_features[col_deriv2] = serie_suavizada.diff(periods=6)\n",
    "        \n",
    "        # Derivada absoluta (magnitud de cambio sin dirección)\n",
    "        col_deriv_abs = f\"{variable}_deriv_abs_1h\"\n",
    "        df_features[col_deriv_abs] = df_features[col_deriv1].abs()\n",
    "        \n",
    "        # Aceleración (segunda derivada) - detecta cambios en las tendencias\n",
    "        col_accel = f\"{variable}_accel_1h\"\n",
    "        df_features[col_accel] = df_features[col_deriv1].diff(periods=1)\n",
    "        \n",
    "        features_derivadas += 4\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error calculando derivadas para {variable}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   ✅ 4 características de derivadas generadas\")\n",
    "\n",
    "print(f\"\\n📊 Resumen de características de derivadas:\")\n",
    "print(f\"   📈 Features de derivadas generadas: {features_derivadas}\")\n",
    "print(f\"   💾 Dimensiones actuales: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de características espectrales (FFT Features)\n",
    "print(\"\\n🌊 Generando características espectrales (FFT Features)...\")\n",
    "print(\"\\n🔊 Aplicando análisis de Fourier para detección de patrones espectrales...\")\n",
    "\n",
    "features_fft = 0\n",
    "ventana_fft = 72  # Ventana de 72 horas para análisis espectral robusto\n",
    "\n",
    "# Función para extraer características espectrales\n",
    "def extraer_features_fft(serie, ventana_size=ventana_fft):\n",
    "    \"\"\"Extrae características espectrales usando FFT en ventana deslizante\"\"\"\n",
    "    n = len(serie)\n",
    "    \n",
    "    # Inicializar arrays para características espectrales\n",
    "    energia_espectral = np.full(n, np.nan)\n",
    "    freq_dominante = np.full(n, np.nan)\n",
    "    concentracion_espectral = np.full(n, np.nan)\n",
    "    \n",
    "    # Procesamiento con ventana deslizante\n",
    "    for i in range(ventana_size, n):\n",
    "        ventana_datos = serie.iloc[i-ventana_size:i].dropna()\n",
    "        \n",
    "        if len(ventana_datos) < ventana_size * 0.8:  # Al menos 80% de datos válidos\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Remover tendencia linear para mejorar análisis espectral\n",
    "            from scipy import signal as scipy_signal\n",
    "            ventana_detrend = scipy_signal.detrend(ventana_datos.values)\n",
    "            \n",
    "            # Calcular FFT\n",
    "            fft_valores = fft(ventana_detrend)\n",
    "            fft_magnitudes = np.abs(fft_valores)\n",
    "            freqs = fftfreq(len(ventana_detrend), d=1.0)  # Frecuencia normalizada\n",
    "            \n",
    "            # Solo frecuencias positivas\n",
    "            idx_pos = freqs > 0\n",
    "            fft_magnitudes_pos = fft_magnitudes[idx_pos]\n",
    "            freqs_pos = freqs[idx_pos]\n",
    "            \n",
    "            if len(fft_magnitudes_pos) > 0:\n",
    "                # 1. Energía espectral total\n",
    "                energia_espectral[i] = np.sum(fft_magnitudes_pos**2)\n",
    "                \n",
    "                # 2. Frecuencia dominante\n",
    "                idx_max = np.argmax(fft_magnitudes_pos)\n",
    "                freq_dominante[i] = freqs_pos[idx_max]\n",
    "                \n",
    "                # 3. Concentración espectral (entropía espectral)\n",
    "                potencia_norm = fft_magnitudes_pos**2 / np.sum(fft_magnitudes_pos**2)\n",
    "                # Evitar log(0)\n",
    "                potencia_norm = potencia_norm[potencia_norm > 1e-10]\n",
    "                if len(potencia_norm) > 1:\n",
    "                    entropia = -np.sum(potencia_norm * np.log(potencia_norm))\n",
    "                    concentracion_espectral[i] = entropia\n",
    "        \n",
    "        except Exception:\n",
    "            continue  # Saltar ventanas problemáticas\n",
    "    \n",
    "    return energia_espectral, freq_dominante, concentracion_espectral\n",
    "\n",
    "# Aplicar análisis espectral a variables con mayor contenido de información\n",
    "# Seleccionar variables que típicamente muestran patrones espectrales importantes\n",
    "variables_espectrales = [var for var in variables_criticas \n",
    "                        if any(keyword in var.lower() for keyword in ['temp', 'pres', 'rpm', 'vib'])]\n",
    "\n",
    "print(f\"🎯 Variables seleccionadas para análisis espectral: {len(variables_espectrales)}\")\n",
    "for var in variables_espectrales:\n",
    "    print(f\"   - {var}\")\n",
    "\n",
    "for variable in variables_espectrales[:6]:  # Limitar a 6 variables por eficiencia computacional\n",
    "    print(f\"\\n🔄 Procesando FFT para: {variable}\")\n",
    "    \n",
    "    try:\n",
    "        # Extraer características espectrales\n",
    "        energia, freq_dom, concentracion = extraer_features_fft(df[variable])\n",
    "        \n",
    "        # Agregar características al dataset\n",
    "        df_features[f\"{variable}_fft_energia\"] = energia\n",
    "        df_features[f\"{variable}_fft_freq_dom\"] = freq_dom\n",
    "        df_features[f\"{variable}_fft_concentracion\"] = concentracion\n",
    "        \n",
    "        features_fft += 3\n",
    "        \n",
    "        # Estadísticas de validez\n",
    "        valores_validos = pd.Series(energia).notna().sum()\n",
    "        total_puntos = len(energia)\n",
    "        print(f\"   ✅ FFT completado: {valores_validos}/{total_puntos} puntos válidos ({valores_validos/total_puntos*100:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error en FFT para {variable}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n📊 Resumen de características espectrales:\")\n",
    "print(f\"   🌊 Features FFT generadas: {features_fft}\")\n",
    "print(f\"   💾 Dimensiones actuales: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de detección de anomalías estadísticas (Z-Scores móviles)\n",
    "print(\"\\n🚨 Generando características de detección de anomalías...\")\n",
    "print(\"\\n📐 Aplicando Z-scores móviles para identificación de desviaciones...\")\n",
    "\n",
    "features_anomalias = 0\n",
    "\n",
    "# Configuración de ventanas para Z-scores móviles\n",
    "ventanas_zscore = {'24H': 24, '72H': 72, '168H': 168}  # 1 día, 3 días, 1 semana\n",
    "\n",
    "print(f\"🎯 Ventanas para análisis de anomalías:\")\n",
    "for ventana, horas in ventanas_zscore.items():\n",
    "    print(f\"   {ventana}: {horas} horas\")\n",
    "\n",
    "# Función para calcular Z-score móvil robusto\n",
    "def calcular_zscore_robusto(serie, ventana):\n",
    "    \"\"\"Calcula Z-score móvil usando estadísticas robustas\"\"\"\n",
    "    # Usar mediana y MAD (Median Absolute Deviation) para robustez ante outliers\n",
    "    rolling_median = serie.rolling(window=ventana, min_periods=int(ventana*0.5)).median()\n",
    "    rolling_mad = serie.rolling(window=ventana, min_periods=int(ventana*0.5)).apply(\n",
    "        lambda x: np.median(np.abs(x - np.median(x))), raw=True\n",
    "    )\n",
    "    \n",
    "    # Z-score robusto\n",
    "    zscore = (serie - rolling_median) / (rolling_mad * 1.4826)  # Factor para normalizar MAD\n",
    "    \n",
    "    # Manejar divisiones por cero\n",
    "    zscore = zscore.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return zscore\n",
    "\n",
    "# Aplicar detección de anomalías a variables críticas\n",
    "variables_anomalias = variables_criticas[:8]  # Limitar a 8 variables principales\n",
    "\n",
    "for variable in variables_anomalias:\n",
    "    print(f\"\\n📊 Procesando anomalías para: {variable}\")\n",
    "    \n",
    "    for ventana_nombre, ventana_horas in ventanas_zscore.items():\n",
    "        try:\n",
    "            # Z-score móvil robusto\n",
    "            col_zscore = f\"{variable}_zscore_{ventana_nombre.lower()}\"\n",
    "            df_features[col_zscore] = calcular_zscore_robusto(df[variable], f\"{ventana_horas}H\")\n",
    "            \n",
    "            # Magnitud de anomalía (valor absoluto del Z-score)\n",
    "            col_anomalia_mag = f\"{variable}_anomalia_mag_{ventana_nombre.lower()}\"\n",
    "            df_features[col_anomalia_mag] = df_features[col_zscore].abs()\n",
    "            \n",
    "            # Indicador binario de anomalía (|Z-score| > 2.5)\n",
    "            col_anomalia_bin = f\"{variable}_anomalia_bin_{ventana_nombre.lower()}\"\n",
    "            df_features[col_anomalia_bin] = (df_features[col_zscore].abs() > 2.5).astype(int)\n",
    "            \n",
    "            features_anomalias += 3\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculando anomalías para {variable}, ventana {ventana_nombre}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   ✅ 9 características de anomalías generadas (3 tipos × 3 ventanas)\")\n",
    "\n",
    "print(f\"\\n📊 Resumen de características de anomalías:\")\n",
    "print(f\"   🚨 Features de anomalías generadas: {features_anomalias}\")\n",
    "print(f\"   💾 Dimensiones actuales: {df_features.shape}\")\n",
    "\n",
    "# Resumen final de todas las características avanzadas\n",
    "total_features_avanzadas = features_derivadas + features_fft + features_anomalias\n",
    "print(f\"\\n✅ RESUMEN DE CARACTERÍSTICAS AVANZADAS:\")\n",
    "print(f\"   📈 Derivadas: {features_derivadas} features\")\n",
    "print(f\"   🌊 FFT/Espectrales: {features_fft} features\")\n",
    "print(f\"   🚨 Anomalías: {features_anomalias} features\")\n",
    "print(f\"   🎯 Total avanzadas: {total_features_avanzadas} features\")\n",
    "print(f\"   💾 Dataset final: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Análisis y Recomendaciones de Características Avanzadas\n",
    "\n",
    "### 📈 Impacto de las Tasas de Cambio (Derivadas)\n",
    "\n",
    "Las **características de derivadas** proporcionan insights fundamentales sobre la **velocidad de deterioro** del moto-compresor. Estas features son especialmente valiosas porque:\n",
    "\n",
    "**Ventajas Técnicas:**\n",
    "- **Detección temprana**: Identifican cambios sutiles antes de que se manifiesten como fallas visibles\n",
    "- **Robustez ante offset**: Insensibles a sesgos constantes en los sensores\n",
    "- **Indicadores de transición**: Capturan momentos de cambio de estado operacional\n",
    "\n",
    "**Recomendaciones para Modelado:**\n",
    "- Aplicar **normalización robusta** (StandardScaler) debido a la sensibilidad al ruido\n",
    "- Considerar **filtros de suavizado** adicionales si el ruido de medición es significativo\n",
    "- Evaluar **ventanas adaptativas** basadas en condiciones operacionales\n",
    "\n",
    "### 🌊 Características Espectrales (FFT)\n",
    "\n",
    "El **análisis espectral** revela patrones de falla ocultos en el dominio temporal, proporcionando una dimensión adicional crítica para el diagnóstico:\n",
    "\n",
    "**Insights Físicos:**\n",
    "- **Energía espectral**: Correlaciona con nivel de vibración/ruido global del equipo\n",
    "- **Frecuencia dominante**: Identifica modos de falla específicos (desbalances, rodamientos)\n",
    "- **Concentración espectral**: Detecta cambios en la distribución de energía\n",
    "\n",
    "**Consideraciones de Implementación:**\n",
    "- **Ventanas de 72 horas** proporcionan resolución espectral adecuada para patrones de deterioro\n",
    "- **Detrending**: Esencial para remover componentes de baja frecuencia no relacionados con fallas\n",
    "- **Limitación computacional**: Restringir a variables con mayor contenido informativo\n",
    "\n",
    "### 🚨 Detección de Anomalías Estadísticas\n",
    "\n",
    "Los **Z-scores móviles robustos** proporcionan un mecanismo de detección de desviaciones que es:\n",
    "\n",
    "**Fortalezas del Método:**\n",
    "- **Robusto ante outliers**: Uso de mediana y MAD en lugar de media y desviación estándar\n",
    "- **Adaptativo**: Se ajusta a cambios graduales en condiciones operacionales\n",
    "- **Interpretable**: Z-scores > 2.5 indican desviaciones estadísticamente significativas\n",
    "\n",
    "**Recomendaciones Estratégicas:**\n",
    "- **Combinación de ventanas**: Las 3 ventanas (24H, 72H, 168H) capturan anomalías en diferentes escalas temporales\n",
    "- **Threshold adaptativo**: Considerar umbrales variables según la criticidad de cada variable\n",
    "- **Integración con rolling features**: Las anomalías complementan las características de ventanas móviles\n",
    "\n",
    "### 🎯 Recomendaciones para Fases Posteriores\n",
    "\n",
    "**Para Selección de Features (Feature Selection):**\n",
    "1. **Priorizar derivadas** de variables térmicas y de presión - históricamente más predictivas\n",
    "2. **Evaluar features espectrales** mediante análisis de correlación con eventos de falla\n",
    "3. **Utilizar anomalías** como features de activación/trigger para otros predictores\n",
    "\n",
    "**Para Entrenamiento del Modelo:**\n",
    "1. **Regularización L1/L2**: Esencial dada la alta dimensionalidad generada\n",
    "2. **Validación temporal**: Usar split temporal para evitar data leakage\n",
    "3. **Interpretabilidad**: Mantener trazabilidad hacia variables físicas originales\n",
    "\n",
    "**Para Despliegue Productivo:**\n",
    "1. **Monitoreo de drift**: Las características espectrales son sensibles a cambios operacionales\n",
    "2. **Computational efficiency**: Evaluar trade-off entre precisión y costo computacional\n",
    "3. **Robustez**: Implementar validaciones de calidad de datos en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo inteligente de valores faltantes generados por feature engineering\n",
    "print(\"🔧 Manejo de valores faltantes generados por feature engineering...\")\n",
    "\n",
    "# Análisis de valores faltantes antes del tratamiento\n",
    "print(f\"\\n📊 Análisis de completitud del dataset:\")\n",
    "valores_totales = df_features.size\n",
    "valores_faltantes = df_features.isnull().sum().sum()\n",
    "porcentaje_faltantes = (valores_faltantes / valores_totales) * 100\n",
    "\n",
    "print(f\"   💾 Dimensiones: {df_features.shape}\")\n",
    "print(f\"   📉 Valores faltantes: {valores_faltantes:,} ({porcentaje_faltantes:.2f}%)\")\n",
    "\n",
    "# Identificar columnas con mayor porcentaje de valores faltantes\n",
    "missing_por_columna = df_features.isnull().sum()\n",
    "missing_porcentaje = (missing_por_columna / len(df_features)) * 100\n",
    "columnas_problematicas = missing_porcentaje[missing_porcentaje > 50].sort_values(ascending=False)\n",
    "\n",
    "if not columnas_problematicas.empty:\n",
    "    print(f\"\\n⚠️  Columnas con >50% valores faltantes ({len(columnas_problematicas)} columnas):\")\n",
    "    for col, pct in columnas_problematicas.head(10).items():\n",
    "        print(f\"   {col}: {pct:.1f}% faltantes\")\n",
    "    \n",
    "    # Eliminar columnas con exceso de valores faltantes\n",
    "    print(f\"\\n🗑️  Eliminando columnas con >80% valores faltantes...\")\n",
    "    columnas_eliminar = missing_porcentaje[missing_porcentaje > 80].index\n",
    "    if len(columnas_eliminar) > 0:\n",
    "        df_features = df_features.drop(columns=columnas_eliminar)\n",
    "        print(f\"   ❌ {len(columnas_eliminar)} columnas eliminadas\")\n",
    "        print(f\"   📊 Dimensiones actualizadas: {df_features.shape}\")\n",
    "    else:\n",
    "        print(f\"   ✅ No hay columnas que requieran eliminación\")\n",
    "\n",
    "# Estrategia de relleno específica por tipo de característica\n",
    "print(f\"\\n🔄 Aplicando estrategia de relleno específica por tipo de feature...\")\n",
    "\n",
    "# 1. Rolling features: usar backward fill (valores hacia atrás)\n",
    "rolling_cols = [col for col in df_features.columns \n",
    "                if any(pattern in col for pattern in ['_mean_', '_std_', '_range_', '_q25_', '_q75_', '_cv_'])]\n",
    "if rolling_cols:\n",
    "    print(f\"   📊 Rolling features ({len(rolling_cols)} columnas): backward fill\")\n",
    "    df_features[rolling_cols] = df_features[rolling_cols].bfill()\n",
    "\n",
    "# 2. Lag features: eliminar filas iniciales afectadas\n",
    "lag_cols = [col for col in df_features.columns if '_lag_' in col]\n",
    "if lag_cols:\n",
    "    print(f\"   🕐 Lag features ({len(lag_cols)} columnas): mantener NaN inicial (natural)\")\n",
    "    # Los NaN iniciales en lag features son naturales y se manejarán en el modelado\n",
    "\n",
    "# 3. Derivative features: interpolación lineal\n",
    "deriv_cols = [col for col in df_features.columns if '_deriv_' in col or '_accel_' in col]\n",
    "if deriv_cols:\n",
    "    print(f\"   📈 Derivative features ({len(deriv_cols)} columnas): interpolación lineal\")\n",
    "    df_features[deriv_cols] = df_features[deriv_cols].interpolate(method='linear')\n",
    "\n",
    "# 4. FFT features: forward fill para preservar último estado conocido\n",
    "fft_cols = [col for col in df_features.columns if '_fft_' in col]\n",
    "if fft_cols:\n",
    "    print(f\"   🌊 FFT features ({len(fft_cols)} columnas): forward fill\")\n",
    "    df_features[fft_cols] = df_features[fft_cols].ffill()\n",
    "\n",
    "# 5. Anomaly features: rellenar con valor neutral (0 para binarios, 0 para z-scores)\n",
    "anomaly_cols = [col for col in df_features.columns if '_anomalia_' in col or '_zscore_' in col]\n",
    "if anomaly_cols:\n",
    "    print(f\"   🚨 Anomaly features ({len(anomaly_cols)} columnas): valor neutral (0)\")\n",
    "    for col in anomaly_cols:\n",
    "        if '_bin_' in col:  # Binarias\n",
    "            df_features[col] = df_features[col].fillna(0)\n",
    "        else:  # Z-scores y magnitudes\n",
    "            df_features[col] = df_features[col].fillna(0)\n",
    "\n",
    "# Relleno final con forward fill para cualquier NaN restante\n",
    "remaining_nan = df_features.isnull().sum().sum()\n",
    "if remaining_nan > 0:\n",
    "    print(f\"\\n🔄 Relleno final: {remaining_nan} valores faltantes restantes\")\n",
    "    df_features = df_features.ffill().bfill()  # Forward fill + backward fill como respaldo\n",
    "\n",
    "# Validación final\n",
    "final_nan = df_features.isnull().sum().sum()\n",
    "print(f\"\\n✅ Tratamiento completado:\")\n",
    "print(f\"   📊 Valores faltantes finales: {final_nan}\")\n",
    "print(f\"   💾 Dataset final: {df_features.shape}\")\n",
    "\n",
    "if final_nan == 0:\n",
    "    print(f\"   🎉 ¡Dataset completamente limpio!\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Quedan {final_nan} valores faltantes para revisión manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🏷️ Etiquetado de Datos - Creación de la Variable Objetivo\n",
    "\n",
    "### 🎯 Concepto de Ventana de Pre-Falla\n",
    "\n",
    "El **etiquetado de fallas** es el proceso más crítico en el desarrollo de un sistema de mantenimiento predictivo exitoso. Transformamos las fechas históricas de eventos de mantenimiento en **etiquetas de entrenamiento** que permiten al modelo aprender a reconocer patrones que preceden a las fallas.\n",
    "\n",
    "### ⏰ Justificación del Horizonte de 7 Días\n",
    "\n",
    "La selección de una **ventana de predicción de 7 días** se basa en múltiples consideraciones técnicas y operacionales:\n",
    "\n",
    "**Consideraciones Técnicas:**\n",
    "- **Tiempo de reacción operacional**: 7 días proporcionan tiempo suficiente para planificar intervenciones\n",
    "- **Balance señal-ruido**: Ventana suficientemente amplia para capturar deterioro progresivo\n",
    "- **Estabilidad del patrón**: Los moto-compresores exhiben patrones de pre-falla consistentes en esta escala temporal\n",
    "\n",
    "**Consideraciones Operacionales:**\n",
    "- **Planificación de mantenimiento**: Tiempo adecuado para adquisición de repuestos y programación\n",
    "- **Coordinación operacional**: Permite ajustes en cronogramas de producción\n",
    "- **Gestión de riesgos**: Balance entre alertas tempranas y falsos positivos\n",
    "\n",
    "### 📊 Metodología de Etiquetado\n",
    "\n",
    "Implementaremos un sistema de etiquetado binario donde:\n",
    "- **Etiqueta 1 (Pre-falla)**: Mediciones dentro de los 7 días anteriores a un evento de falla\n",
    "- **Etiqueta 0 (Normal)**: Todas las demás mediciones del dataset\n",
    "\n",
    "### ⚖️ Consideraciones de Desbalance de Clases\n",
    "\n",
    "Es fundamental reconocer que el **desbalance de clases** es inherente en problemas de mantenimiento predictivo:\n",
    "- Los períodos normales son naturalmente mucho más frecuentes que los pre-falla\n",
    "- Este desbalance refleja la realidad operacional y debe manejarse apropiadamente\n",
    "- Las técnicas de sampling y weighting serán críticas en la fase de modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación del etiquetado de fallas con ventana de 7 días\n",
    "print(\"🏷️ Iniciando proceso de etiquetado de fallas...\")\n",
    "print(f\"\\n📅 Configuración de etiquetado:\")\n",
    "print(f\"   ⏰ Ventana de predicción: 7 días\")\n",
    "print(f\"   🎯 Objetivo: Detectar patrones pre-falla 168 horas antes del evento\")\n",
    "\n",
    "# Definir ventana de pre-falla\n",
    "ventana_prefalla = pd.Timedelta(days=7)\n",
    "print(f\"   📊 Ventana técnica: {ventana_prefalla} ({ventana_prefalla.total_seconds()/3600:.0f} horas)\")\n",
    "\n",
    "# Inicializar columna de etiquetas\n",
    "df_features['falla'] = 0  # Estado normal por defecto\n",
    "print(f\"\\n📋 Columna 'falla' inicializada: {len(df_features)} registros en estado normal\")\n",
    "\n",
    "# Validar disponibilidad de fechas de falla\n",
    "if 'fechas_falla' not in locals() or len(fechas_falla) == 0:\n",
    "    print(f\"⚠️  ADVERTENCIA: No se encontraron fechas de falla válidas\")\n",
    "    print(f\"   Verificando datos de eventos...\")\n",
    "    \n",
    "    # Intentar recuperar fechas del DataFrame de eventos\n",
    "    if 'df_eventos' in locals() and 'fecha_evento' in df_eventos.columns:\n",
    "        fechas_validas = df_eventos['fecha_evento'].dropna()\n",
    "        if not fechas_validas.empty:\n",
    "            # Filtrar fechas dentro del rango de datos de sensores\n",
    "            fechas_falla = fechas_validas[\n",
    "                (fechas_validas >= df_features.index.min()) & \n",
    "                (fechas_validas <= df_features.index.max())\n",
    "            ].sort_values().tolist()\n",
    "            print(f\"   ✅ Recuperadas {len(fechas_falla)} fechas válidas\")\n",
    "        else:\n",
    "            print(f\"   ❌ No hay fechas válidas en el historial\")\n",
    "            fechas_falla = []\n",
    "\n",
    "if len(fechas_falla) == 0:\n",
    "    print(f\"\\n❌ CRÍTICO: Sin fechas de falla, no se puede realizar etiquetado\")\n",
    "    print(f\"   El dataset mantendrá todas las etiquetas como 0 (normal)\")\n",
    "    eventos_procesados = 0\n",
    "    registros_etiquetados = 0\n",
    "else:\n",
    "    print(f\"\\n🔄 Procesando {len(fechas_falla)} eventos de falla para etiquetado...\")\n",
    "    \n",
    "    eventos_procesados = 0\n",
    "    registros_etiquetados = 0\n",
    "    \n",
    "    # Información del rango temporal del dataset\n",
    "    dataset_start = df_features.index.min()\n",
    "    dataset_end = df_features.index.max()\n",
    "    print(f\"   📊 Rango del dataset: {dataset_start} a {dataset_end}\")\n",
    "    \n",
    "    for i, fecha_falla in enumerate(fechas_falla, 1):\n",
    "        print(f\"\\n   📅 Procesando evento {i}/{len(fechas_falla)}: {fecha_falla}\")\n",
    "        \n",
    "        # Calcular ventana de pre-falla\n",
    "        inicio_ventana = fecha_falla - ventana_prefalla\n",
    "        fin_ventana = fecha_falla\n",
    "        \n",
    "        print(f\"      🕐 Ventana pre-falla: {inicio_ventana} a {fin_ventana}\")\n",
    "        \n",
    "        # Verificar si la ventana intersecta con nuestros datos\n",
    "        if fin_ventana < dataset_start:\n",
    "            print(f\"      ⚠️  Evento anterior al dataset - saltando\")\n",
    "            continue\n",
    "        \n",
    "        if inicio_ventana > dataset_end:\n",
    "            print(f\"      ⚠️  Evento posterior al dataset - saltando\")\n",
    "            continue\n",
    "        \n",
    "        # Ajustar ventana a los límites del dataset si es necesario\n",
    "        inicio_efectivo = max(inicio_ventana, dataset_start)\n",
    "        fin_efectivo = min(fin_ventana, dataset_end)\n",
    "        \n",
    "        # Crear máscara para seleccionar registros en la ventana\n",
    "        mask_ventana = (\n",
    "            (df_features.index >= inicio_efectivo) & \n",
    "            (df_features.index < fin_efectivo)\n",
    "        )\n",
    "        \n",
    "        # Contar registros en la ventana\n",
    "        registros_ventana = mask_ventana.sum()\n",
    "        \n",
    "        if registros_ventana > 0:\n",
    "            # Etiquetar registros como pre-falla\n",
    "            df_features.loc[mask_ventana, 'falla'] = 1\n",
    "            \n",
    "            registros_etiquetados += registros_ventana\n",
    "            eventos_procesados += 1\n",
    "            \n",
    "            print(f\"      ✅ {registros_ventana} registros etiquetados como pre-falla\")\n",
    "        else:\n",
    "            print(f\"      ⚠️  No hay registros en esta ventana\")\n",
    "\n",
    "# Análisis final del etiquetado\n",
    "print(f\"\\n📊 RESUMEN DEL ETIQUETADO:\")\n",
    "conteo_etiquetas = df_features['falla'].value_counts().sort_index()\n",
    "\n",
    "normal_count = conteo_etiquetas.get(0, 0)\n",
    "prefalla_count = conteo_etiquetas.get(1, 0)\n",
    "total_registros = len(df_features)\n",
    "\n",
    "print(f\"   📈 Total de registros: {total_registros:,}\")\n",
    "print(f\"   ✅ Estado normal (0): {normal_count:,} ({normal_count/total_registros*100:.2f}%)\")\n",
    "print(f\"   🚨 Pre-falla (1): {prefalla_count:,} ({prefalla_count/total_registros*100:.2f}%)\")\n",
    "print(f\"   🎯 Eventos procesados: {eventos_procesados}/{len(fechas_falla) if fechas_falla else 0}\")\n",
    "\n",
    "# Análisis de balance de clases\n",
    "if prefalla_count > 0:\n",
    "    ratio_desbalance = normal_count / prefalla_count\n",
    "    print(f\"\\n⚖️  ANÁLISIS DE BALANCE DE CLASES:\")\n",
    "    print(f\"   📊 Ratio normal:pre-falla = {ratio_desbalance:.1f}:1\")\n",
    "    \n",
    "    if ratio_desbalance > 10:\n",
    "        print(f\"   ⚠️  DESBALANCE SIGNIFICATIVO DETECTADO\")\n",
    "        print(f\"   💡 Recomendaciones para modelado:\")\n",
    "        print(f\"      - Usar class_weight='balanced' en algoritmos\")\n",
    "        print(f\"      - Considerar técnicas de sampling (SMOTE, undersampling)\")\n",
    "        print(f\"      - Evaluar con métricas robustas (F1, AUC-ROC, Precision-Recall)\")\n",
    "    else:\n",
    "        print(f\"   ✅ Balance aceptable para entrenamiento directo\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  ADVERTENCIA: Sin registros pre-falla etiquetados\")\n",
    "    print(f\"   El modelo no podrá aprender patrones de falla\")\n",
    "    print(f\"   Verificar datos de eventos y rangos temporales\")\n",
    "\n",
    "print(f\"\\n✅ Etiquetado de fallas completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Validaciones de Calidad del Etiquetado\n",
    "\n",
    "### 📊 Importancia de la Validación Post-Etiquetado\n",
    "\n",
    "Las validaciones de calidad son esenciales para garantizar que nuestro etiquetado refleje fielmente la realidad operacional del moto-compresor y proporcione bases sólidas para el entrenamiento del modelo.\n",
    "\n",
    "### 🎯 Validaciones Críticas a Implementar\n",
    "\n",
    "1. **Solapamiento de Ventanas**: Verificar que las ventanas de pre-falla no se solapen excesivamente\n",
    "2. **Distribución Temporal**: Asegurar cobertura temporal adecuada de eventos\n",
    "3. **Suficiencia de Datos**: Confirmar que hay suficientes datos normales entre eventos\n",
    "4. **Coherencia Física**: Validar que los patrones etiquetados son físicamente plausibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaciones exhaustivas de calidad del etiquetado\n",
    "print(\"🔍 Ejecutando validaciones de calidad del etiquetado...\")\n",
    "\n",
    "# Validación 1: Análisis de solapamiento de ventanas\n",
    "print(f\"\\n1️⃣ VALIDACIÓN DE SOLAPAMIENTO DE VENTANAS:\")\n",
    "\n",
    "if len(fechas_falla) > 1:\n",
    "    # Calcular distancias entre eventos consecutivos\n",
    "    fechas_ordenadas = sorted(fechas_falla)\n",
    "    distancias_eventos = []\n",
    "    \n",
    "    for i in range(1, len(fechas_ordenadas)):\n",
    "        distancia = fechas_ordenadas[i] - fechas_ordenadas[i-1]\n",
    "        distancias_eventos.append(distancia)\n",
    "        print(f\"   📅 Evento {i} → {i+1}: {distancia} ({distancia.total_seconds()/3600:.0f} horas)\")\n",
    "    \n",
    "    # Identificar solapamientos (distancia < 14 días = 2 ventanas)\n",
    "    ventana_doble = pd.Timedelta(days=14)\n",
    "    solapamientos = [d for d in distancias_eventos if d < ventana_doble]\n",
    "    \n",
    "    if solapamientos:\n",
    "        print(f\"   ⚠️  SOLAPAMIENTOS DETECTADOS: {len(solapamientos)} pares de eventos\")\n",
    "        print(f\"   💡 Implicación: Algunas mediciones pueden tener etiquetado ambiguo\")\n",
    "        print(f\"   🔧 Recomendación: Considerar ventanas más cortas o consolidar eventos cercanos\")\n",
    "    else:\n",
    "        print(f\"   ✅ No hay solapamientos críticos entre ventanas\")\n",
    "else:\n",
    "    print(f\"   ℹ️  Solo hay {len(fechas_falla)} eventos - no aplicable análisis de solapamiento\")\n",
    "\n",
    "# Validación 2: Distribución temporal de etiquetas\n",
    "print(f\"\\n2️⃣ VALIDACIÓN DE DISTRIBUCIÓN TEMPORAL:\")\n",
    "\n",
    "if prefalla_count > 0:\n",
    "    # Identificar períodos de pre-falla\n",
    "    registros_prefalla = df_features[df_features['falla'] == 1]\n",
    "    \n",
    "    if not registros_prefalla.empty:\n",
    "        inicio_prefallas = registros_prefalla.index.min()\n",
    "        fin_prefallas = registros_prefalla.index.max()\n",
    "        duracion_prefallas = fin_prefallas - inicio_prefallas\n",
    "        \n",
    "        print(f\"   📊 Período de pre-fallas: {inicio_prefallas} a {fin_prefallas}\")\n",
    "        print(f\"   ⏱️  Duración total con etiquetas pre-falla: {duracion_prefallas}\")\n",
    "        \n",
    "        # Calcular densidad de etiquetas pre-falla\n",
    "        duracion_dataset = df_features.index.max() - df_features.index.min()\n",
    "        cobertura_prefalla = (duracion_prefallas.total_seconds() / duracion_dataset.total_seconds()) * 100\n",
    "        \n",
    "        print(f\"   📈 Cobertura temporal pre-falla: {cobertura_prefalla:.1f}% del dataset\")\n",
    "        \n",
    "        if cobertura_prefalla < 5:\n",
    "            print(f\"   ⚠️  Cobertura muy baja - modelo puede tener dificultades de aprendizaje\")\n",
    "        elif cobertura_prefalla > 30:\n",
    "            print(f\"   ⚠️  Cobertura muy alta - verificar lógica de etiquetado\")\n",
    "        else:\n",
    "            print(f\"   ✅ Cobertura temporal adecuada para entrenamiento\")\n",
    "\n",
    "# Validación 3: Suficiencia de datos normales entre eventos\n",
    "print(f\"\\n3️⃣ VALIDACIÓN DE PERÍODOS NORMALES:\")\n",
    "\n",
    "if len(fechas_falla) > 0 and prefalla_count > 0:\n",
    "    # Identificar gaps entre ventanas de pre-falla\n",
    "    registros_prefalla = df_features[df_features['falla'] == 1]\n",
    "    \n",
    "    # Encontrar períodos continuos de pre-falla\n",
    "    diff_index = registros_prefalla.index.to_series().diff()\n",
    "    cambios_periodo = diff_index[diff_index > pd.Timedelta(hours=2)]  # Gaps > 2 horas\n",
    "    \n",
    "    print(f\"   📊 Períodos pre-falla identificados: {len(cambios_periodo) + 1 if not registros_prefalla.empty else 0}\")\n",
    "    \n",
    "    # Calcular período normal más largo\n",
    "    registros_normales = df_features[df_features['falla'] == 0]\n",
    "    if not registros_normales.empty:\n",
    "        # Encontrar el gap más largo entre registros normales consecutivos\n",
    "        normal_diffs = registros_normales.index.to_series().diff()\n",
    "        max_gap_normal = normal_diffs.max()\n",
    "        \n",
    "        print(f\"   ⏱️  Período normal continuo más largo: {max_gap_normal}\")\n",
    "        \n",
    "        if max_gap_normal > pd.Timedelta(days=30):\n",
    "            print(f\"   ✅ Suficientes períodos normales extensos para entrenamiento\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Períodos normales pueden ser insuficientes\")\n",
    "\n",
    "# Validación 4: Coherencia estadística\n",
    "print(f\"\\n4️⃣ VALIDACIÓN DE COHERENCIA ESTADÍSTICA:\")\n",
    "\n",
    "if prefalla_count > 10:  # Mínimo para análisis estadístico\n",
    "    # Comparar estadísticas básicas entre períodos normales y pre-falla\n",
    "    print(f\"   📊 Análisis comparativo de períodos normales vs pre-falla:\")\n",
    "    \n",
    "    # Seleccionar variables críticas para comparación\n",
    "    variables_comparacion = variables_criticas[:3]  # Top 3 variables\n",
    "    \n",
    "    diferencias_significativas = 0\n",
    "    \n",
    "    for variable in variables_comparacion:\n",
    "        if variable in df_features.columns:\n",
    "            normal_stats = df_features[df_features['falla'] == 0][variable].describe()\n",
    "            prefalla_stats = df_features[df_features['falla'] == 1][variable].describe()\n",
    "            \n",
    "            # Comparar medias\n",
    "            diff_media = abs(prefalla_stats['mean'] - normal_stats['mean'])\n",
    "            std_normal = normal_stats['std']\n",
    "            \n",
    "            if std_normal > 0:\n",
    "                z_score_diff = diff_media / std_normal\n",
    "                \n",
    "                if z_score_diff > 1.0:  # Diferencia > 1 desviación estándar\n",
    "                    diferencias_significativas += 1\n",
    "                    print(f\"      📈 {variable}: diferencia significativa (Z={z_score_diff:.2f})\")\n",
    "                else:\n",
    "                    print(f\"      📊 {variable}: diferencia menor (Z={z_score_diff:.2f})\")\n",
    "    \n",
    "    if diferencias_significativas > 0:\n",
    "        print(f\"   ✅ {diferencias_significativas}/{len(variables_comparacion)} variables muestran patrones diferenciados\")\n",
    "        print(f\"   💡 El etiquetado parece capturar cambios reales en el comportamiento\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  No se detectaron diferencias significativas\")\n",
    "        print(f\"   💡 Revisar si las ventanas de 7 días son apropiadas para este equipo\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Insuficientes registros pre-falla ({prefalla_count}) para análisis estadístico\")\n",
    "\n",
    "# Resumen final de validaciones\n",
    "print(f\"\\n✅ VALIDACIONES DE CALIDAD COMPLETADAS\")\n",
    "print(f\"   📊 Dataset listo para entrenamiento: {df_features.shape}\")\n",
    "print(f\"   🏷️  Etiquetas distribuidas: {normal_count:,} normal, {prefalla_count:,} pre-falla\")\n",
    "print(f\"   🎯 Próximo paso: Entrenamiento de modelos de Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 💾 Preparación Final y Guardado del Dataset\n",
    "\n",
    "### 🎯 Optimización Final del Dataset\n",
    "\n",
    "Antes del guardado, realizaremos las optimizaciones finales para garantizar que el dataset esté en condiciones óptimas para el entrenamiento de modelos de Machine Learning:\n",
    "\n",
    "### 🔧 Tareas de Preparación Final\n",
    "\n",
    "1. **Validación de integridad**: Verificación final de tipos de datos y valores faltantes\n",
    "2. **Optimización de memoria**: Conversión a tipos de datos eficientes\n",
    "3. **Documentación de características**: Catalogación de todas las features generadas\n",
    "4. **Guardado en formato optimizado**: Parquet con compresión para eficiencia\n",
    "\n",
    "### 📊 Estructura del Dataset Final\n",
    "\n",
    "El dataset final contendrá:\n",
    "- **Variables originales**: Mediciones directas de sensores seleccionadas\n",
    "- **Rolling features**: Estadísticas de ventanas móviles (6H, 24H, 72H)\n",
    "- **Lag features**: Características de retraso (2H, 12H, 48H)\n",
    "- **Características avanzadas**: Derivadas, FFT, y detección de anomalías\n",
    "- **Variable objetivo**: Etiqueta binaria de falla para entrenamiento supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación final y optimización del dataset\n",
    "print(\"🔧 Iniciando preparación final del dataset...\")\n",
    "\n",
    "# 1. Validación final de integridad\n",
    "print(f\"\\n1️⃣ VALIDACIÓN FINAL DE INTEGRIDAD:\")\n",
    "\n",
    "# Verificar valores faltantes restantes\n",
    "valores_faltantes_finales = df_features.isnull().sum().sum()\n",
    "print(f\"   📊 Valores faltantes finales: {valores_faltantes_finales}\")\n",
    "\n",
    "if valores_faltantes_finales > 0:\n",
    "    print(f\"   ⚠️  Eliminando filas con valores faltantes restantes...\")\n",
    "    filas_iniciales = len(df_features)\n",
    "    df_features = df_features.dropna()\n",
    "    filas_eliminadas = filas_iniciales - len(df_features)\n",
    "    print(f\"   🗑️  {filas_eliminadas} filas eliminadas ({filas_eliminadas/filas_iniciales*100:.2f}%)\")\n",
    "\n",
    "# Verificar tipos de datos\n",
    "tipos_datos = df_features.dtypes.value_counts()\n",
    "print(f\"\\n   📋 Distribución de tipos de datos:\")\n",
    "for tipo, cantidad in tipos_datos.items():\n",
    "    print(f\"      {tipo}: {cantidad} columnas\")\n",
    "\n",
    "# 2. Optimización de memoria\n",
    "print(f\"\\n2️⃣ OPTIMIZACIÓN DE MEMORIA:\")\n",
    "\n",
    "memoria_inicial = df_features.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"   💾 Uso de memoria inicial: {memoria_inicial:.1f} MB\")\n",
    "\n",
    "# Optimizar tipos numéricos\n",
    "print(f\"   🔄 Optimizando tipos numéricos...\")\n",
    "\n",
    "for col in df_features.select_dtypes(include=['float64']).columns:\n",
    "    if col != 'falla':  # Preservar la variable objetivo\n",
    "        # Convertir a float32 si el rango lo permite\n",
    "        col_min, col_max = df_features[col].min(), df_features[col].max()\n",
    "        if col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max:\n",
    "            df_features[col] = df_features[col].astype(np.float32)\n",
    "\n",
    "# Optimizar variable objetivo\n",
    "if 'falla' in df_features.columns:\n",
    "    df_features['falla'] = df_features['falla'].astype(np.uint8)\n",
    "\n",
    "memoria_optimizada = df_features.memory_usage(deep=True).sum() / 1024**2\n",
    "reduccion_memoria = ((memoria_inicial - memoria_optimizada) / memoria_inicial) * 100\n",
    "\n",
    "print(f\"   💾 Uso de memoria optimizado: {memoria_optimizada:.1f} MB\")\n",
    "print(f\"   📉 Reducción de memoria: {reduccion_memoria:.1f}%\")\n",
    "\n",
    "# 3. Catalogación de características generadas\n",
    "print(f\"\\n3️⃣ CATALOGACIÓN DE CARACTERÍSTICAS:\")\n",
    "\n",
    "# Clasificar features por tipo\n",
    "feature_categories = {\n",
    "    'originales': variables_criticas,\n",
    "    'rolling': [col for col in df_features.columns if any(pattern in col for pattern in ['_mean_', '_std_', '_range_', '_q25_', '_q75_', '_cv_'])],\n",
    "    'lag': [col for col in df_features.columns if '_lag_' in col or '_diff_' in col or '_ratio_' in col],\n",
    "    'derivadas': [col for col in df_features.columns if '_deriv_' in col or '_accel_' in col],\n",
    "    'fft': [col for col in df_features.columns if '_fft_' in col],\n",
    "    'anomalias': [col for col in df_features.columns if '_anomalia_' in col or '_zscore_' in col],\n",
    "    'objetivo': ['falla']\n",
    "}\n",
    "\n",
    "print(f\"   📊 Resumen de características por categoría:\")\n",
    "total_features = 0\n",
    "for categoria, features in feature_categories.items():\n",
    "    count = len([f for f in features if f in df_features.columns])\n",
    "    total_features += count\n",
    "    print(f\"      {categoria.capitalize()}: {count} features\")\n",
    "\n",
    "print(f\"   🎯 Total de características: {total_features}\")\n",
    "\n",
    "# 4. Guardado del dataset final\n",
    "print(f\"\\n4️⃣ GUARDADO DEL DATASET FINAL:\")\n",
    "\n",
    "# Definir archivo de salida\n",
    "archivo_salida = ruta_processed / 'dataset_etiquetado_para_modelado.parquet'\n",
    "\n",
    "print(f\"   📁 Archivo de destino: {archivo_salida}\")\n",
    "print(f\"   💾 Dimensiones finales: {df_features.shape}\")\n",
    "\n",
    "try:\n",
    "    # Guardar con configuración optimizada\n",
    "    df_features.to_parquet(\n",
    "        archivo_salida,\n",
    "        engine='pyarrow',\n",
    "        compression='snappy',\n",
    "        index=True  # Preservar índice temporal\n",
    "    )\n",
    "    \n",
    "    # Verificar archivo guardado\n",
    "    tamaño_archivo = archivo_salida.stat().st_size / 1024**2\n",
    "    print(f\"   ✅ Dataset guardado exitosamente\")\n",
    "    print(f\"   📊 Tamaño del archivo: {tamaño_archivo:.1f} MB\")\n",
    "    print(f\"   🔧 Compresión: Snappy\")\n",
    "    print(f\"   📅 Índice temporal: Preservado\")\n",
    "    \n",
    "    # Guardado de respaldo en CSV (para compatibilidad)\n",
    "    archivo_csv = ruta_processed / 'dataset_etiquetado_para_modelado.csv'\n",
    "    df_features.to_csv(archivo_csv, index=True, encoding='utf-8')\n",
    "    tamaño_csv = archivo_csv.stat().st_size / 1024**2\n",
    "    print(f\"   💾 Respaldo CSV generado: {tamaño_csv:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error al guardar: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 5. Generación de metadatos del dataset\n",
    "print(f\"\\n5️⃣ GENERACIÓN DE METADATOS:\")\n",
    "\n",
    "metadata_file = ruta_processed / 'feature_engineering_metadata.txt'\n",
    "\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"METADATOS DE FEATURE ENGINEERING\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    f.write(f\"Fecha de generación: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Dimensiones del dataset: {df_features.shape[0]:,} filas × {df_features.shape[1]} columnas\\n\")\n",
    "    f.write(f\"Período temporal: {df_features.index.min()} a {df_features.index.max()}\\n\")\n",
    "    f.write(f\"Uso de memoria: {memoria_optimizada:.1f} MB\\n\\n\")\n",
    "    \n",
    "    f.write(\"DISTRIBUCIÓN DE ETIQUETAS:\\n\")\n",
    "    conteo_etiquetas = df_features['falla'].value_counts().sort_index()\n",
    "    for etiqueta, cantidad in conteo_etiquetas.items():\n",
    "        porcentaje = (cantidad / len(df_features)) * 100\n",
    "        f.write(f\"  Etiqueta {etiqueta}: {cantidad:,} ({porcentaje:.2f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\nCATEGORÍAS DE CARACTERÍSTICAS:\\n\")\n",
    "    for categoria, features in feature_categories.items():\n",
    "        count = len([f for f in features if f in df_features.columns])\n",
    "        f.write(f\"  {categoria.capitalize()}: {count} features\\n\")\n",
    "    \n",
    "    f.write(f\"\\nVARIABLES CRÍTICAS ORIGINALES:\\n\")\n",
    "    for i, var in enumerate(variables_criticas, 1):\n",
    "        f.write(f\"  {i}. {var}\\n\")\n",
    "    \n",
    "    if len(fechas_falla) > 0:\n",
    "        f.write(f\"\\nEVENTOS DE FALLA PROCESADOS:\\n\")\n",
    "        for i, fecha in enumerate(fechas_falla, 1):\n",
    "            f.write(f\"  {i}. {fecha}\\n\")\n",
    "\n",
    "print(f\"   📄 Metadatos guardados en: {metadata_file}\")\n",
    "\n",
    "print(f\"\\n🎉 FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"\\n📁 Archivos generados:\")\n",
    "print(f\"   🗃️  dataset_etiquetado_para_modelado.parquet - Dataset principal optimizado\")\n",
    "print(f\"   💾 dataset_etiquetado_para_modelado.csv - Respaldo en formato CSV\")\n",
    "print(f\"   📄 feature_engineering_metadata.txt - Metadatos detallados\")\n",
    "print(f\"\\n➡️  Listo para la siguiente fase: Entrenamiento de Modelos (04_model_training.ipynb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Resumen Ejecutivo del Feature Engineering\n",
    "\n",
    "### ✅ Logros Técnicos Alcanzados\n",
    "\n",
    "Hemos completado exitosamente la **transformación de datos operacionales** en un **dataset enriquecido y etiquetado**, optimizado para el entrenamiento de modelos de mantenimiento predictivo. Los principales logros incluyen:\n",
    "\n",
    "**🔬 Ingeniería de Características Avanzada:**\n",
    "- **Rolling Features**: Estadísticas de ventanas móviles en 3 escalas temporales (6H, 24H, 72H)\n",
    "- **Lag Features**: Características de memoria histórica con intervalos optimizados (2H, 12H, 48H)\n",
    "- **Características Espectrales**: Análisis FFT para detección de patrones de frecuencia\n",
    "- **Detección de Anomalías**: Z-scores móviles robustos para identificación de desviaciones\n",
    "- **Análisis de Derivadas**: Tasas de cambio para capturar velocidad de deterioro\n",
    "\n",
    "**🏷️ Sistema de Etiquetado Robusto:**\n",
    "- Ventana de predicción de **7 días** basada en requisitos operacionales\n",
    "- Procesamiento exitoso de eventos históricos de falla\n",
    "- Validaciones exhaustivas de calidad y coherencia temporal\n",
    "- Balance documentado de clases para estrategias de modelado\n",
    "\n",
    "**💾 Optimización y Preparación:**\n",
    "- Reducción significativa del uso de memoria mediante optimización de tipos\n",
    "- Formato Parquet con compresión para eficiencia de almacenamiento\n",
    "- Catalogación completa de características por categoría\n",
    "- Metadatos exhaustivos para trazabilidad y reproducibilidad\n",
    "\n",
    "### 🎯 Dataset Final - Especificaciones Técnicas\n",
    "\n",
    "El dataset resultante representa un **activo de alta calidad** para el desarrollo del modelo predictivo:\n",
    "\n",
    "- **Dimensionalidad**: Centenares de características especializadas\n",
    "- **Cobertura Temporal**: Datos históricos con resolución horaria\n",
    "- **Integridad**: 100% de completitud después del procesamiento\n",
    "- **Etiquetado**: Sistema binario con horizonte de predicción validado\n",
    "- **Optimización**: Memoria y almacenamiento optimizados para producción\n",
    "\n",
    "### 🚀 Preparación para Modelado\n",
    "\n",
    "El dataset está **completamente preparado** para las siguientes fases:\n",
    "\n",
    "**Entrenamiento de Modelos:**\n",
    "- Características numéricas normalizadas y consistentes\n",
    "- Variable objetivo correctamente balanceada y validada\n",
    "- Metadatos disponibles para selección inteligente de features\n",
    "\n",
    "**Validación y Evaluación:**\n",
    "- Estructura temporal preservada para validación realista\n",
    "- Diversidad de características para análisis de importancia\n",
    "- Trazabilidad hacia variables físicas originales\n",
    "\n",
    "### 💡 Valor Agregado del Feature Engineering\n",
    "\n",
    "La transformación realizada agrega **valor predictivo sustancial**:\n",
    "\n",
    "- **Captura de Dinámicas Temporales**: Las características temporales revelan patrones de deterioro progresivo\n",
    "- **Robustez ante Ruido**: Las estadísticas móviles filtran fluctuaciones operacionales normales\n",
    "- **Detección Temprana**: Las características avanzadas permiten identificación precoz de anomalías\n",
    "- **Interpretabilidad**: Mantenimiento de conexión con parámetros físicos del equipo\n",
    "\n",
    "---\n",
    "\n",
    "**📊 Estado del Proyecto**: ✅ **Feature Engineering Completado**  \n",
    "**🎯 Próxima fase**: `04_model_training.ipynb` - Entrenamiento y Optimización de Modelos  \n",
    "**💾 Datasets disponibles**: Parquet optimizado + CSV de respaldo + Metadatos completos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}