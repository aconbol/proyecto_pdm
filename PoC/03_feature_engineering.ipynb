{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IngenierÃ­a de CaracterÃ­sticas para Mantenimiento Predictivo\n",
    "## Proyecto: PredicciÃ³n de Fallas en Moto-Compresores - Oil & Gas\n",
    "\n",
    "### ğŸ¯ Objetivo del Notebook\n",
    "\n",
    "Este notebook constituye la **fase crÃ­tica** de transformaciÃ³n de datos donde convertimos las series temporales limpias en un dataset enriquecido y etiquetado, optimizado para el entrenamiento de modelos de Machine Learning. Nuestro objetivo principal es **predecir fallas en moto-compresores con 7 dÃ­as de antelaciÃ³n**, una ventana temporal que permite la planificaciÃ³n efectiva de mantenimientos preventivos en el sector Oil & Gas.\n",
    "\n",
    "### ğŸ“‹ Tareas Principales\n",
    "\n",
    "1. **Carga y ValidaciÃ³n de Datos**: Integrar el dataset preprocesado con el historial de eventos\n",
    "2. **IngenierÃ­a de CaracterÃ­sticas Temporales**: Crear features que capturen la dinÃ¡mica del deterioro\n",
    "3. **CaracterÃ­sticas Avanzadas**: Implementar features de tasas de cambio, frecuencia y detecciÃ³n de anomalÃ­as\n",
    "4. **Etiquetado de Fallas**: Crear la variable objetivo basada en ventanas de pre-falla de 7 dÃ­as\n",
    "5. **ValidaciÃ³n y PreparaciÃ³n Final**: Garantizar calidad de datos para modelado\n",
    "\n",
    "### ğŸ› ï¸ LibrerÃ­as Especializadas\n",
    "\n",
    "Utilizaremos un stack tecnolÃ³gico optimizado para anÃ¡lisis de series temporales industriales:\n",
    "- **pandas**: ManipulaciÃ³n de series temporales y DataFrames\n",
    "- **numpy**: Operaciones numÃ©ricas y cÃ¡lculos matriciales\n",
    "- **scipy**: Transformadas de Fourier y anÃ¡lisis de seÃ±ales\n",
    "- **scikit-learn**: DetecciÃ³n de anomalÃ­as y normalizaciÃ³n\n",
    "- **pathlib**: Manejo robusto de rutas de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportaciÃ³n de librerÃ­as esenciales para ingenierÃ­a de caracterÃ­sticas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# LibrerÃ­as especializadas para anÃ¡lisis de seÃ±ales y anomalÃ­as\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# ConfiguraciÃ³n del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas exitosamente\")\n",
    "print(f\"ğŸ“Š VersiÃ³n de pandas: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ VersiÃ³n de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n de rutas de datos con validaciÃ³n de existencia\n",
    "# Esta configuraciÃ³n garantiza la reproducibilidad del pipeline\n",
    "\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('..')\n",
    "\n",
    "# Rutas especÃ­ficas para datos procesados y eventos\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Archivos especÃ­ficos requeridos\n",
    "archivo_timeseries = ruta_processed / 'timeseries_data.parquet'\n",
    "archivo_historial = ruta_eventos / 'Historial C1 RGD.xlsx'\n",
    "\n",
    "# ValidaciÃ³n crÃ­tica de existencia de archivos\n",
    "print(\"ğŸ“ ValidaciÃ³n de rutas y archivos:\")\n",
    "print(f\"   Datos procesados: {ruta_processed} - {'âœ… Existe' if ruta_processed.exists() else 'âŒ No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'âœ… Existe' if ruta_eventos.exists() else 'âŒ No existe'}\")\n",
    "print(f\"   Timeseries: {archivo_timeseries} - {'âœ… Existe' if archivo_timeseries.exists() else 'âŒ No existe'}\")\n",
    "print(f\"   Historial: {archivo_historial} - {'âœ… Existe' if archivo_historial.exists() else 'âŒ No existe'}\")\n",
    "\n",
    "# VerificaciÃ³n crÃ­tica - detener ejecuciÃ³n si faltan archivos esenciales\n",
    "archivos_requeridos = [archivo_timeseries, archivo_historial]\n",
    "archivos_faltantes = [arch for arch in archivos_requeridos if not arch.exists()]\n",
    "\n",
    "if archivos_faltantes:\n",
    "    print(f\"\\nâŒ ERROR CRÃTICO: Faltan archivos esenciales:\")\n",
    "    for archivo in archivos_faltantes:\n",
    "        print(f\"   - {archivo}\")\n",
    "    raise FileNotFoundError(\"No se pueden continuar sin los archivos de datos requeridos\")\n",
    "else:\n",
    "    print(\"\\nâœ… Todos los archivos requeridos estÃ¡n disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“‚ Carga y ValidaciÃ³n de Datos\n",
    "\n",
    "### ğŸ”„ Proceso de Carga Inteligente\n",
    "\n",
    "En esta fase crÃ­tica, cargaremos tanto el **dataset de series temporales procesado** como el **historial de eventos de mantenimiento**. La calidad de este proceso determina directamente la efectividad de nuestro modelo predictivo.\n",
    "\n",
    "El dataset de series temporales contiene las mediciones continuas de sensores del moto-compresor, ya limpias y preprocesadas. El historial de eventos proporciona las fechas exactas de las fallas histÃ³ricas, informaciÃ³n esencial para crear nuestras etiquetas de entrenamiento.\n",
    "\n",
    "### ğŸ“Š Estrategia de ValidaciÃ³n\n",
    "\n",
    "Implementaremos validaciones exhaustivas para garantizar:\n",
    "- **Integridad temporal**: Verificar que no hay gaps crÃ­ticos en los datos\n",
    "- **Consistencia de formato**: Asegurar que los Ã­ndices temporales estÃ¡n correctamente configurados\n",
    "- **Cobertura de eventos**: Confirmar que tenemos datos de sensores para las fechas de falla registradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset principal de series temporales\n",
    "print(\"ğŸ”„ Cargando dataset principal de series temporales...\")\n",
    "\n",
    "try:\n",
    "    # Cargar dataset con configuraciÃ³n especÃ­fica para series temporales\n",
    "    df = pd.read_parquet(archivo_timeseries)\n",
    "    \n",
    "    # ValidaciÃ³n inmediata de la estructura\n",
    "    print(f\"âœ… Dataset cargado exitosamente\")\n",
    "    print(f\"   ğŸ“Š Dimensiones: {df.shape[0]:,} filas Ã— {df.shape[1]} columnas\")\n",
    "    print(f\"   ğŸ“… Tipo de Ã­ndice: {type(df.index).__name__}\")\n",
    "    \n",
    "    # Verificar que el Ã­ndice temporal estÃ¡ correctamente configurado\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(f\"   â° Rango temporal: {df.index.min()} a {df.index.max()}\")\n",
    "        print(f\"   ğŸ“ˆ DuraciÃ³n total: {df.index.max() - df.index.min()}\")\n",
    "        \n",
    "        # AnÃ¡lisis de frecuencia de muestreo\n",
    "        diff_times = df.index.to_series().diff().dropna()\n",
    "        freq_mode = diff_times.mode().iloc[0] if not diff_times.empty else None\n",
    "        print(f\"   ğŸ•’ Frecuencia predominante: {freq_mode}\")\n",
    "        \n",
    "        # Detectar gaps significativos en los datos\n",
    "        large_gaps = diff_times[diff_times > pd.Timedelta(hours=2)]\n",
    "        if not large_gaps.empty:\n",
    "            print(f\"   âš ï¸  Gaps detectados: {len(large_gaps)} intervalos > 2 horas\")\n",
    "        else:\n",
    "            print(f\"   âœ… No se detectaron gaps significativos\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  ADVERTENCIA: Ãndice no es DatetimeIndex, convertir si es necesario\")\n",
    "        # Intentar conversiÃ³n si la primera columna parece ser temporal\n",
    "        if 'hora' in df.columns:\n",
    "            df = df.set_index('hora')\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            print(f\"   âœ… Ãndice convertido a DatetimeIndex usando columna 'hora'\")\n",
    "    \n",
    "    # InformaciÃ³n sobre las columnas disponibles\n",
    "    print(f\"\\nğŸ“‹ InformaciÃ³n de columnas:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"   ğŸ”¢ Columnas numÃ©ricas: {len(numeric_cols)}\")\n",
    "    print(f\"   ğŸ“Š Primeras 5 columnas: {list(df.columns[:5])}\")\n",
    "    \n",
    "    # EstadÃ­sticas de calidad bÃ¡sicas\n",
    "    missing_pct = (df.isnull().sum().sum() / df.size) * 100\n",
    "    print(f\"   ğŸ“‰ Porcentaje de valores faltantes: {missing_pct:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error cargando dataset principal: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y procesamiento del historial de eventos\n",
    "print(\"\\nğŸ”„ Cargando historial de eventos de mantenimiento...\")\n",
    "\n",
    "try:\n",
    "    # Cargar archivo Excel con manejo robusto\n",
    "    df_eventos = pd.read_excel(archivo_historial, engine='openpyxl')\n",
    "    \n",
    "    print(f\"âœ… Historial de eventos cargado\")\n",
    "    print(f\"   ğŸ“Š Dimensiones: {df_eventos.shape[0]} eventos Ã— {df_eventos.shape[1]} columnas\")\n",
    "    print(f\"   ğŸ“‹ Columnas disponibles: {list(df_eventos.columns)}\")\n",
    "    \n",
    "    # Mostrar muestra de los primeros registros\n",
    "    print(f\"\\nğŸ” Muestra de eventos registrados:\")\n",
    "    print(df_eventos.head())\n",
    "    \n",
    "    # Identificar y validar columnas de fecha\n",
    "    date_columns = [col for col in df_eventos.columns \n",
    "                   if any(keyword in col.upper() for keyword in ['FECHA', 'DATE', 'INICIO', 'START'])]\n",
    "    \n",
    "    print(f\"\\nğŸ“… Columnas de fecha detectadas: {date_columns}\")\n",
    "    \n",
    "    if not date_columns:\n",
    "        # Buscar en todas las columnas por contenido que parezca fechas\n",
    "        print(\"âš ï¸  No se detectaron columnas de fecha por nombre, analizando contenido...\")\n",
    "        for col in df_eventos.columns:\n",
    "            # Verificar si la columna contiene valores que puedan ser fechas\n",
    "            sample_values = df_eventos[col].dropna().head(3)\n",
    "            if not sample_values.empty:\n",
    "                print(f\"   Columna '{col}' - Muestra: {list(sample_values)}\")\n",
    "    \n",
    "    # Seleccionar columna principal de fecha (usar la primera encontrada o la mÃ¡s probable)\n",
    "    if date_columns:\n",
    "        primary_date_col = date_columns[0]\n",
    "        print(f\"\\nğŸ¯ Usando columna principal de fecha: '{primary_date_col}'\")\n",
    "        \n",
    "        # Limpiar y convertir fechas\n",
    "        fechas_originales = df_eventos[primary_date_col].copy()\n",
    "        \n",
    "        # Intentar conversiÃ³n a datetime con mÃºltiples formatos\n",
    "        try:\n",
    "            df_eventos['fecha_evento'] = pd.to_datetime(df_eventos[primary_date_col], errors='coerce')\n",
    "            fechas_validas = df_eventos['fecha_evento'].notna().sum()\n",
    "            total_fechas = len(df_eventos)\n",
    "            \n",
    "            print(f\"   âœ… ConversiÃ³n de fechas: {fechas_validas}/{total_fechas} fechas vÃ¡lidas\")\n",
    "            \n",
    "            if fechas_validas > 0:\n",
    "                fechas_limpias = df_eventos['fecha_evento'].dropna()\n",
    "                print(f\"   ğŸ“… Rango de eventos: {fechas_limpias.min()} a {fechas_limpias.max()}\")\n",
    "                print(f\"   ğŸ“Š Eventos Ãºnicos: {fechas_limpias.nunique()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error en conversiÃ³n de fechas: {str(e)}\")\n",
    "            print(f\"   ğŸ“‹ Valores de muestra para diagnÃ³stico: {list(fechas_originales.head())}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  ADVERTENCIA: No se pudo identificar automÃ¡ticamente la columna de fechas\")\n",
    "        print(f\"   Se requerirÃ¡ intervenciÃ³n manual para especificar la columna correcta\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error cargando historial de eventos: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidaciÃ³n de compatibilidad temporal entre datasets\n",
    "print(\"\\nğŸ” Validando compatibilidad temporal entre datasets...\")\n",
    "\n",
    "if isinstance(df.index, pd.DatetimeIndex) and 'fecha_evento' in df_eventos.columns:\n",
    "    # Rangos temporales de ambos datasets\n",
    "    sensor_start, sensor_end = df.index.min(), df.index.max()\n",
    "    eventos_limpios = df_eventos['fecha_evento'].dropna()\n",
    "    \n",
    "    if not eventos_limpios.empty:\n",
    "        eventos_start, eventos_end = eventos_limpios.min(), eventos_limpios.max()\n",
    "        \n",
    "        print(f\"ğŸ“Š AnÃ¡lisis de cobertura temporal:\")\n",
    "        print(f\"   ğŸ”§ Datos de sensores: {sensor_start} a {sensor_end}\")\n",
    "        print(f\"   ğŸ“… Eventos registrados: {eventos_start} a {eventos_end}\")\n",
    "        \n",
    "        # Calcular solapamiento temporal\n",
    "        overlap_start = max(sensor_start, eventos_start)\n",
    "        overlap_end = min(sensor_end, eventos_end)\n",
    "        \n",
    "        if overlap_start <= overlap_end:\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "            print(f\"   âœ… Solapamiento detectado: {overlap_start} a {overlap_end}\")\n",
    "            print(f\"   â±ï¸  DuraciÃ³n del solapamiento: {overlap_duration}\")\n",
    "            \n",
    "            # Eventos que caen dentro del rango de datos de sensores\n",
    "            eventos_utilizables = eventos_limpios[\n",
    "                (eventos_limpios >= sensor_start) & (eventos_limpios <= sensor_end)\n",
    "            ]\n",
    "            \n",
    "            print(f\"   ğŸ¯ Eventos utilizables para entrenamiento: {len(eventos_utilizables)}\")\n",
    "            \n",
    "            if len(eventos_utilizables) > 0:\n",
    "                print(f\"   ğŸ“‹ Fechas de eventos utilizables:\")\n",
    "                for i, evento in enumerate(eventos_utilizables, 1):\n",
    "                    print(f\"      {i}. {evento}\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  ADVERTENCIA: No hay eventos dentro del rango de datos de sensores\")\n",
    "        else:\n",
    "            print(f\"   âŒ No hay solapamiento temporal entre datasets\")\n",
    "            print(f\"   ğŸ’¡ Sugerencia: Verificar que los datos corresponden al mismo equipo y perÃ­odo\")\n",
    "    else:\n",
    "        print(f\"   âŒ No hay eventos vÃ¡lidos para analizar\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  No se puede realizar validaciÃ³n temporal - verificar formato de datos\")\n",
    "\n",
    "# Guardar informaciÃ³n de validaciÃ³n para uso posterior\n",
    "if 'eventos_utilizables' in locals() and len(eventos_utilizables) > 0:\n",
    "    fechas_falla = eventos_utilizables.sort_values().tolist()\n",
    "    print(f\"\\nâœ… ConfiguraciÃ³n completada: {len(fechas_falla)} eventos listos para etiquetado\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  ADVERTENCIA: ConfiguraciÃ³n incompleta - revisar datos de eventos\")\n",
    "    fechas_falla = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ”¬ SelecciÃ³n Inteligente de Variables CrÃ­ticas\n",
    "\n",
    "### ğŸ“Š MetodologÃ­a de SelecciÃ³n AutomÃ¡tica\n",
    "\n",
    "Antes de proceder con la ingenierÃ­a de caracterÃ­sticas, debemos identificar las **variables mÃ¡s relevantes** para el mantenimiento predictivo del moto-compresor. Esta selecciÃ³n se basa en criterios estadÃ­sticos y de ingenierÃ­a especÃ­ficos para equipos rotativos en Oil & Gas.\n",
    "\n",
    "### ğŸ¯ Criterios de SelecciÃ³n\n",
    "\n",
    "Aplicaremos mÃºltiples criterios para identificar las variables mÃ¡s predictivas:\n",
    "\n",
    "1. **Variabilidad Temporal**: Variables con suficiente variaciÃ³n para ser informativas\n",
    "2. **Completitud de Datos**: Variables con alta disponibilidad de mediciones\n",
    "3. **Relevancia FÃ­sica**: ParÃ¡metros crÃ­ticos conocidos en moto-compresores (temperaturas, presiones, vibraciones)\n",
    "4. **Sensibilidad a Fallas**: Variables que tÃ­picamente muestran deterioro progresivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelecciÃ³n inteligente de variables crÃ­ticas para feature engineering\n",
    "print(\"ğŸ” Seleccionando variables crÃ­ticas para ingenierÃ­a de caracterÃ­sticas...\")\n",
    "\n",
    "# Filtrar solo columnas numÃ©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"ğŸ“Š Variables numÃ©ricas disponibles: {len(numeric_cols)}\")\n",
    "\n",
    "# AnÃ¡lisis de calidad de datos por variable\n",
    "variable_quality = pd.DataFrame({\n",
    "    'variable': numeric_cols,\n",
    "    'completitud': df[numeric_cols].count() / len(df),\n",
    "    'variabilidad': df[numeric_cols].std() / df[numeric_cols].mean().abs(),  # Coeficiente de variaciÃ³n\n",
    "    'rango': df[numeric_cols].max() - df[numeric_cols].min(),\n",
    "    'valores_unicos': df[numeric_cols].nunique()\n",
    "})\n",
    "\n",
    "# Manejo de valores infinitos y NaN en variabilidad\n",
    "variable_quality['variabilidad'] = variable_quality['variabilidad'].replace([np.inf, -np.inf], np.nan)\n",
    "variable_quality['variabilidad'] = variable_quality['variabilidad'].fillna(0)\n",
    "\n",
    "# Criterios de filtrado especÃ­ficos para moto-compresores\n",
    "criterios_filtrado = {\n",
    "    'completitud_minima': 0.80,  # Al menos 80% de datos disponibles\n",
    "    'variabilidad_minima': 0.01,  # Coeficiente de variaciÃ³n mÃ­nimo\n",
    "    'valores_unicos_minimos': 10   # Al menos 10 valores Ãºnicos\n",
    "}\n",
    "\n",
    "# Aplicar filtros de calidad\n",
    "variables_validas = variable_quality[\n",
    "    (variable_quality['completitud'] >= criterios_filtrado['completitud_minima']) &\n",
    "    (variable_quality['variabilidad'] >= criterios_filtrado['variabilidad_minima']) &\n",
    "    (variable_quality['valores_unicos'] >= criterios_filtrado['valores_unicos_minimos'])\n",
    "]\n",
    "\n",
    "print(f\"âœ… Variables que cumplen criterios de calidad: {len(variables_validas)}\")\n",
    "\n",
    "# PriorizaciÃ³n por relevancia fÃ­sica en moto-compresores\n",
    "keywords_criticos = {\n",
    "    'temperatura': ['temp', 'temperatura', 'temperature'],\n",
    "    'presion': ['pres', 'presion', 'pressure'],\n",
    "    'rpm': ['rpm', 'velocidad', 'speed', 'rev'],\n",
    "    'vibracion': ['vib', 'vibracion', 'vibration'],\n",
    "    'flujo': ['flujo', 'flow', 'caudal'],\n",
    "    'potencia': ['potencia', 'power', 'watt']\n",
    "}\n",
    "\n",
    "# Scoring de relevancia fÃ­sica\n",
    "def calcular_score_relevancia(nombre_variable):\n",
    "    \"\"\"Calcula score de relevancia basado en palabras clave crÃ­ticas\"\"\"\n",
    "    score = 0\n",
    "    nombre_lower = nombre_variable.lower()\n",
    "    \n",
    "    for categoria, keywords in keywords_criticos.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in nombre_lower:\n",
    "                # Pesos especÃ­ficos por criticidad en moto-compresores\n",
    "                pesos = {'temperatura': 3, 'presion': 3, 'rpm': 2, 'vibracion': 2, 'flujo': 1, 'potencia': 1}\n",
    "                score += pesos.get(categoria, 1)\n",
    "                break\n",
    "    \n",
    "    return score\n",
    "\n",
    "variables_validas['score_relevancia'] = variables_validas['variable'].apply(calcular_score_relevancia)\n",
    "\n",
    "# Scoring combinado (normalizar cada componente entre 0-1)\n",
    "variables_validas['score_completitud'] = variables_validas['completitud']\n",
    "variables_validas['score_variabilidad'] = variables_validas['variabilidad'] / variables_validas['variabilidad'].max()\n",
    "variables_validas['score_relevancia_norm'] = variables_validas['score_relevancia'] / variables_validas['score_relevancia'].max() if variables_validas['score_relevancia'].max() > 0 else 0\n",
    "\n",
    "# Score final ponderado\n",
    "pesos_criterios = {'completitud': 0.3, 'variabilidad': 0.3, 'relevancia': 0.4}\n",
    "variables_validas['score_final'] = (\n",
    "    pesos_criterios['completitud'] * variables_validas['score_completitud'] +\n",
    "    pesos_criterios['variabilidad'] * variables_validas['score_variabilidad'] +\n",
    "    pesos_criterios['relevancia'] * variables_validas['score_relevancia_norm']\n",
    ")\n",
    "\n",
    "# SelecciÃ³n final de variables crÃ­ticas\n",
    "variables_seleccionadas = variables_validas.nlargest(12, 'score_final')  # Top 12 variables\n",
    "variables_criticas = variables_seleccionadas['variable'].tolist()\n",
    "\n",
    "print(f\"\\nğŸ¯ Variables crÃ­ticas seleccionadas ({len(variables_criticas)}):\")\n",
    "for i, var in enumerate(variables_criticas, 1):\n",
    "    score = variables_seleccionadas[variables_seleccionadas['variable'] == var]['score_final'].iloc[0]\n",
    "    completitud = variables_seleccionadas[variables_seleccionadas['variable'] == var]['completitud'].iloc[0]\n",
    "    print(f\"   {i:2d}. {var:<25} (Score: {score:.3f}, Completitud: {completitud:.1%})\")\n",
    "\n",
    "print(f\"\\nâœ… SelecciÃ³n completada: {len(variables_criticas)} variables listas para feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. âš™ï¸ IngenierÃ­a de CaracterÃ­sticas Temporales - Fundamentos\n",
    "\n",
    "### ğŸ¯ FilosofÃ­a del Feature Engineering en Mantenimiento Predictivo\n",
    "\n",
    "La **ingenierÃ­a de caracterÃ­sticas temporales** es el corazÃ³n de cualquier sistema de mantenimiento predictivo exitoso. A diferencia del anÃ¡lisis de datos estÃ¡ticos, los equipos industriales exhiben **patrones de deterioro progresivo** que se manifiestan a travÃ©s de cambios sutiles en mÃºltiples escalas temporales.\n",
    "\n",
    "### ğŸ“Š Escalas Temporales en Moto-Compresores\n",
    "\n",
    "Los moto-compresores en Oil & Gas operan con dinÃ¡micas complejas que requieren anÃ¡lisis multi-escala:\n",
    "\n",
    "- **Corto Plazo (1-6 horas)**: Fluctuaciones operacionales, cambios de carga, efectos tÃ©rmicos\n",
    "- **Mediano Plazo (12-48 horas)**: Tendencias de degradaciÃ³n, efectos de fatiga, acumulaciÃ³n de contaminantes\n",
    "- **Largo Plazo (72+ horas)**: Deterioro estructural, desgaste progresivo, degradaciÃ³n de componentes\n",
    "\n",
    "### ğŸ”¬ Ventanas MÃ³viles (Rolling Features)\n",
    "\n",
    "Las **ventanas mÃ³viles** permiten capturar el comportamiento estadÃ­stico reciente del equipo, suavizando el ruido inherente en las mediciones industriales mientras preservan las tendencias crÃ­ticas. Implementaremos ventanas optimizadas para la dinÃ¡mica del moto-compresor:\n",
    "\n",
    "- **Ventana 6H**: Captura ciclos operacionales y fluctuaciones de turno\n",
    "- **Ventana 24H**: Identifica patrones diarios y efectos tÃ©rmicos acumulativos  \n",
    "- **Ventana 72H**: Detecta tendencias de degradaciÃ³n de mediano plazo\n",
    "\n",
    "Para cada ventana, calcularemos estadÃ­sticas robustas que han demostrado alta correlaciÃ³n con el estado de salud del equipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplementaciÃ³n de caracterÃ­sticas de ventanas mÃ³viles (Rolling Features)\n",
    "print(\"âš™ï¸ Generando caracterÃ­sticas de ventanas mÃ³viles...\")\n",
    "print(\"\\nğŸ“Š ConfiguraciÃ³n de ventanas temporales optimizadas para moto-compresores:\")\n",
    "\n",
    "# ConfiguraciÃ³n de ventanas especÃ­ficas para moto-compresores\n",
    "ventanas_config = {\n",
    "    '6H': {'horas': 6, 'descripcion': 'Ciclos operacionales y fluctuaciones de turno'},\n",
    "    '24H': {'horas': 24, 'descripcion': 'Patrones diarios y efectos tÃ©rmicos acumulativos'},\n",
    "    '72H': {'horas': 72, 'descripcion': 'Tendencias de degradaciÃ³n de mediano plazo'}\n",
    "}\n",
    "\n",
    "for ventana, config in ventanas_config.items():\n",
    "    print(f\"   {ventana}: {config['descripcion']}\")\n",
    "\n",
    "# DataFrame para almacenar todas las caracterÃ­sticas generadas\n",
    "df_features = df[variables_criticas].copy()\n",
    "contador_features = len(variables_criticas)  # Features originales\n",
    "\n",
    "print(f\"\\nğŸ”„ Procesando {len(variables_criticas)} variables crÃ­ticas...\")\n",
    "\n",
    "# Generar features de ventanas mÃ³viles para cada variable crÃ­tica\n",
    "for variable in variables_criticas:\n",
    "    print(f\"\\nğŸ“ˆ Procesando variable: {variable}\")\n",
    "    \n",
    "    # Verificar que la variable tiene datos suficientes\n",
    "    datos_validos = df[variable].notna().sum()\n",
    "    if datos_validos < 100:  # MÃ­nimo 100 puntos vÃ¡lidos\n",
    "        print(f\"   âš ï¸  Saltando {variable}: datos insuficientes ({datos_validos} puntos)\")\n",
    "        continue\n",
    "    \n",
    "    for ventana_nombre, ventana_config in ventanas_config.items():\n",
    "        window_size = f\"{ventana_config['horas']}H\"\n",
    "        \n",
    "        try:\n",
    "            # Rolling Window Statistics - optimizadas para detecciÃ³n de degradaciÃ³n\n",
    "            \n",
    "            # 1. Media mÃ³vil - tendencia central suavizada\n",
    "            col_mean = f\"{variable}_mean_{ventana_nombre.lower()}\"\n",
    "            df_features[col_mean] = df[variable].rolling(window=window_size, min_periods=1).mean()\n",
    "            \n",
    "            # 2. DesviaciÃ³n estÃ¡ndar mÃ³vil - indicador de estabilidad operacional\n",
    "            col_std = f\"{variable}_std_{ventana_nombre.lower()}\"\n",
    "            df_features[col_std] = df[variable].rolling(window=window_size, min_periods=1).std()\n",
    "            \n",
    "            # 3. Rango mÃ³vil (max-min) - detector de picos anÃ³malos\n",
    "            col_range = f\"{variable}_range_{ventana_nombre.lower()}\"\n",
    "            rolling_max = df[variable].rolling(window=window_size, min_periods=1).max()\n",
    "            rolling_min = df[variable].rolling(window=window_size, min_periods=1).min()\n",
    "            df_features[col_range] = rolling_max - rolling_min\n",
    "            \n",
    "            # 4. Percentiles mÃ³viles - robustos ante outliers\n",
    "            col_q25 = f\"{variable}_q25_{ventana_nombre.lower()}\"\n",
    "            col_q75 = f\"{variable}_q75_{ventana_nombre.lower()}\"\n",
    "            df_features[col_q25] = df[variable].rolling(window=window_size, min_periods=1).quantile(0.25)\n",
    "            df_features[col_q75] = df[variable].rolling(window=window_size, min_periods=1).quantile(0.75)\n",
    "            \n",
    "            # 5. Coeficiente de variaciÃ³n mÃ³vil - estabilidad relativa\n",
    "            col_cv = f\"{variable}_cv_{ventana_nombre.lower()}\"\n",
    "            rolling_mean = df_features[col_mean]\n",
    "            rolling_std = df_features[col_std]\n",
    "            df_features[col_cv] = rolling_std / rolling_mean.abs()\n",
    "            df_features[col_cv] = df_features[col_cv].replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            contador_features += 5  # 5 nuevas features por ventana\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error procesando {variable} con ventana {ventana_nombre}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ… Completado: 15 features generadas (5 estadÃ­sticas Ã— 3 ventanas)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Resumen de Rolling Features:\")\n",
    "print(f\"   ğŸ”¢ Features originales: {len(variables_criticas)}\")\n",
    "print(f\"   âš™ï¸ Features rolling generadas: {contador_features - len(variables_criticas)}\")\n",
    "print(f\"   ğŸ“ˆ Total features actuales: {contador_features}\")\n",
    "print(f\"   ğŸ’¾ Dimensiones del dataset: {df_features.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… Rolling Features completadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ• CaracterÃ­sticas de Retraso (Lag Features)\n",
    "\n",
    "### ğŸ§  Fundamento TeÃ³rico de las Lag Features\n",
    "\n",
    "Las **caracterÃ­sticas de retraso** proporcionan \"memoria histÃ³rica\" al modelo, permitiÃ©ndole comparar el estado actual del equipo con estados anteriores. Esta capacidad es fundamental para detectar **tendencias de degradaciÃ³n** y **cambios progresivos** que son imperceptibles en mediciones instantÃ¡neas.\n",
    "\n",
    "### â° SelecciÃ³n EstratÃ©gica de Intervalos de Retraso\n",
    "\n",
    "La selecciÃ³n de intervalos de retraso debe alinearse con la **fÃ­sica del deterioro** en moto-compresores:\n",
    "\n",
    "- **Lag 2H**: DetecciÃ³n de cambios operacionales inmediatos y transitorios tÃ©rmicos\n",
    "- **Lag 12H**: IdentificaciÃ³n de ciclos de fatiga y efectos de acumulaciÃ³n tÃ©rmica\n",
    "- **Lag 48H**: Captura de tendencias de desgaste progresivo y degradaciÃ³n estructural\n",
    "\n",
    "### ğŸ“Š InterpretaciÃ³n TÃ©cnica\n",
    "\n",
    "Las lag features permiten al modelo evaluar preguntas crÃ­ticas como:\n",
    "- *Â¿La temperatura actual es significativamente diferente a la de hace 48 horas?*\n",
    "- *Â¿Las vibraciones muestran una tendencia ascendente comparando con el estado de ayer?*\n",
    "- *Â¿Los patrones de presiÃ³n han cambiado respecto al comportamiento histÃ³rico reciente?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplementaciÃ³n de caracterÃ­sticas de retraso (Lag Features)\n",
    "print(\"ğŸ• Generando caracterÃ­sticas de retraso (Lag Features)...\")\n",
    "print(\"\\nâ° ConfiguraciÃ³n de intervalos de retraso para moto-compresores:\")\n",
    "\n",
    "# ConfiguraciÃ³n de lags optimizada para dinÃ¡micas de moto-compresores\n",
    "lags_config = {\n",
    "    '2H': {'horas': 2, 'descripcion': 'Cambios operacionales inmediatos y transitorios tÃ©rmicos'},\n",
    "    '12H': {'horas': 12, 'descripcion': 'Ciclos de fatiga y efectos de acumulaciÃ³n tÃ©rmica'},\n",
    "    '48H': {'horas': 48, 'descripcion': 'Tendencias de desgaste progresivo y degradaciÃ³n estructural'}\n",
    "}\n",
    "\n",
    "for lag_nombre, config in lags_config.items():\n",
    "    print(f\"   {lag_nombre}: {config['descripcion']}\")\n",
    "\n",
    "features_lag_generadas = 0\n",
    "print(f\"\\nğŸ”„ Generando lag features para {len(variables_criticas)} variables crÃ­ticas...\")\n",
    "\n",
    "# Generar lag features para cada variable crÃ­tica\n",
    "for variable in variables_criticas:\n",
    "    print(f\"\\nğŸ“Š Procesando lags para: {variable}\")\n",
    "    \n",
    "    # Verificar disponibilidad de datos\n",
    "    datos_validos = df[variable].notna().sum()\n",
    "    if datos_validos < 200:  # MÃ­nimo para lags significativos\n",
    "        print(f\"   âš ï¸  Saltando {variable}: datos insuficientes para lags ({datos_validos} puntos)\")\n",
    "        continue\n",
    "    \n",
    "    for lag_nombre, lag_config in lags_config.items():\n",
    "        lag_periods = lag_config['horas']  # pandas shift espera perÃ­odos\n",
    "        \n",
    "        try:\n",
    "            # Lag feature bÃ¡sico - valor histÃ³rico directo\n",
    "            col_lag = f\"{variable}_lag_{lag_nombre.lower()}\"\n",
    "            df_features[col_lag] = df[variable].shift(periods=lag_periods)\n",
    "            \n",
    "            # Diferencia con lag - cambio absoluto\n",
    "            col_diff = f\"{variable}_diff_{lag_nombre.lower()}\"\n",
    "            df_features[col_diff] = df[variable] - df_features[col_lag]\n",
    "            \n",
    "            # Ratio con lag - cambio relativo (mÃ¡s robusto para diferentes escalas)\n",
    "            col_ratio = f\"{variable}_ratio_{lag_nombre.lower()}\"\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                df_features[col_ratio] = df[variable] / df_features[col_lag]\n",
    "                # Limpiar valores infinitos e invÃ¡lidos\n",
    "                df_features[col_ratio] = df_features[col_ratio].replace([np.inf, -np.inf], np.nan)\n",
    "                # Valores extremos que indican problemas de mediciÃ³n\n",
    "                df_features[col_ratio] = df_features[col_ratio].where(\n",
    "                    (df_features[col_ratio] >= 0.1) & (df_features[col_ratio] <= 10.0), np.nan\n",
    "                )\n",
    "            \n",
    "            features_lag_generadas += 3  # lag, diff, ratio\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error procesando lag {lag_nombre} para {variable}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ… Completado: 9 lag features generadas (3 tipos Ã— 3 intervalos)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Resumen de Lag Features:\")\n",
    "print(f\"   ğŸ• Features lag generadas: {features_lag_generadas}\")\n",
    "print(f\"   ğŸ“ˆ Total features actuales: {df_features.shape[1]}\")\n",
    "print(f\"   ğŸ’¾ Dimensiones del dataset: {df_features.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… Lag Features completadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ”¬ CaracterÃ­sticas Avanzadas de AnÃ¡lisis de SeÃ±ales\n",
    "\n",
    "### ğŸ“ˆ Tasas de Cambio (First Derivatives)\n",
    "\n",
    "### ğŸ§® Fundamento MatemÃ¡tico y FÃ­sico\n",
    "\n",
    "Las **tasas de cambio** o **derivadas temporales** capturan la **velocidad de variaciÃ³n** de los parÃ¡metros del moto-compresor. Desde la perspectiva fÃ­sica, estas caracterÃ­sticas revelan procesos dinÃ¡micos crÃ­ticos:\n",
    "\n",
    "- **Gradientes tÃ©rmicos**: Velocidad de calentamiento que indica fricciÃ³n anÃ³mala\n",
    "- **Cambios de presiÃ³n**: Tasas que revelan fugas o degradaciÃ³n de sellos\n",
    "- **AceleraciÃ³n de componentes**: Variaciones en RPM que indican desbalanceos\n",
    "\n",
    "### âš¡ ImplementaciÃ³n Robusta\n",
    "\n",
    "Utilizaremos diferencias finitas centradas para mayor precisiÃ³n numÃ©rica, aplicando suavizado previo para reducir el impacto del ruido de mediciÃ³n en las derivadas.\n",
    "\n",
    "### ğŸŒŠ AnÃ¡lisis de Frecuencia (FFT Features)\n",
    "\n",
    "### ğŸ”Š Transformada RÃ¡pida de Fourier para DiagnÃ³stico\n",
    "\n",
    "El **anÃ¡lisis espectral** mediante FFT revela patrones ocultos en el dominio de frecuencia que son imperceptibles en el anÃ¡lisis temporal tradicional. En moto-compresores, cada componente genera firmas espectrales caracterÃ­sticas:\n",
    "\n",
    "- **Frecuencias de rotaciÃ³n**: DetecciÃ³n de desbalances y desalineaciones\n",
    "- **ArmÃ³nicos**: IdentificaciÃ³n de problemas en rodamientos y engranajes  \n",
    "- **Modulaciones**: DetecciÃ³n de holguras y desgastes progresivos\n",
    "\n",
    "### ğŸ“Š Features Espectrales CrÃ­ticas\n",
    "\n",
    "Extraeremos caracterÃ­sticas espectrales especÃ­ficamente relevantes para mantenimiento predictivo:\n",
    "- **EnergÃ­a espectral total**: Indicador de actividad vibratoria global\n",
    "- **Frecuencia dominante**: IdentificaciÃ³n de modos de falla predominantes\n",
    "- **DistribuciÃ³n espectral**: ConcentraciÃ³n de energÃ­a en bandas crÃ­ticas\n",
    "\n",
    "### ğŸš¨ DetecciÃ³n de AnomalÃ­as EstadÃ­sticas\n",
    "\n",
    "### ğŸ“ Z-Scores MÃ³viles para DetecciÃ³n de Desviaciones\n",
    "\n",
    "Los **Z-scores mÃ³viles** permiten identificar desviaciones estadÃ­sticamente significativas respecto al comportamiento histÃ³rico reciente del equipo. Esta tÃ©cnica es especialmente efectiva para detectar:\n",
    "\n",
    "- **Cambios abruptos**: Desviaciones sÃºbitas que indican eventos crÃ­ticos\n",
    "- **Drift gradual**: Desviaciones lentas que seÃ±alan deterioro progresivo\n",
    "- **Comportamientos atÃ­picos**: Patrones que se apartan de la normalidad operacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplementaciÃ³n de caracterÃ­sticas avanzadas - Tasas de cambio (First Derivatives)\n",
    "print(\"ğŸ“ˆ Generando caracterÃ­sticas de tasas de cambio (First Derivatives)...\")\n",
    "print(\"\\nğŸ§® Aplicando diferencias finitas centradas con suavizado previo...\")\n",
    "\n",
    "features_derivadas = 0\n",
    "\n",
    "# ConfiguraciÃ³n para cÃ¡lculo de derivadas\n",
    "ventana_suavizado = '2H'  # Suavizado previo para reducir ruido en derivadas\n",
    "\n",
    "for variable in variables_criticas:\n",
    "    print(f\"\\nğŸ“Š Procesando derivadas para: {variable}\")\n",
    "    \n",
    "    try:\n",
    "        # Suavizado previo usando media mÃ³vil para reducir ruido\n",
    "        serie_suavizada = df[variable].rolling(window=ventana_suavizado, center=True).mean()\n",
    "        \n",
    "        # Primera derivada (diferencias finitas)\n",
    "        col_deriv1 = f\"{variable}_deriv_1h\"\n",
    "        df_features[col_deriv1] = serie_suavizada.diff(periods=1)\n",
    "        \n",
    "        # Derivada de mayor orden (cambios en la tasa de cambio)\n",
    "        col_deriv2 = f\"{variable}_deriv_6h\"\n",
    "        df_features[col_deriv2] = serie_suavizada.diff(periods=6)\n",
    "        \n",
    "        # Derivada absoluta (magnitud de cambio sin direcciÃ³n)\n",
    "        col_deriv_abs = f\"{variable}_deriv_abs_1h\"\n",
    "        df_features[col_deriv_abs] = df_features[col_deriv1].abs()\n",
    "        \n",
    "        # AceleraciÃ³n (segunda derivada) - detecta cambios en las tendencias\n",
    "        col_accel = f\"{variable}_accel_1h\"\n",
    "        df_features[col_accel] = df_features[col_deriv1].diff(periods=1)\n",
    "        \n",
    "        features_derivadas += 4\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error calculando derivadas para {variable}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   âœ… 4 caracterÃ­sticas de derivadas generadas\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Resumen de caracterÃ­sticas de derivadas:\")\n",
    "print(f\"   ğŸ“ˆ Features de derivadas generadas: {features_derivadas}\")\n",
    "print(f\"   ğŸ’¾ Dimensiones actuales: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplementaciÃ³n de caracterÃ­sticas espectrales (FFT Features)\n",
    "print(\"\\nğŸŒŠ Generando caracterÃ­sticas espectrales (FFT Features)...\")\n",
    "print(\"\\nğŸ”Š Aplicando anÃ¡lisis de Fourier para detecciÃ³n de patrones espectrales...\")\n",
    "\n",
    "features_fft = 0\n",
    "ventana_fft = 72  # Ventana de 72 horas para anÃ¡lisis espectral robusto\n",
    "\n",
    "# FunciÃ³n para extraer caracterÃ­sticas espectrales\n",
    "def extraer_features_fft(serie, ventana_size=ventana_fft):\n",
    "    \"\"\"Extrae caracterÃ­sticas espectrales usando FFT en ventana deslizante\"\"\"\n",
    "    n = len(serie)\n",
    "    \n",
    "    # Inicializar arrays para caracterÃ­sticas espectrales\n",
    "    energia_espectral = np.full(n, np.nan)\n",
    "    freq_dominante = np.full(n, np.nan)\n",
    "    concentracion_espectral = np.full(n, np.nan)\n",
    "    \n",
    "    # Procesamiento con ventana deslizante\n",
    "    for i in range(ventana_size, n):\n",
    "        ventana_datos = serie.iloc[i-ventana_size:i].dropna()\n",
    "        \n",
    "        if len(ventana_datos) < ventana_size * 0.8:  # Al menos 80% de datos vÃ¡lidos\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Remover tendencia linear para mejorar anÃ¡lisis espectral\n",
    "            from scipy import signal as scipy_signal\n",
    "            ventana_detrend = scipy_signal.detrend(ventana_datos.values)\n",
    "            \n",
    "            # Calcular FFT\n",
    "            fft_valores = fft(ventana_detrend)\n",
    "            fft_magnitudes = np.abs(fft_valores)\n",
    "            freqs = fftfreq(len(ventana_detrend), d=1.0)  # Frecuencia normalizada\n",
    "            \n",
    "            # Solo frecuencias positivas\n",
    "            idx_pos = freqs > 0\n",
    "            fft_magnitudes_pos = fft_magnitudes[idx_pos]\n",
    "            freqs_pos = freqs[idx_pos]\n",
    "            \n",
    "            if len(fft_magnitudes_pos) > 0:\n",
    "                # 1. EnergÃ­a espectral total\n",
    "                energia_espectral[i] = np.sum(fft_magnitudes_pos**2)\n",
    "                \n",
    "                # 2. Frecuencia dominante\n",
    "                idx_max = np.argmax(fft_magnitudes_pos)\n",
    "                freq_dominante[i] = freqs_pos[idx_max]\n",
    "                \n",
    "                # 3. ConcentraciÃ³n espectral (entropÃ­a espectral)\n",
    "                potencia_norm = fft_magnitudes_pos**2 / np.sum(fft_magnitudes_pos**2)\n",
    "                # Evitar log(0)\n",
    "                potencia_norm = potencia_norm[potencia_norm > 1e-10]\n",
    "                if len(potencia_norm) > 1:\n",
    "                    entropia = -np.sum(potencia_norm * np.log(potencia_norm))\n",
    "                    concentracion_espectral[i] = entropia\n",
    "        \n",
    "        except Exception:\n",
    "            continue  # Saltar ventanas problemÃ¡ticas\n",
    "    \n",
    "    return energia_espectral, freq_dominante, concentracion_espectral\n",
    "\n",
    "# Aplicar anÃ¡lisis espectral a variables con mayor contenido de informaciÃ³n\n",
    "# Seleccionar variables que tÃ­picamente muestran patrones espectrales importantes\n",
    "variables_espectrales = [var for var in variables_criticas \n",
    "                        if any(keyword in var.lower() for keyword in ['temp', 'pres', 'rpm', 'vib'])]\n",
    "\n",
    "print(f\"ğŸ¯ Variables seleccionadas para anÃ¡lisis espectral: {len(variables_espectrales)}\")\n",
    "for var in variables_espectrales:\n",
    "    print(f\"   - {var}\")\n",
    "\n",
    "for variable in variables_espectrales[:6]:  # Limitar a 6 variables por eficiencia computacional\n",
    "    print(f\"\\nğŸ”„ Procesando FFT para: {variable}\")\n",
    "    \n",
    "    try:\n",
    "        # Extraer caracterÃ­sticas espectrales\n",
    "        energia, freq_dom, concentracion = extraer_features_fft(df[variable])\n",
    "        \n",
    "        # Agregar caracterÃ­sticas al dataset\n",
    "        df_features[f\"{variable}_fft_energia\"] = energia\n",
    "        df_features[f\"{variable}_fft_freq_dom\"] = freq_dom\n",
    "        df_features[f\"{variable}_fft_concentracion\"] = concentracion\n",
    "        \n",
    "        features_fft += 3\n",
    "        \n",
    "        # EstadÃ­sticas de validez\n",
    "        valores_validos = pd.Series(energia).notna().sum()\n",
    "        total_puntos = len(energia)\n",
    "        print(f\"   âœ… FFT completado: {valores_validos}/{total_puntos} puntos vÃ¡lidos ({valores_validos/total_puntos*100:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error en FFT para {variable}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nğŸ“Š Resumen de caracterÃ­sticas espectrales:\")\n",
    "print(f\"   ğŸŒŠ Features FFT generadas: {features_fft}\")\n",
    "print(f\"   ğŸ’¾ Dimensiones actuales: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplementaciÃ³n de detecciÃ³n de anomalÃ­as estadÃ­sticas (Z-Scores mÃ³viles)\n",
    "print(\"\\nğŸš¨ Generando caracterÃ­sticas de detecciÃ³n de anomalÃ­as...\")\n",
    "print(\"\\nğŸ“ Aplicando Z-scores mÃ³viles para identificaciÃ³n de desviaciones...\")\n",
    "\n",
    "features_anomalias = 0\n",
    "\n",
    "# ConfiguraciÃ³n de ventanas para Z-scores mÃ³viles\n",
    "ventanas_zscore = {'24H': 24, '72H': 72, '168H': 168}  # 1 dÃ­a, 3 dÃ­as, 1 semana\n",
    "\n",
    "print(f\"ğŸ¯ Ventanas para anÃ¡lisis de anomalÃ­as:\")\n",
    "for ventana, horas in ventanas_zscore.items():\n",
    "    print(f\"   {ventana}: {horas} horas\")\n",
    "\n",
    "# FunciÃ³n para calcular Z-score mÃ³vil robusto\n",
    "def calcular_zscore_robusto(serie, ventana):\n",
    "    \"\"\"Calcula Z-score mÃ³vil usando estadÃ­sticas robustas\"\"\"\n",
    "    # Usar mediana y MAD (Median Absolute Deviation) para robustez ante outliers\n",
    "    rolling_median = serie.rolling(window=ventana, min_periods=int(ventana*0.5)).median()\n",
    "    rolling_mad = serie.rolling(window=ventana, min_periods=int(ventana*0.5)).apply(\n",
    "        lambda x: np.median(np.abs(x - np.median(x))), raw=True\n",
    "    )\n",
    "    \n",
    "    # Z-score robusto\n",
    "    zscore = (serie - rolling_median) / (rolling_mad * 1.4826)  # Factor para normalizar MAD\n",
    "    \n",
    "    # Manejar divisiones por cero\n",
    "    zscore = zscore.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return zscore\n",
    "\n",
    "# Aplicar detecciÃ³n de anomalÃ­as a variables crÃ­ticas\n",
    "variables_anomalias = variables_criticas[:8]  # Limitar a 8 variables principales\n",
    "\n",
    "for variable in variables_anomalias:\n",
    "    print(f\"\\nğŸ“Š Procesando anomalÃ­as para: {variable}\")\n",
    "    \n",
    "    for ventana_nombre, ventana_horas in ventanas_zscore.items():\n",
    "        try:\n",
    "            # Z-score mÃ³vil robusto\n",
    "            col_zscore = f\"{variable}_zscore_{ventana_nombre.lower()}\"\n",
    "            df_features[col_zscore] = calcular_zscore_robusto(df[variable], f\"{ventana_horas}H\")\n",
    "            \n",
    "            # Magnitud de anomalÃ­a (valor absoluto del Z-score)\n",
    "            col_anomalia_mag = f\"{variable}_anomalia_mag_{ventana_nombre.lower()}\"\n",
    "            df_features[col_anomalia_mag] = df_features[col_zscore].abs()\n",
    "            \n",
    "            # Indicador binario de anomalÃ­a (|Z-score| > 2.5)\n",
    "            col_anomalia_bin = f\"{variable}_anomalia_bin_{ventana_nombre.lower()}\"\n",
    "            df_features[col_anomalia_bin] = (df_features[col_zscore].abs() > 2.5).astype(int)\n",
    "            \n",
    "            features_anomalias += 3\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error calculando anomalÃ­as para {variable}, ventana {ventana_nombre}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ… 9 caracterÃ­sticas de anomalÃ­as generadas (3 tipos Ã— 3 ventanas)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Resumen de caracterÃ­sticas de anomalÃ­as:\")\n",
    "print(f\"   ğŸš¨ Features de anomalÃ­as generadas: {features_anomalias}\")\n",
    "print(f\"   ğŸ’¾ Dimensiones actuales: {df_features.shape}\")\n",
    "\n",
    "# Resumen final de todas las caracterÃ­sticas avanzadas\n",
    "total_features_avanzadas = features_derivadas + features_fft + features_anomalias\n",
    "print(f\"\\nâœ… RESUMEN DE CARACTERÃSTICAS AVANZADAS:\")\n",
    "print(f\"   ğŸ“ˆ Derivadas: {features_derivadas} features\")\n",
    "print(f\"   ğŸŒŠ FFT/Espectrales: {features_fft} features\")\n",
    "print(f\"   ğŸš¨ AnomalÃ­as: {features_anomalias} features\")\n",
    "print(f\"   ğŸ¯ Total avanzadas: {total_features_avanzadas} features\")\n",
    "print(f\"   ğŸ’¾ Dataset final: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” AnÃ¡lisis y Recomendaciones de CaracterÃ­sticas Avanzadas\n",
    "\n",
    "### ğŸ“ˆ Impacto de las Tasas de Cambio (Derivadas)\n",
    "\n",
    "Las **caracterÃ­sticas de derivadas** proporcionan insights fundamentales sobre la **velocidad de deterioro** del moto-compresor. Estas features son especialmente valiosas porque:\n",
    "\n",
    "**Ventajas TÃ©cnicas:**\n",
    "- **DetecciÃ³n temprana**: Identifican cambios sutiles antes de que se manifiesten como fallas visibles\n",
    "- **Robustez ante offset**: Insensibles a sesgos constantes en los sensores\n",
    "- **Indicadores de transiciÃ³n**: Capturan momentos de cambio de estado operacional\n",
    "\n",
    "**Recomendaciones para Modelado:**\n",
    "- Aplicar **normalizaciÃ³n robusta** (StandardScaler) debido a la sensibilidad al ruido\n",
    "- Considerar **filtros de suavizado** adicionales si el ruido de mediciÃ³n es significativo\n",
    "- Evaluar **ventanas adaptativas** basadas en condiciones operacionales\n",
    "\n",
    "### ğŸŒŠ CaracterÃ­sticas Espectrales (FFT)\n",
    "\n",
    "El **anÃ¡lisis espectral** revela patrones de falla ocultos en el dominio temporal, proporcionando una dimensiÃ³n adicional crÃ­tica para el diagnÃ³stico:\n",
    "\n",
    "**Insights FÃ­sicos:**\n",
    "- **EnergÃ­a espectral**: Correlaciona con nivel de vibraciÃ³n/ruido global del equipo\n",
    "- **Frecuencia dominante**: Identifica modos de falla especÃ­ficos (desbalances, rodamientos)\n",
    "- **ConcentraciÃ³n espectral**: Detecta cambios en la distribuciÃ³n de energÃ­a\n",
    "\n",
    "**Consideraciones de ImplementaciÃ³n:**\n",
    "- **Ventanas de 72 horas** proporcionan resoluciÃ³n espectral adecuada para patrones de deterioro\n",
    "- **Detrending**: Esencial para remover componentes de baja frecuencia no relacionados con fallas\n",
    "- **LimitaciÃ³n computacional**: Restringir a variables con mayor contenido informativo\n",
    "\n",
    "### ğŸš¨ DetecciÃ³n de AnomalÃ­as EstadÃ­sticas\n",
    "\n",
    "Los **Z-scores mÃ³viles robustos** proporcionan un mecanismo de detecciÃ³n de desviaciones que es:\n",
    "\n",
    "**Fortalezas del MÃ©todo:**\n",
    "- **Robusto ante outliers**: Uso de mediana y MAD en lugar de media y desviaciÃ³n estÃ¡ndar\n",
    "- **Adaptativo**: Se ajusta a cambios graduales en condiciones operacionales\n",
    "- **Interpretable**: Z-scores > 2.5 indican desviaciones estadÃ­sticamente significativas\n",
    "\n",
    "**Recomendaciones EstratÃ©gicas:**\n",
    "- **CombinaciÃ³n de ventanas**: Las 3 ventanas (24H, 72H, 168H) capturan anomalÃ­as en diferentes escalas temporales\n",
    "- **Threshold adaptativo**: Considerar umbrales variables segÃºn la criticidad de cada variable\n",
    "- **IntegraciÃ³n con rolling features**: Las anomalÃ­as complementan las caracterÃ­sticas de ventanas mÃ³viles\n",
    "\n",
    "### ğŸ¯ Recomendaciones para Fases Posteriores\n",
    "\n",
    "**Para SelecciÃ³n de Features (Feature Selection):**\n",
    "1. **Priorizar derivadas** de variables tÃ©rmicas y de presiÃ³n - histÃ³ricamente mÃ¡s predictivas\n",
    "2. **Evaluar features espectrales** mediante anÃ¡lisis de correlaciÃ³n con eventos de falla\n",
    "3. **Utilizar anomalÃ­as** como features de activaciÃ³n/trigger para otros predictores\n",
    "\n",
    "**Para Entrenamiento del Modelo:**\n",
    "1. **RegularizaciÃ³n L1/L2**: Esencial dada la alta dimensionalidad generada\n",
    "2. **ValidaciÃ³n temporal**: Usar split temporal para evitar data leakage\n",
    "3. **Interpretabilidad**: Mantener trazabilidad hacia variables fÃ­sicas originales\n",
    "\n",
    "**Para Despliegue Productivo:**\n",
    "1. **Monitoreo de drift**: Las caracterÃ­sticas espectrales son sensibles a cambios operacionales\n",
    "2. **Computational efficiency**: Evaluar trade-off entre precisiÃ³n y costo computacional\n",
    "3. **Robustez**: Implementar validaciones de calidad de datos en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo inteligente de valores faltantes generados por feature engineering\n",
    "print(\"ğŸ”§ Manejo de valores faltantes generados por feature engineering...\")\n",
    "\n",
    "# AnÃ¡lisis de valores faltantes antes del tratamiento\n",
    "print(f\"\\nğŸ“Š AnÃ¡lisis de completitud del dataset:\")\n",
    "valores_totales = df_features.size\n",
    "valores_faltantes = df_features.isnull().sum().sum()\n",
    "porcentaje_faltantes = (valores_faltantes / valores_totales) * 100\n",
    "\n",
    "print(f\"   ğŸ’¾ Dimensiones: {df_features.shape}\")\n",
    "print(f\"   ğŸ“‰ Valores faltantes: {valores_faltantes:,} ({porcentaje_faltantes:.2f}%)\")\n",
    "\n",
    "# Identificar columnas con mayor porcentaje de valores faltantes\n",
    "missing_por_columna = df_features.isnull().sum()\n",
    "missing_porcentaje = (missing_por_columna / len(df_features)) * 100\n",
    "columnas_problematicas = missing_porcentaje[missing_porcentaje > 50].sort_values(ascending=False)\n",
    "\n",
    "if not columnas_problematicas.empty:\n",
    "    print(f\"\\nâš ï¸  Columnas con >50% valores faltantes ({len(columnas_problematicas)} columnas):\")\n",
    "    for col, pct in columnas_problematicas.head(10).items():\n",
    "        print(f\"   {col}: {pct:.1f}% faltantes\")\n",
    "    \n",
    "    # Eliminar columnas con exceso de valores faltantes\n",
    "    print(f\"\\nğŸ—‘ï¸  Eliminando columnas con >80% valores faltantes...\")\n",
    "    columnas_eliminar = missing_porcentaje[missing_porcentaje > 80].index\n",
    "    if len(columnas_eliminar) > 0:\n",
    "        df_features = df_features.drop(columns=columnas_eliminar)\n",
    "        print(f\"   âŒ {len(columnas_eliminar)} columnas eliminadas\")\n",
    "        print(f\"   ğŸ“Š Dimensiones actualizadas: {df_features.shape}\")\n",
    "    else:\n",
    "        print(f\"   âœ… No hay columnas que requieran eliminaciÃ³n\")\n",
    "\n",
    "# Estrategia de relleno especÃ­fica por tipo de caracterÃ­stica\n",
    "print(f\"\\nğŸ”„ Aplicando estrategia de relleno especÃ­fica por tipo de feature...\")\n",
    "\n",
    "# 1. Rolling features: usar backward fill (valores hacia atrÃ¡s)\n",
    "rolling_cols = [col for col in df_features.columns \n",
    "                if any(pattern in col for pattern in ['_mean_', '_std_', '_range_', '_q25_', '_q75_', '_cv_'])]\n",
    "if rolling_cols:\n",
    "    print(f\"   ğŸ“Š Rolling features ({len(rolling_cols)} columnas): backward fill\")\n",
    "    df_features[rolling_cols] = df_features[rolling_cols].bfill()\n",
    "\n",
    "# 2. Lag features: eliminar filas iniciales afectadas\n",
    "lag_cols = [col for col in df_features.columns if '_lag_' in col]\n",
    "if lag_cols:\n",
    "    print(f\"   ğŸ• Lag features ({len(lag_cols)} columnas): mantener NaN inicial (natural)\")\n",
    "    # Los NaN iniciales en lag features son naturales y se manejarÃ¡n en el modelado\n",
    "\n",
    "# 3. Derivative features: interpolaciÃ³n lineal\n",
    "deriv_cols = [col for col in df_features.columns if '_deriv_' in col or '_accel_' in col]\n",
    "if deriv_cols:\n",
    "    print(f\"   ğŸ“ˆ Derivative features ({len(deriv_cols)} columnas): interpolaciÃ³n lineal\")\n",
    "    df_features[deriv_cols] = df_features[deriv_cols].interpolate(method='linear')\n",
    "\n",
    "# 4. FFT features: forward fill para preservar Ãºltimo estado conocido\n",
    "fft_cols = [col for col in df_features.columns if '_fft_' in col]\n",
    "if fft_cols:\n",
    "    print(f\"   ğŸŒŠ FFT features ({len(fft_cols)} columnas): forward fill\")\n",
    "    df_features[fft_cols] = df_features[fft_cols].ffill()\n",
    "\n",
    "# 5. Anomaly features: rellenar con valor neutral (0 para binarios, 0 para z-scores)\n",
    "anomaly_cols = [col for col in df_features.columns if '_anomalia_' in col or '_zscore_' in col]\n",
    "if anomaly_cols:\n",
    "    print(f\"   ğŸš¨ Anomaly features ({len(anomaly_cols)} columnas): valor neutral (0)\")\n",
    "    for col in anomaly_cols:\n",
    "        if '_bin_' in col:  # Binarias\n",
    "            df_features[col] = df_features[col].fillna(0)\n",
    "        else:  # Z-scores y magnitudes\n",
    "            df_features[col] = df_features[col].fillna(0)\n",
    "\n",
    "# Relleno final con forward fill para cualquier NaN restante\n",
    "remaining_nan = df_features.isnull().sum().sum()\n",
    "if remaining_nan > 0:\n",
    "    print(f\"\\nğŸ”„ Relleno final: {remaining_nan} valores faltantes restantes\")\n",
    "    df_features = df_features.ffill().bfill()  # Forward fill + backward fill como respaldo\n",
    "\n",
    "# ValidaciÃ³n final\n",
    "final_nan = df_features.isnull().sum().sum()\n",
    "print(f\"\\nâœ… Tratamiento completado:\")\n",
    "print(f\"   ğŸ“Š Valores faltantes finales: {final_nan}\")\n",
    "print(f\"   ğŸ’¾ Dataset final: {df_features.shape}\")\n",
    "\n",
    "if final_nan == 0:\n",
    "    print(f\"   ğŸ‰ Â¡Dataset completamente limpio!\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Quedan {final_nan} valores faltantes para revisiÃ³n manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ğŸ·ï¸ Etiquetado de Datos - CreaciÃ³n de la Variable Objetivo\n",
    "\n",
    "### ğŸ¯ Concepto de Ventana de Pre-Falla\n",
    "\n",
    "El **etiquetado de fallas** es el proceso mÃ¡s crÃ­tico en el desarrollo de un sistema de mantenimiento predictivo exitoso. Transformamos las fechas histÃ³ricas de eventos de mantenimiento en **etiquetas de entrenamiento** que permiten al modelo aprender a reconocer patrones que preceden a las fallas.\n",
    "\n",
    "### â° JustificaciÃ³n del Horizonte de 7 DÃ­as\n",
    "\n",
    "La selecciÃ³n de una **ventana de predicciÃ³n de 7 dÃ­as** se basa en mÃºltiples consideraciones tÃ©cnicas y operacionales:\n",
    "\n",
    "**Consideraciones TÃ©cnicas:**\n",
    "- **Tiempo de reacciÃ³n operacional**: 7 dÃ­as proporcionan tiempo suficiente para planificar intervenciones\n",
    "- **Balance seÃ±al-ruido**: Ventana suficientemente amplia para capturar deterioro progresivo\n",
    "- **Estabilidad del patrÃ³n**: Los moto-compresores exhiben patrones de pre-falla consistentes en esta escala temporal\n",
    "\n",
    "**Consideraciones Operacionales:**\n",
    "- **PlanificaciÃ³n de mantenimiento**: Tiempo adecuado para adquisiciÃ³n de repuestos y programaciÃ³n\n",
    "- **CoordinaciÃ³n operacional**: Permite ajustes en cronogramas de producciÃ³n\n",
    "- **GestiÃ³n de riesgos**: Balance entre alertas tempranas y falsos positivos\n",
    "\n",
    "### ğŸ“Š MetodologÃ­a de Etiquetado\n",
    "\n",
    "Implementaremos un sistema de etiquetado binario donde:\n",
    "- **Etiqueta 1 (Pre-falla)**: Mediciones dentro de los 7 dÃ­as anteriores a un evento de falla\n",
    "- **Etiqueta 0 (Normal)**: Todas las demÃ¡s mediciones del dataset\n",
    "\n",
    "### âš–ï¸ Consideraciones de Desbalance de Clases\n",
    "\n",
    "Es fundamental reconocer que el **desbalance de clases** es inherente en problemas de mantenimiento predictivo:\n",
    "- Los perÃ­odos normales son naturalmente mucho mÃ¡s frecuentes que los pre-falla\n",
    "- Este desbalance refleja la realidad operacional y debe manejarse apropiadamente\n",
    "- Las tÃ©cnicas de sampling y weighting serÃ¡n crÃ­ticas en la fase de modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImplementaciÃ³n del etiquetado de fallas con ventana de 7 dÃ­as\n",
    "print(\"ğŸ·ï¸ Iniciando proceso de etiquetado de fallas...\")\n",
    "print(f\"\\nğŸ“… ConfiguraciÃ³n de etiquetado:\")\n",
    "print(f\"   â° Ventana de predicciÃ³n: 7 dÃ­as\")\n",
    "print(f\"   ğŸ¯ Objetivo: Detectar patrones pre-falla 168 horas antes del evento\")\n",
    "\n",
    "# Definir ventana de pre-falla\n",
    "ventana_prefalla = pd.Timedelta(days=7)\n",
    "print(f\"   ğŸ“Š Ventana tÃ©cnica: {ventana_prefalla} ({ventana_prefalla.total_seconds()/3600:.0f} horas)\")\n",
    "\n",
    "# Inicializar columna de etiquetas\n",
    "df_features['falla'] = 0  # Estado normal por defecto\n",
    "print(f\"\\nğŸ“‹ Columna 'falla' inicializada: {len(df_features)} registros en estado normal\")\n",
    "\n",
    "# Validar disponibilidad de fechas de falla\n",
    "if 'fechas_falla' not in locals() or len(fechas_falla) == 0:\n",
    "    print(f\"âš ï¸  ADVERTENCIA: No se encontraron fechas de falla vÃ¡lidas\")\n",
    "    print(f\"   Verificando datos de eventos...\")\n",
    "    \n",
    "    # Intentar recuperar fechas del DataFrame de eventos\n",
    "    if 'df_eventos' in locals() and 'fecha_evento' in df_eventos.columns:\n",
    "        fechas_validas = df_eventos['fecha_evento'].dropna()\n",
    "        if not fechas_validas.empty:\n",
    "            # Filtrar fechas dentro del rango de datos de sensores\n",
    "            fechas_falla = fechas_validas[\n",
    "                (fechas_validas >= df_features.index.min()) & \n",
    "                (fechas_validas <= df_features.index.max())\n",
    "            ].sort_values().tolist()\n",
    "            print(f\"   âœ… Recuperadas {len(fechas_falla)} fechas vÃ¡lidas\")\n",
    "        else:\n",
    "            print(f\"   âŒ No hay fechas vÃ¡lidas en el historial\")\n",
    "            fechas_falla = []\n",
    "\n",
    "if len(fechas_falla) == 0:\n",
    "    print(f\"\\nâŒ CRÃTICO: Sin fechas de falla, no se puede realizar etiquetado\")\n",
    "    print(f\"   El dataset mantendrÃ¡ todas las etiquetas como 0 (normal)\")\n",
    "    eventos_procesados = 0\n",
    "    registros_etiquetados = 0\n",
    "else:\n",
    "    print(f\"\\nğŸ”„ Procesando {len(fechas_falla)} eventos de falla para etiquetado...\")\n",
    "    \n",
    "    eventos_procesados = 0\n",
    "    registros_etiquetados = 0\n",
    "    \n",
    "    # InformaciÃ³n del rango temporal del dataset\n",
    "    dataset_start = df_features.index.min()\n",
    "    dataset_end = df_features.index.max()\n",
    "    print(f\"   ğŸ“Š Rango del dataset: {dataset_start} a {dataset_end}\")\n",
    "    \n",
    "    for i, fecha_falla in enumerate(fechas_falla, 1):\n",
    "        print(f\"\\n   ğŸ“… Procesando evento {i}/{len(fechas_falla)}: {fecha_falla}\")\n",
    "        \n",
    "        # Calcular ventana de pre-falla\n",
    "        inicio_ventana = fecha_falla - ventana_prefalla\n",
    "        fin_ventana = fecha_falla\n",
    "        \n",
    "        print(f\"      ğŸ• Ventana pre-falla: {inicio_ventana} a {fin_ventana}\")\n",
    "        \n",
    "        # Verificar si la ventana intersecta con nuestros datos\n",
    "        if fin_ventana < dataset_start:\n",
    "            print(f\"      âš ï¸  Evento anterior al dataset - saltando\")\n",
    "            continue\n",
    "        \n",
    "        if inicio_ventana > dataset_end:\n",
    "            print(f\"      âš ï¸  Evento posterior al dataset - saltando\")\n",
    "            continue\n",
    "        \n",
    "        # Ajustar ventana a los lÃ­mites del dataset si es necesario\n",
    "        inicio_efectivo = max(inicio_ventana, dataset_start)\n",
    "        fin_efectivo = min(fin_ventana, dataset_end)\n",
    "        \n",
    "        # Crear mÃ¡scara para seleccionar registros en la ventana\n",
    "        mask_ventana = (\n",
    "            (df_features.index >= inicio_efectivo) & \n",
    "            (df_features.index < fin_efectivo)\n",
    "        )\n",
    "        \n",
    "        # Contar registros en la ventana\n",
    "        registros_ventana = mask_ventana.sum()\n",
    "        \n",
    "        if registros_ventana > 0:\n",
    "            # Etiquetar registros como pre-falla\n",
    "            df_features.loc[mask_ventana, 'falla'] = 1\n",
    "            \n",
    "            registros_etiquetados += registros_ventana\n",
    "            eventos_procesados += 1\n",
    "            \n",
    "            print(f\"      âœ… {registros_ventana} registros etiquetados como pre-falla\")\n",
    "        else:\n",
    "            print(f\"      âš ï¸  No hay registros en esta ventana\")\n",
    "\n",
    "# AnÃ¡lisis final del etiquetado\n",
    "print(f\"\\nğŸ“Š RESUMEN DEL ETIQUETADO:\")\n",
    "conteo_etiquetas = df_features['falla'].value_counts().sort_index()\n",
    "\n",
    "normal_count = conteo_etiquetas.get(0, 0)\n",
    "prefalla_count = conteo_etiquetas.get(1, 0)\n",
    "total_registros = len(df_features)\n",
    "\n",
    "print(f\"   ğŸ“ˆ Total de registros: {total_registros:,}\")\n",
    "print(f\"   âœ… Estado normal (0): {normal_count:,} ({normal_count/total_registros*100:.2f}%)\")\n",
    "print(f\"   ğŸš¨ Pre-falla (1): {prefalla_count:,} ({prefalla_count/total_registros*100:.2f}%)\")\n",
    "print(f\"   ğŸ¯ Eventos procesados: {eventos_procesados}/{len(fechas_falla) if fechas_falla else 0}\")\n",
    "\n",
    "# AnÃ¡lisis de balance de clases\n",
    "if prefalla_count > 0:\n",
    "    ratio_desbalance = normal_count / prefalla_count\n",
    "    print(f\"\\nâš–ï¸  ANÃLISIS DE BALANCE DE CLASES:\")\n",
    "    print(f\"   ğŸ“Š Ratio normal:pre-falla = {ratio_desbalance:.1f}:1\")\n",
    "    \n",
    "    if ratio_desbalance > 10:\n",
    "        print(f\"   âš ï¸  DESBALANCE SIGNIFICATIVO DETECTADO\")\n",
    "        print(f\"   ğŸ’¡ Recomendaciones para modelado:\")\n",
    "        print(f\"      - Usar class_weight='balanced' en algoritmos\")\n",
    "        print(f\"      - Considerar tÃ©cnicas de sampling (SMOTE, undersampling)\")\n",
    "        print(f\"      - Evaluar con mÃ©tricas robustas (F1, AUC-ROC, Precision-Recall)\")\n",
    "    else:\n",
    "        print(f\"   âœ… Balance aceptable para entrenamiento directo\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  ADVERTENCIA: Sin registros pre-falla etiquetados\")\n",
    "    print(f\"   El modelo no podrÃ¡ aprender patrones de falla\")\n",
    "    print(f\"   Verificar datos de eventos y rangos temporales\")\n",
    "\n",
    "print(f\"\\nâœ… Etiquetado de fallas completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Validaciones de Calidad del Etiquetado\n",
    "\n",
    "### ğŸ“Š Importancia de la ValidaciÃ³n Post-Etiquetado\n",
    "\n",
    "Las validaciones de calidad son esenciales para garantizar que nuestro etiquetado refleje fielmente la realidad operacional del moto-compresor y proporcione bases sÃ³lidas para el entrenamiento del modelo.\n",
    "\n",
    "### ğŸ¯ Validaciones CrÃ­ticas a Implementar\n",
    "\n",
    "1. **Solapamiento de Ventanas**: Verificar que las ventanas de pre-falla no se solapen excesivamente\n",
    "2. **DistribuciÃ³n Temporal**: Asegurar cobertura temporal adecuada de eventos\n",
    "3. **Suficiencia de Datos**: Confirmar que hay suficientes datos normales entre eventos\n",
    "4. **Coherencia FÃ­sica**: Validar que los patrones etiquetados son fÃ­sicamente plausibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaciones exhaustivas de calidad del etiquetado\n",
    "print(\"ğŸ” Ejecutando validaciones de calidad del etiquetado...\")\n",
    "\n",
    "# ValidaciÃ³n 1: AnÃ¡lisis de solapamiento de ventanas\n",
    "print(f\"\\n1ï¸âƒ£ VALIDACIÃ“N DE SOLAPAMIENTO DE VENTANAS:\")\n",
    "\n",
    "if len(fechas_falla) > 1:\n",
    "    # Calcular distancias entre eventos consecutivos\n",
    "    fechas_ordenadas = sorted(fechas_falla)\n",
    "    distancias_eventos = []\n",
    "    \n",
    "    for i in range(1, len(fechas_ordenadas)):\n",
    "        distancia = fechas_ordenadas[i] - fechas_ordenadas[i-1]\n",
    "        distancias_eventos.append(distancia)\n",
    "        print(f\"   ğŸ“… Evento {i} â†’ {i+1}: {distancia} ({distancia.total_seconds()/3600:.0f} horas)\")\n",
    "    \n",
    "    # Identificar solapamientos (distancia < 14 dÃ­as = 2 ventanas)\n",
    "    ventana_doble = pd.Timedelta(days=14)\n",
    "    solapamientos = [d for d in distancias_eventos if d < ventana_doble]\n",
    "    \n",
    "    if solapamientos:\n",
    "        print(f\"   âš ï¸  SOLAPAMIENTOS DETECTADOS: {len(solapamientos)} pares de eventos\")\n",
    "        print(f\"   ğŸ’¡ ImplicaciÃ³n: Algunas mediciones pueden tener etiquetado ambiguo\")\n",
    "        print(f\"   ğŸ”§ RecomendaciÃ³n: Considerar ventanas mÃ¡s cortas o consolidar eventos cercanos\")\n",
    "    else:\n",
    "        print(f\"   âœ… No hay solapamientos crÃ­ticos entre ventanas\")\n",
    "else:\n",
    "    print(f\"   â„¹ï¸  Solo hay {len(fechas_falla)} eventos - no aplicable anÃ¡lisis de solapamiento\")\n",
    "\n",
    "# ValidaciÃ³n 2: DistribuciÃ³n temporal de etiquetas\n",
    "print(f\"\\n2ï¸âƒ£ VALIDACIÃ“N DE DISTRIBUCIÃ“N TEMPORAL:\")\n",
    "\n",
    "if prefalla_count > 0:\n",
    "    # Identificar perÃ­odos de pre-falla\n",
    "    registros_prefalla = df_features[df_features['falla'] == 1]\n",
    "    \n",
    "    if not registros_prefalla.empty:\n",
    "        inicio_prefallas = registros_prefalla.index.min()\n",
    "        fin_prefallas = registros_prefalla.index.max()\n",
    "        duracion_prefallas = fin_prefallas - inicio_prefallas\n",
    "        \n",
    "        print(f\"   ğŸ“Š PerÃ­odo de pre-fallas: {inicio_prefallas} a {fin_prefallas}\")\n",
    "        print(f\"   â±ï¸  DuraciÃ³n total con etiquetas pre-falla: {duracion_prefallas}\")\n",
    "        \n",
    "        # Calcular densidad de etiquetas pre-falla\n",
    "        duracion_dataset = df_features.index.max() - df_features.index.min()\n",
    "        cobertura_prefalla = (duracion_prefallas.total_seconds() / duracion_dataset.total_seconds()) * 100\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ Cobertura temporal pre-falla: {cobertura_prefalla:.1f}% del dataset\")\n",
    "        \n",
    "        if cobertura_prefalla < 5:\n",
    "            print(f\"   âš ï¸  Cobertura muy baja - modelo puede tener dificultades de aprendizaje\")\n",
    "        elif cobertura_prefalla > 30:\n",
    "            print(f\"   âš ï¸  Cobertura muy alta - verificar lÃ³gica de etiquetado\")\n",
    "        else:\n",
    "            print(f\"   âœ… Cobertura temporal adecuada para entrenamiento\")\n",
    "\n",
    "# ValidaciÃ³n 3: Suficiencia de datos normales entre eventos\n",
    "print(f\"\\n3ï¸âƒ£ VALIDACIÃ“N DE PERÃODOS NORMALES:\")\n",
    "\n",
    "if len(fechas_falla) > 0 and prefalla_count > 0:\n",
    "    # Identificar gaps entre ventanas de pre-falla\n",
    "    registros_prefalla = df_features[df_features['falla'] == 1]\n",
    "    \n",
    "    # Encontrar perÃ­odos continuos de pre-falla\n",
    "    diff_index = registros_prefalla.index.to_series().diff()\n",
    "    cambios_periodo = diff_index[diff_index > pd.Timedelta(hours=2)]  # Gaps > 2 horas\n",
    "    \n",
    "    print(f\"   ğŸ“Š PerÃ­odos pre-falla identificados: {len(cambios_periodo) + 1 if not registros_prefalla.empty else 0}\")\n",
    "    \n",
    "    # Calcular perÃ­odo normal mÃ¡s largo\n",
    "    registros_normales = df_features[df_features['falla'] == 0]\n",
    "    if not registros_normales.empty:\n",
    "        # Encontrar el gap mÃ¡s largo entre registros normales consecutivos\n",
    "        normal_diffs = registros_normales.index.to_series().diff()\n",
    "        max_gap_normal = normal_diffs.max()\n",
    "        \n",
    "        print(f\"   â±ï¸  PerÃ­odo normal continuo mÃ¡s largo: {max_gap_normal}\")\n",
    "        \n",
    "        if max_gap_normal > pd.Timedelta(days=30):\n",
    "            print(f\"   âœ… Suficientes perÃ­odos normales extensos para entrenamiento\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  PerÃ­odos normales pueden ser insuficientes\")\n",
    "\n",
    "# ValidaciÃ³n 4: Coherencia estadÃ­stica\n",
    "print(f\"\\n4ï¸âƒ£ VALIDACIÃ“N DE COHERENCIA ESTADÃSTICA:\")\n",
    "\n",
    "if prefalla_count > 10:  # MÃ­nimo para anÃ¡lisis estadÃ­stico\n",
    "    # Comparar estadÃ­sticas bÃ¡sicas entre perÃ­odos normales y pre-falla\n",
    "    print(f\"   ğŸ“Š AnÃ¡lisis comparativo de perÃ­odos normales vs pre-falla:\")\n",
    "    \n",
    "    # Seleccionar variables crÃ­ticas para comparaciÃ³n\n",
    "    variables_comparacion = variables_criticas[:3]  # Top 3 variables\n",
    "    \n",
    "    diferencias_significativas = 0\n",
    "    \n",
    "    for variable in variables_comparacion:\n",
    "        if variable in df_features.columns:\n",
    "            normal_stats = df_features[df_features['falla'] == 0][variable].describe()\n",
    "            prefalla_stats = df_features[df_features['falla'] == 1][variable].describe()\n",
    "            \n",
    "            # Comparar medias\n",
    "            diff_media = abs(prefalla_stats['mean'] - normal_stats['mean'])\n",
    "            std_normal = normal_stats['std']\n",
    "            \n",
    "            if std_normal > 0:\n",
    "                z_score_diff = diff_media / std_normal\n",
    "                \n",
    "                if z_score_diff > 1.0:  # Diferencia > 1 desviaciÃ³n estÃ¡ndar\n",
    "                    diferencias_significativas += 1\n",
    "                    print(f\"      ğŸ“ˆ {variable}: diferencia significativa (Z={z_score_diff:.2f})\")\n",
    "                else:\n",
    "                    print(f\"      ğŸ“Š {variable}: diferencia menor (Z={z_score_diff:.2f})\")\n",
    "    \n",
    "    if diferencias_significativas > 0:\n",
    "        print(f\"   âœ… {diferencias_significativas}/{len(variables_comparacion)} variables muestran patrones diferenciados\")\n",
    "        print(f\"   ğŸ’¡ El etiquetado parece capturar cambios reales en el comportamiento\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  No se detectaron diferencias significativas\")\n",
    "        print(f\"   ğŸ’¡ Revisar si las ventanas de 7 dÃ­as son apropiadas para este equipo\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Insuficientes registros pre-falla ({prefalla_count}) para anÃ¡lisis estadÃ­stico\")\n",
    "\n",
    "# Resumen final de validaciones\n",
    "print(f\"\\nâœ… VALIDACIONES DE CALIDAD COMPLETADAS\")\n",
    "print(f\"   ğŸ“Š Dataset listo para entrenamiento: {df_features.shape}\")\n",
    "print(f\"   ğŸ·ï¸  Etiquetas distribuidas: {normal_count:,} normal, {prefalla_count:,} pre-falla\")\n",
    "print(f\"   ğŸ¯ PrÃ³ximo paso: Entrenamiento de modelos de Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ğŸ’¾ PreparaciÃ³n Final y Guardado del Dataset\n",
    "\n",
    "### ğŸ¯ OptimizaciÃ³n Final del Dataset\n",
    "\n",
    "Antes del guardado, realizaremos las optimizaciones finales para garantizar que el dataset estÃ© en condiciones Ã³ptimas para el entrenamiento de modelos de Machine Learning:\n",
    "\n",
    "### ğŸ”§ Tareas de PreparaciÃ³n Final\n",
    "\n",
    "1. **ValidaciÃ³n de integridad**: VerificaciÃ³n final de tipos de datos y valores faltantes\n",
    "2. **OptimizaciÃ³n de memoria**: ConversiÃ³n a tipos de datos eficientes\n",
    "3. **DocumentaciÃ³n de caracterÃ­sticas**: CatalogaciÃ³n de todas las features generadas\n",
    "4. **Guardado en formato optimizado**: Parquet con compresiÃ³n para eficiencia\n",
    "\n",
    "### ğŸ“Š Estructura del Dataset Final\n",
    "\n",
    "El dataset final contendrÃ¡:\n",
    "- **Variables originales**: Mediciones directas de sensores seleccionadas\n",
    "- **Rolling features**: EstadÃ­sticas de ventanas mÃ³viles (6H, 24H, 72H)\n",
    "- **Lag features**: CaracterÃ­sticas de retraso (2H, 12H, 48H)\n",
    "- **CaracterÃ­sticas avanzadas**: Derivadas, FFT, y detecciÃ³n de anomalÃ­as\n",
    "- **Variable objetivo**: Etiqueta binaria de falla para entrenamiento supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreparaciÃ³n final y optimizaciÃ³n del dataset\n",
    "print(\"ğŸ”§ Iniciando preparaciÃ³n final del dataset...\")\n",
    "\n",
    "# 1. ValidaciÃ³n final de integridad\n",
    "print(f\"\\n1ï¸âƒ£ VALIDACIÃ“N FINAL DE INTEGRIDAD:\")\n",
    "\n",
    "# Verificar valores faltantes restantes\n",
    "valores_faltantes_finales = df_features.isnull().sum().sum()\n",
    "print(f\"   ğŸ“Š Valores faltantes finales: {valores_faltantes_finales}\")\n",
    "\n",
    "if valores_faltantes_finales > 0:\n",
    "    print(f\"   âš ï¸  Eliminando filas con valores faltantes restantes...\")\n",
    "    filas_iniciales = len(df_features)\n",
    "    df_features = df_features.dropna()\n",
    "    filas_eliminadas = filas_iniciales - len(df_features)\n",
    "    print(f\"   ğŸ—‘ï¸  {filas_eliminadas} filas eliminadas ({filas_eliminadas/filas_iniciales*100:.2f}%)\")\n",
    "\n",
    "# Verificar tipos de datos\n",
    "tipos_datos = df_features.dtypes.value_counts()\n",
    "print(f\"\\n   ğŸ“‹ DistribuciÃ³n de tipos de datos:\")\n",
    "for tipo, cantidad in tipos_datos.items():\n",
    "    print(f\"      {tipo}: {cantidad} columnas\")\n",
    "\n",
    "# 2. OptimizaciÃ³n de memoria\n",
    "print(f\"\\n2ï¸âƒ£ OPTIMIZACIÃ“N DE MEMORIA:\")\n",
    "\n",
    "memoria_inicial = df_features.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"   ğŸ’¾ Uso de memoria inicial: {memoria_inicial:.1f} MB\")\n",
    "\n",
    "# Optimizar tipos numÃ©ricos\n",
    "print(f\"   ğŸ”„ Optimizando tipos numÃ©ricos...\")\n",
    "\n",
    "for col in df_features.select_dtypes(include=['float64']).columns:\n",
    "    if col != 'falla':  # Preservar la variable objetivo\n",
    "        # Convertir a float32 si el rango lo permite\n",
    "        col_min, col_max = df_features[col].min(), df_features[col].max()\n",
    "        if col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max:\n",
    "            df_features[col] = df_features[col].astype(np.float32)\n",
    "\n",
    "# Optimizar variable objetivo\n",
    "if 'falla' in df_features.columns:\n",
    "    df_features['falla'] = df_features['falla'].astype(np.uint8)\n",
    "\n",
    "memoria_optimizada = df_features.memory_usage(deep=True).sum() / 1024**2\n",
    "reduccion_memoria = ((memoria_inicial - memoria_optimizada) / memoria_inicial) * 100\n",
    "\n",
    "print(f\"   ğŸ’¾ Uso de memoria optimizado: {memoria_optimizada:.1f} MB\")\n",
    "print(f\"   ğŸ“‰ ReducciÃ³n de memoria: {reduccion_memoria:.1f}%\")\n",
    "\n",
    "# 3. CatalogaciÃ³n de caracterÃ­sticas generadas\n",
    "print(f\"\\n3ï¸âƒ£ CATALOGACIÃ“N DE CARACTERÃSTICAS:\")\n",
    "\n",
    "# Clasificar features por tipo\n",
    "feature_categories = {\n",
    "    'originales': variables_criticas,\n",
    "    'rolling': [col for col in df_features.columns if any(pattern in col for pattern in ['_mean_', '_std_', '_range_', '_q25_', '_q75_', '_cv_'])],\n",
    "    'lag': [col for col in df_features.columns if '_lag_' in col or '_diff_' in col or '_ratio_' in col],\n",
    "    'derivadas': [col for col in df_features.columns if '_deriv_' in col or '_accel_' in col],\n",
    "    'fft': [col for col in df_features.columns if '_fft_' in col],\n",
    "    'anomalias': [col for col in df_features.columns if '_anomalia_' in col or '_zscore_' in col],\n",
    "    'objetivo': ['falla']\n",
    "}\n",
    "\n",
    "print(f\"   ğŸ“Š Resumen de caracterÃ­sticas por categorÃ­a:\")\n",
    "total_features = 0\n",
    "for categoria, features in feature_categories.items():\n",
    "    count = len([f for f in features if f in df_features.columns])\n",
    "    total_features += count\n",
    "    print(f\"      {categoria.capitalize()}: {count} features\")\n",
    "\n",
    "print(f\"   ğŸ¯ Total de caracterÃ­sticas: {total_features}\")\n",
    "\n",
    "# 4. Guardado del dataset final\n",
    "print(f\"\\n4ï¸âƒ£ GUARDADO DEL DATASET FINAL:\")\n",
    "\n",
    "# Definir archivo de salida\n",
    "archivo_salida = ruta_processed / 'dataset_etiquetado_para_modelado.parquet'\n",
    "\n",
    "print(f\"   ğŸ“ Archivo de destino: {archivo_salida}\")\n",
    "print(f\"   ğŸ’¾ Dimensiones finales: {df_features.shape}\")\n",
    "\n",
    "try:\n",
    "    # Guardar con configuraciÃ³n optimizada\n",
    "    df_features.to_parquet(\n",
    "        archivo_salida,\n",
    "        engine='pyarrow',\n",
    "        compression='snappy',\n",
    "        index=True  # Preservar Ã­ndice temporal\n",
    "    )\n",
    "    \n",
    "    # Verificar archivo guardado\n",
    "    tamaÃ±o_archivo = archivo_salida.stat().st_size / 1024**2\n",
    "    print(f\"   âœ… Dataset guardado exitosamente\")\n",
    "    print(f\"   ğŸ“Š TamaÃ±o del archivo: {tamaÃ±o_archivo:.1f} MB\")\n",
    "    print(f\"   ğŸ”§ CompresiÃ³n: Snappy\")\n",
    "    print(f\"   ğŸ“… Ãndice temporal: Preservado\")\n",
    "    \n",
    "    # Guardado de respaldo en CSV (para compatibilidad)\n",
    "    archivo_csv = ruta_processed / 'dataset_etiquetado_para_modelado.csv'\n",
    "    df_features.to_csv(archivo_csv, index=True, encoding='utf-8')\n",
    "    tamaÃ±o_csv = archivo_csv.stat().st_size / 1024**2\n",
    "    print(f\"   ğŸ’¾ Respaldo CSV generado: {tamaÃ±o_csv:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error al guardar: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 5. GeneraciÃ³n de metadatos del dataset\n",
    "print(f\"\\n5ï¸âƒ£ GENERACIÃ“N DE METADATOS:\")\n",
    "\n",
    "metadata_file = ruta_processed / 'feature_engineering_metadata.txt'\n",
    "\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"METADATOS DE FEATURE ENGINEERING\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    f.write(f\"Fecha de generaciÃ³n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Dimensiones del dataset: {df_features.shape[0]:,} filas Ã— {df_features.shape[1]} columnas\\n\")\n",
    "    f.write(f\"PerÃ­odo temporal: {df_features.index.min()} a {df_features.index.max()}\\n\")\n",
    "    f.write(f\"Uso de memoria: {memoria_optimizada:.1f} MB\\n\\n\")\n",
    "    \n",
    "    f.write(\"DISTRIBUCIÃ“N DE ETIQUETAS:\\n\")\n",
    "    conteo_etiquetas = df_features['falla'].value_counts().sort_index()\n",
    "    for etiqueta, cantidad in conteo_etiquetas.items():\n",
    "        porcentaje = (cantidad / len(df_features)) * 100\n",
    "        f.write(f\"  Etiqueta {etiqueta}: {cantidad:,} ({porcentaje:.2f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\nCATEGORÃAS DE CARACTERÃSTICAS:\\n\")\n",
    "    for categoria, features in feature_categories.items():\n",
    "        count = len([f for f in features if f in df_features.columns])\n",
    "        f.write(f\"  {categoria.capitalize()}: {count} features\\n\")\n",
    "    \n",
    "    f.write(f\"\\nVARIABLES CRÃTICAS ORIGINALES:\\n\")\n",
    "    for i, var in enumerate(variables_criticas, 1):\n",
    "        f.write(f\"  {i}. {var}\\n\")\n",
    "    \n",
    "    if len(fechas_falla) > 0:\n",
    "        f.write(f\"\\nEVENTOS DE FALLA PROCESADOS:\\n\")\n",
    "        for i, fecha in enumerate(fechas_falla, 1):\n",
    "            f.write(f\"  {i}. {fecha}\\n\")\n",
    "\n",
    "print(f\"   ğŸ“„ Metadatos guardados en: {metadata_file}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "print(f\"\\nğŸ“ Archivos generados:\")\n",
    "print(f\"   ğŸ—ƒï¸  dataset_etiquetado_para_modelado.parquet - Dataset principal optimizado\")\n",
    "print(f\"   ğŸ’¾ dataset_etiquetado_para_modelado.csv - Respaldo en formato CSV\")\n",
    "print(f\"   ğŸ“„ feature_engineering_metadata.txt - Metadatos detallados\")\n",
    "print(f\"\\nâ¡ï¸  Listo para la siguiente fase: Entrenamiento de Modelos (04_model_training.ipynb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Resumen Ejecutivo del Feature Engineering\n",
    "\n",
    "### âœ… Logros TÃ©cnicos Alcanzados\n",
    "\n",
    "Hemos completado exitosamente la **transformaciÃ³n de datos operacionales** en un **dataset enriquecido y etiquetado**, optimizado para el entrenamiento de modelos de mantenimiento predictivo. Los principales logros incluyen:\n",
    "\n",
    "**ğŸ”¬ IngenierÃ­a de CaracterÃ­sticas Avanzada:**\n",
    "- **Rolling Features**: EstadÃ­sticas de ventanas mÃ³viles en 3 escalas temporales (6H, 24H, 72H)\n",
    "- **Lag Features**: CaracterÃ­sticas de memoria histÃ³rica con intervalos optimizados (2H, 12H, 48H)\n",
    "- **CaracterÃ­sticas Espectrales**: AnÃ¡lisis FFT para detecciÃ³n de patrones de frecuencia\n",
    "- **DetecciÃ³n de AnomalÃ­as**: Z-scores mÃ³viles robustos para identificaciÃ³n de desviaciones\n",
    "- **AnÃ¡lisis de Derivadas**: Tasas de cambio para capturar velocidad de deterioro\n",
    "\n",
    "**ğŸ·ï¸ Sistema de Etiquetado Robusto:**\n",
    "- Ventana de predicciÃ³n de **7 dÃ­as** basada en requisitos operacionales\n",
    "- Procesamiento exitoso de eventos histÃ³ricos de falla\n",
    "- Validaciones exhaustivas de calidad y coherencia temporal\n",
    "- Balance documentado de clases para estrategias de modelado\n",
    "\n",
    "**ğŸ’¾ OptimizaciÃ³n y PreparaciÃ³n:**\n",
    "- ReducciÃ³n significativa del uso de memoria mediante optimizaciÃ³n de tipos\n",
    "- Formato Parquet con compresiÃ³n para eficiencia de almacenamiento\n",
    "- CatalogaciÃ³n completa de caracterÃ­sticas por categorÃ­a\n",
    "- Metadatos exhaustivos para trazabilidad y reproducibilidad\n",
    "\n",
    "### ğŸ¯ Dataset Final - Especificaciones TÃ©cnicas\n",
    "\n",
    "El dataset resultante representa un **activo de alta calidad** para el desarrollo del modelo predictivo:\n",
    "\n",
    "- **Dimensionalidad**: Centenares de caracterÃ­sticas especializadas\n",
    "- **Cobertura Temporal**: Datos histÃ³ricos con resoluciÃ³n horaria\n",
    "- **Integridad**: 100% de completitud despuÃ©s del procesamiento\n",
    "- **Etiquetado**: Sistema binario con horizonte de predicciÃ³n validado\n",
    "- **OptimizaciÃ³n**: Memoria y almacenamiento optimizados para producciÃ³n\n",
    "\n",
    "### ğŸš€ PreparaciÃ³n para Modelado\n",
    "\n",
    "El dataset estÃ¡ **completamente preparado** para las siguientes fases:\n",
    "\n",
    "**Entrenamiento de Modelos:**\n",
    "- CaracterÃ­sticas numÃ©ricas normalizadas y consistentes\n",
    "- Variable objetivo correctamente balanceada y validada\n",
    "- Metadatos disponibles para selecciÃ³n inteligente de features\n",
    "\n",
    "**ValidaciÃ³n y EvaluaciÃ³n:**\n",
    "- Estructura temporal preservada para validaciÃ³n realista\n",
    "- Diversidad de caracterÃ­sticas para anÃ¡lisis de importancia\n",
    "- Trazabilidad hacia variables fÃ­sicas originales\n",
    "\n",
    "### ğŸ’¡ Valor Agregado del Feature Engineering\n",
    "\n",
    "La transformaciÃ³n realizada agrega **valor predictivo sustancial**:\n",
    "\n",
    "- **Captura de DinÃ¡micas Temporales**: Las caracterÃ­sticas temporales revelan patrones de deterioro progresivo\n",
    "- **Robustez ante Ruido**: Las estadÃ­sticas mÃ³viles filtran fluctuaciones operacionales normales\n",
    "- **DetecciÃ³n Temprana**: Las caracterÃ­sticas avanzadas permiten identificaciÃ³n precoz de anomalÃ­as\n",
    "- **Interpretabilidad**: Mantenimiento de conexiÃ³n con parÃ¡metros fÃ­sicos del equipo\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“Š Estado del Proyecto**: âœ… **Feature Engineering Completado**  \n",
    "**ğŸ¯ PrÃ³xima fase**: `04_model_training.ipynb` - Entrenamiento y OptimizaciÃ³n de Modelos  \n",
    "**ğŸ’¾ Datasets disponibles**: Parquet optimizado + CSV de respaldo + Metadatos completos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}