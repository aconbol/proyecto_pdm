{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "## Proyecto de Mantenimiento Predictivo - Moto-Compresores\n",
    "\n",
    "### üéØ Objetivo del Notebook\n",
    "Este notebook realiza la **limpieza y preprocesamiento** de los datos operacionales del moto-compresor para prepararlos para el modelado de Machine Learning. El objetivo es consolidar m√∫ltiples archivos de sensores, limpiar inconsistencias, manejar valores at√≠picos y crear un dataset estructurado y confiable.\n",
    "\n",
    "### üìã Tareas Principales\n",
    "1. **Carga y Consolidaci√≥n**: Integrar datos de 28 archivos Excel de sensores\n",
    "2. **Limpieza de Estructura**: Estandarizar nombres de columnas y tipos de datos\n",
    "3. **Conversi√≥n Temporal**: Procesar diferentes formatos de fecha/hora\n",
    "4. **Tratamiento de Valores Faltantes**: Interpolaci√≥n basada en tiempo\n",
    "5. **Manejo de Outliers**: Clipping estad√≠stico para preservar informaci√≥n √∫til\n",
    "6. **Validaci√≥n de Calidad**: M√©tricas de evaluaci√≥n del procesamiento\n",
    "\n",
    "### üõ†Ô∏è Librer√≠as Utilizadas\n",
    "- **pandas**: Manipulaci√≥n y an√°lisis de datos\n",
    "- **numpy**: Operaciones num√©ricas\n",
    "- **pathlib**: Manejo de rutas de archivos\n",
    "- **warnings**: Supresi√≥n de advertencias menores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as esenciales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configuraci√≥n del entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
    "print(f\"üìä Versi√≥n de pandas: {pd.__version__}\")\n",
    "print(f\"üî¢ Versi√≥n de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas de directorio usando pathlib\n",
    "# Esto garantiza compatibilidad cross-platform y rutas relativas robustas\n",
    "# Directorio base del proyecto\n",
    "base_dir = Path('..')\n",
    "\n",
    "# Rutas de datos\n",
    "ruta_raw = base_dir / 'data' / 'raw'\n",
    "ruta_processed = base_dir / 'data' / 'processed'\n",
    "ruta_eventos = base_dir / 'eventos'\n",
    "\n",
    "# Crear directorio processed si no existe\n",
    "ruta_processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Validaci√≥n de rutas\n",
    "print(\"üìÅ Configuraci√≥n de rutas:\")\n",
    "print(f\"   Raw data: {ruta_raw} - {'‚úÖ Existe' if ruta_raw.exists() else '‚ùå No existe'}\")\n",
    "print(f\"   Processed: {ruta_processed} - {'‚úÖ Existe' if ruta_processed.exists() else '‚ùå No existe'}\")\n",
    "print(f\"   Eventos: {ruta_eventos} - {'‚úÖ Existe' if ruta_eventos.exists() else '‚ùå No existe'}\")\n",
    "\n",
    "# Listar archivos disponibles\n",
    "archivos_excel = list(ruta_raw.glob('*.xls*'))\n",
    "print(f\"\\nüìä Archivos de datos encontrados: {len(archivos_excel)} archivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìÅ Funci√≥n de Carga y Consolidaci√≥n de Datos\n",
    "\n",
    "### üîß Estrategia T√©cnica\n",
    "Desarrollaremos una funci√≥n robusta que:\n",
    "- **Detecta autom√°ticamente** la estructura de cada archivo Excel\n",
    "- **Identifica el encabezado** buscando palabras clave como 'COMPRESOR' y 'MOTOR'\n",
    "- **Maneja diferentes engines** (.xls con xlrd, .xlsx con openpyxl)\n",
    "- **Consolida datos** de m√∫ltiples archivos manteniendo consistencia\n",
    "\n",
    "Esta aproximaci√≥n es porque los archivos industriales pueden tener estructuras variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_fila_encabezado(df_raw):\n",
    "    \"\"\"\n",
    "    Detecta autom√°ticamente la fila donde comienzan los encabezados de columnas\n",
    "    buscando palabras clave relacionadas con el moto-compresor\n",
    "    \"\"\"\n",
    "    palabras_clave = ['COMPRESOR', 'MOTOR', 'HORA', 'TIEMPO', 'TEMP', 'PRES']\n",
    "    \n",
    "    for idx, fila in df_raw.iterrows():\n",
    "        # Convertir toda la fila a string y buscar palabras clave\n",
    "        fila_str = ' '.join([str(cell).upper() for cell in fila if pd.notna(cell)])\n",
    "        \n",
    "        # Si encontramos al menos 2 palabras clave, probablemente es el encabezado\n",
    "        coincidencias = sum(1 for palabra in palabras_clave if palabra in fila_str)\n",
    "        if coincidencias >= 2:\n",
    "            return idx\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cargar_archivo_sensor(file_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo de sensores con detecci√≥n autom√°tica de estructura\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar archivo completo para an√°lisis de estructura\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df_raw = pd.read_excel(file_path, header=None, engine='openpyxl')\n",
    "        else:\n",
    "            df_raw = pd.read_excel(file_path, header=None, engine='xlrd')\n",
    "        \n",
    "        # Detectar fila de encabezado\n",
    "        header_row = detectar_fila_encabezado(df_raw)\n",
    "        \n",
    "        if header_row is None:\n",
    "            print(f\"‚ö†Ô∏è  No se detect√≥ encabezado autom√°ticamente en {file_path.name}, usando fila 0\")\n",
    "            header_row = 0\n",
    "        \n",
    "        # Cargar datos con el encabezado correcto\n",
    "        if file_path.suffix == '.xlsx':\n",
    "            df = pd.read_excel(file_path, \n",
    "                             header=header_row,\n",
    "                             skiprows=range(0, header_row) if header_row > 0 else None,\n",
    "                             engine='openpyxl')\n",
    "        else:\n",
    "            df = pd.read_excel(file_path, \n",
    "                             header=header_row,\n",
    "                             skiprows=range(0, header_row) if header_row > 0 else None,\n",
    "                             engine='xlrd')\n",
    "        \n",
    "        # Informaci√≥n b√°sica del archivo\n",
    "        print(f\"üìã {file_path.name}: {df.shape[0]} filas, {df.shape[1]} columnas (encabezado en fila {header_row})\")\n",
    "        \n",
    "        return df, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando {file_path.name}: {str(e)}\")\n",
    "        return None, False\n",
    "\n",
    "def cargar_y_consolidar_datos(ruta_raw):\n",
    "    \"\"\"\n",
    "    Carga y consolida todos los archivos de sensores de la ruta especificada\n",
    "    \"\"\"\n",
    "    archivos_excel = list(ruta_raw.glob('*.xls*'))\n",
    "    \n",
    "    if not archivos_excel:\n",
    "        raise FileNotFoundError(f\"No se encontraron archivos Excel en {ruta_raw}\")\n",
    "    \n",
    "    print(f\"üîÑ Iniciando consolidaci√≥n de {len(archivos_excel)} archivos...\\n\")\n",
    "    \n",
    "    dataframes_lista = []\n",
    "    archivos_exitosos = []\n",
    "    archivos_fallidos = []\n",
    "    \n",
    "    for archivo in sorted(archivos_excel):  # Ordenar para procesamiento consistente\n",
    "        df, exito = cargar_archivo_sensor(archivo)\n",
    "        \n",
    "        if exito and df is not None and not df.empty:\n",
    "            dataframes_lista.append(df)\n",
    "            archivos_exitosos.append(archivo.name)\n",
    "        else:\n",
    "            archivos_fallidos.append(archivo.name)\n",
    "    \n",
    "    if not dataframes_lista:\n",
    "        raise ValueError(\"No se pudo cargar ning√∫n archivo exitosamente\")\n",
    "    \n",
    "    # Consolidar todos los DataFrames\n",
    "    print(f\"\\nüîó Consolidando {len(dataframes_lista)} DataFrames...\")\n",
    "    df_consolidado = pd.concat(dataframes_lista, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Reporte de consolidaci√≥n\n",
    "    print(f\"\\nüìä Resumen de Consolidaci√≥n:\")\n",
    "    print(f\"   ‚úÖ Archivos exitosos: {len(archivos_exitosos)}\")\n",
    "    print(f\"   ‚ùå Archivos fallidos: {len(archivos_fallidos)}\")\n",
    "    print(f\"   üìà Dataset final: {df_consolidado.shape[0]:,} filas, {df_consolidado.shape[1]} columnas\")\n",
    "    \n",
    "    if archivos_fallidos:\n",
    "        print(f\"\\n‚ö†Ô∏è  Archivos que fallaron: {', '.join(archivos_fallidos)}\")\n",
    "    \n",
    "    return df_consolidado, archivos_exitosos, archivos_fallidos\n",
    "\n",
    "# Ejecutar consolidaci√≥n\n",
    "print(\"üöÄ Ejecutando carga y consolidaci√≥n de datos...\")\n",
    "df_raw, exitosos, fallidos = cargar_y_consolidar_datos(ruta_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üßπ Pipeline de Limpieza de Datos\n",
    "\n",
    "### üìù Justificaci√≥n de la Estrategia\n",
    "Los datos industriales presentan desaf√≠os t√≠picos:\n",
    "- **Nombres inconsistentes**: Espacios, saltos de l√≠nea, caracteres especiales\n",
    "- **Formatos temporales variables**: N√∫meros de Excel vs. strings de fecha\n",
    "- **Tipos de datos mixtos**: Strings donde deber√≠an ser num√©ricos\n",
    "\n",
    "Implementaremos un pipeline robusto que maneja estos problemas sistem√°ticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_nombres_columnas(df):\n",
    "    \"\"\"\n",
    "    Limpia y estandariza los nombres de columnas a formato snake_case\n",
    "    \"\"\"\n",
    "    print(\"üî§ Limpiando nombres de columnas...\")\n",
    "    \n",
    "    # Mostrar algunos nombres originales para referencia\n",
    "    print(f\"\\nüìã Muestra de nombres originales:\")\n",
    "    for i, col in enumerate(df.columns[:5]):\n",
    "        print(f\"   {i+1}. '{col}'\")\n",
    "    \n",
    "    nombres_limpios = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Convertir a string si no lo es\n",
    "        nombre = str(col)\n",
    "        \n",
    "        # Convertir a min√∫sculas\n",
    "        nombre = nombre.lower()\n",
    "        \n",
    "        # Reemplazar vocales con acento por las vocales sin acento\n",
    "        nombre = nombre.replace('√°', 'a').replace('√©', 'e').replace('√≠', 'i').replace('√≥', 'o').replace('√∫', 'u')\n",
    "        \n",
    "        # Reemplazar saltos de l√≠nea y espacios m√∫ltiples por un espacio\n",
    "        nombre = re.sub(r'\\s+', ' ', nombre)\n",
    "        \n",
    "        # Reemplazar espacios por guiones bajos\n",
    "        nombre = nombre.replace(' ', '_')\n",
    "        \n",
    "        # Eliminar caracteres especiales, mantener solo letras, n√∫meros y guiones bajos\n",
    "        nombre = re.sub(r'[^a-z0-9_]', '', nombre)\n",
    "        \n",
    "        # Eliminar guiones bajos m√∫ltiples\n",
    "        nombre = re.sub(r'_+', '_', nombre)\n",
    "        \n",
    "        # Eliminar guiones bajos al inicio y final\n",
    "        nombre = nombre.strip('_')\n",
    "        \n",
    "        # Si el nombre est√° vac√≠o, generar uno gen√©rico\n",
    "        if not nombre:\n",
    "            nombre = f'columna_{len(nombres_limpios)}'\n",
    "        \n",
    "        \n",
    "        nombres_limpios.append(nombre)\n",
    "    \n",
    "    # Manejar nombres duplicados\n",
    "    nombres_finales = []\n",
    "    contador_nombres = {}\n",
    "    \n",
    "    for nombre in nombres_limpios:\n",
    "        if nombre in contador_nombres:\n",
    "            contador_nombres[nombre] += 1\n",
    "            nombre_final = f\"{nombre}_{contador_nombres[nombre]}\"\n",
    "        else:\n",
    "            contador_nombres[nombre] = 0\n",
    "            nombre_final = nombre\n",
    "        \n",
    "        nombres_finales.append(nombre_final)\n",
    "    \n",
    "    # Aplicar nombres limpios\n",
    "    df.columns = nombres_finales\n",
    "    \n",
    "    print(f\"\\nüìã Muestra de nombres limpios:\")\n",
    "    for i, col in enumerate(df.columns[:5]):\n",
    "        print(f\"   {i+1}. '{col}'\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Limpieza de nombres completada: {len(nombres_finales)} columnas procesadas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejecutar limpieza de nombres\n",
    "df_clean = limpiar_nombres_columnas(df_raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_columna_tiempo(df):\n",
    "    \"\"\"\n",
    "    Detecta la columna de tiempo bas√°ndose en nombres comunes y contenido\n",
    "    \"\"\"\n",
    "    nombres_tiempo = ['hora', 'tiempo', 'time', 'fecha', 'date', 'timestamp']\n",
    "    \n",
    "    # Buscar por nombre\n",
    "    for col in df.columns:\n",
    "        if any(nombre in col.lower() for nombre in nombres_tiempo):\n",
    "            return col\n",
    "    \n",
    "    # Si no encontramos por nombre, buscar la primera columna que parezca temporal\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Verificar si los valores pueden ser fechas\n",
    "            muestra = df[col].dropna().head(100)\n",
    "            if muestra.empty:\n",
    "                continue\n",
    "                \n",
    "            # Intentar conversi√≥n a datetime\n",
    "            pd.to_datetime(muestra, errors='raise')\n",
    "            return col\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def convertir_tiempo_excel_inteligente(serie_tiempo):\n",
    "    \"\"\"\n",
    "    Convierte una serie de tiempo manejando diferentes formatos:\n",
    "    - N√∫meros de Excel (d√≠as desde 1899-12-30)\n",
    "    - Strings de fecha/hora\n",
    "    - Timestamps ya convertidos\n",
    "    \"\"\"\n",
    "    if serie_tiempo.empty:\n",
    "        return serie_tiempo\n",
    "    \n",
    "    # Tomar una muestra para detectar formato\n",
    "    muestra = serie_tiempo.dropna().head(10)\n",
    "    \n",
    "    if muestra.empty:\n",
    "        return serie_tiempo\n",
    "    \n",
    "    primer_valor = muestra.iloc[0]\n",
    "    \n",
    "    try:\n",
    "        # Caso 1: Ya es datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(serie_tiempo):\n",
    "            print(\"   ‚úÖ Columna ya en formato datetime\")\n",
    "            return serie_tiempo\n",
    "        \n",
    "        # Caso 2: N√∫meros de Excel (t√≠picamente entre 40000-50000 para fechas 2009-2037)\n",
    "        if pd.api.types.is_numeric_dtype(serie_tiempo):\n",
    "            if isinstance(primer_valor, (int, float)) and 30000 < primer_valor < 60000:\n",
    "                print(f\"   üî¢ Detectado formato num√©rico Excel (ej: {primer_valor})\")\n",
    "                return pd.to_datetime(serie_tiempo, unit='D', origin='1899-12-30', errors='coerce')\n",
    "        \n",
    "        # Caso 3: Strings de fecha/hora\n",
    "        if isinstance(primer_valor, str):\n",
    "            print(f\"   üìù Detectado formato string (ej: '{primer_valor}')\")\n",
    "            return pd.to_datetime(serie_tiempo, errors='coerce', infer_datetime_format=True)\n",
    "        \n",
    "        # Caso 4: Intentar conversi√≥n general\n",
    "        print(f\"   üîÑ Intentando conversi√≥n general para tipo: {type(primer_valor)}\")\n",
    "        return pd.to_datetime(serie_tiempo, errors='coerce')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error en conversi√≥n temporal: {str(e)}\")\n",
    "        return pd.to_datetime(serie_tiempo, errors='coerce')\n",
    "\n",
    "def convertir_tipos_datos(df):\n",
    "    \"\"\"\n",
    "    Convierte los tipos de datos apropiadamente:\n",
    "    - Columna de tiempo a datetime\n",
    "    - Resto de columnas a num√©rico (float64)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Convirtiendo tipos de datos...\")\n",
    "    \n",
    "    # Detectar columna de tiempo\n",
    "    col_tiempo = detectar_columna_tiempo(df)\n",
    "    \n",
    "    if col_tiempo:\n",
    "        print(f\"‚è∞ Columna de tiempo detectada: '{col_tiempo}'\")\n",
    "        \n",
    "        # Convertir columna de tiempo\n",
    "        df[col_tiempo] = convertir_tiempo_excel_inteligente(df[col_tiempo])\n",
    "        \n",
    "        # Verificar conversi√≥n exitosa\n",
    "        valores_validos = df[col_tiempo].notna().sum()\n",
    "        total_valores = len(df[col_tiempo])\n",
    "        \n",
    "        print(f\"   üìä Conversi√≥n temporal: {valores_validos:,}/{total_valores:,} valores v√°lidos ({valores_validos/total_valores*100:.1f}%)\")\n",
    "        \n",
    "        if valores_validos > 0:\n",
    "            print(f\"   üìÖ Rango temporal: {df[col_tiempo].min()} a {df[col_tiempo].max()}\")\n",
    "            \n",
    "            # Establecer como √≠ndice\n",
    "            df_indexed = df.set_index(col_tiempo)\n",
    "            print(f\"   ‚úÖ Columna '{col_tiempo}' establecida como √≠ndice\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Conversi√≥n temporal fall√≥, manteniendo √≠ndice num√©rico\")\n",
    "            df_indexed = df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No se detect√≥ columna de tiempo, manteniendo √≠ndice num√©rico\")\n",
    "        df_indexed = df\n",
    "    \n",
    "    # Convertir columnas restantes a num√©rico\n",
    "    print(\"\\nüî¢ Convirtiendo columnas num√©ricas...\")\n",
    "    \n",
    "    columnas_numericas = [col for col in df_indexed.columns if col != col_tiempo]\n",
    "    conversiones_exitosas = 0\n",
    "    \n",
    "    for col in columnas_numericas:\n",
    "        try:\n",
    "            # Intentar conversi√≥n a num√©rico\n",
    "            valores_originales = df_indexed[col].notna().sum()\n",
    "            df_indexed[col] = pd.to_numeric(df_indexed[col], errors='coerce')\n",
    "            valores_finales = df_indexed[col].notna().sum()\n",
    "            \n",
    "            if valores_finales >= valores_originales * 0.8:  # 80% de √©xito m√≠nimo\n",
    "                conversiones_exitosas += 1\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  {col}: conversi√≥n perdi√≥ muchos valores ({valores_finales}/{valores_originales})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error convirtiendo {col}: {str(e)}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Conversiones num√©ricas exitosas: {conversiones_exitosas}/{len(columnas_numericas)} columnas\")\n",
    "    \n",
    "    return df_indexed\n",
    "\n",
    "# Ejecutar conversi√≥n de tipos\n",
    "df_typed = convertir_tipos_datos(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar columnas y datos actuales\n",
    "print(df_typed.head(2))\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîß Tratamiento de Valores Faltantes y At√≠picos\n",
    "\n",
    "### ‚ö†Ô∏è Borrar registro de nulos y columnas no identificadas\n",
    "\n",
    "### üìä Estrategia para Valores Faltantes\n",
    "Para series temporales industriales, la **interpolaci√≥n basada en tiempo** es superior a otros m√©todos porque:\n",
    "- **Preserva tendencias temporales** naturales del proceso industrial\n",
    "- **Mantiene correlaciones** entre variables del moto-compresor\n",
    "- **Es m√°s realista** que forward-fill o valores promedio est√°ticos\n",
    "\n",
    "### üìà Estrategia para Outliers\n",
    "Aplicaremos **clipping estad√≠stico global** (percentiles 1-99) porque:\n",
    "- **Preserva informaci√≥n** de eventos operacionales reales\n",
    "- **Elimina errores de medici√≥n** extremos sin perder datos √∫tiles\n",
    "- **Mantiene consistencia** en rangos operacionales para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con todos los valores vac√≠os o nulos\n",
    "df_typed.dropna(inplace=True, how='all')\n",
    "\n",
    "# Eliminar columnas que su nombre empiece con 'unnamed'\n",
    "unnamed_cols = [col for col in df_typed.columns if col.startswith('unnamed')]\n",
    "df_typed.drop(columns=unnamed_cols, inplace=True)\n",
    "\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Analiza el patr√≥n de valores faltantes en el dataset\n",
    "    \"\"\"\n",
    "    print(\"üîç Analizando valores faltantes...\")\n",
    "    \n",
    "    total_filas = len(df)\n",
    "    \n",
    "    # Calcular estad√≠sticas de valores faltantes por columna\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'columna': df.columns,\n",
    "        'valores_faltantes': df.isnull().sum(),\n",
    "        'porcentaje_faltante': (df.isnull().sum() / total_filas * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Ordenar por porcentaje de valores faltantes\n",
    "    missing_stats = missing_stats.sort_values('porcentaje_faltante', ascending=False)\n",
    "    \n",
    "    # Mostrar estad√≠sticas generales\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    total_valores = df.size\n",
    "    porcentaje_total = (total_missing / total_valores * 100)\n",
    "    \n",
    "    print(f\"\\nüìä Resumen General:\")\n",
    "    print(f\"   Total de valores: {total_valores:,}\")\n",
    "    print(f\"   Valores faltantes: {total_missing:,} ({porcentaje_total:.2f}%)\")\n",
    "    print(f\"   Columnas con valores faltantes: {(missing_stats['valores_faltantes'] > 0).sum()}\")\n",
    "    \n",
    "    # Mostrar columnas con m√°s valores faltantes\n",
    "    columnas_con_faltantes = missing_stats[missing_stats['valores_faltantes'] > 0]\n",
    "    \n",
    "    if not columnas_con_faltantes.empty:\n",
    "        print(f\"\\nüìã Top 10 columnas con m√°s valores faltantes:\")\n",
    "        for _, row in columnas_con_faltantes.head(10).iterrows():\n",
    "            print(f\"   {row['columna']}: {row['valores_faltantes']:,} ({row['porcentaje_faltante']}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ ¬°No hay valores faltantes en el dataset!\")\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "def tratar_valores_faltantes(df):\n",
    "    \"\"\"\n",
    "    Trata los valores faltantes usando interpolaci√≥n temporal\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß Aplicando tratamiento de valores faltantes...\")\n",
    "    \n",
    "    # Estad√≠sticas antes del tratamiento\n",
    "    missing_antes = df.isnull().sum().sum()\n",
    "    \n",
    "    if missing_antes == 0:\n",
    "        print(\"‚úÖ No hay valores faltantes que tratar\")\n",
    "        return df, 0, 0\n",
    "    \n",
    "    try:\n",
    "        # Crear una copia del DataFrame para evitar modificar el original\n",
    "        df_filled = df.copy()\n",
    "        \n",
    "        # Aplicar interpolaci√≥n temporal si tenemos √≠ndice de tiempo\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            print(\"   üìÖ Aplicando interpolaci√≥n temporal (method='time')...\")\n",
    "            # Verificar si hay NaNs en el √≠ndice que podr√≠an causar problemas\n",
    "            if df.index.isna().any():\n",
    "                print(\"   ‚ö†Ô∏è  Advertencia: √çndice contiene valores NaN, usando interpolaci√≥n lineal\")\n",
    "                df_filled = df_filled.interpolate(method='linear', limit_direction='both')\n",
    "            else:\n",
    "                df_filled = df_filled.interpolate(method='time', limit_direction='both')\n",
    "        else:\n",
    "            print(\"   üìà Aplicando interpolaci√≥n lineal (no hay √≠ndice temporal)...\")\n",
    "            df_filled = df_filled.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "        # Para valores que siguen faltando al inicio/final, usar backward/forward fill\n",
    "        # Reemplazar m√©todos deprecated\n",
    "        df_filled = df_filled.bfill().ffill()\n",
    "        \n",
    "        # Estad√≠sticas despu√©s del tratamiento\n",
    "        missing_despues = df_filled.isnull().sum().sum()\n",
    "        valores_interpolados = missing_antes - missing_despues\n",
    "        \n",
    "        print(f\"   üìä Resultados:\")\n",
    "        print(f\"      Valores faltantes antes: {missing_antes:,}\")\n",
    "        print(f\"      Valores faltantes despu√©s: {missing_despues:,}\")\n",
    "        print(f\"      Valores interpolados: {valores_interpolados:,}\")\n",
    "        \n",
    "        if missing_antes > 0:\n",
    "            print(f\"      Tasa de √©xito: {(valores_interpolados/missing_antes*100):.1f}%\")\n",
    "        \n",
    "        return df_filled, missing_antes, valores_interpolados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error durante la interpolaci√≥n: {str(e)}\")\n",
    "        print(\"   üîÑ Intentando m√©todo alternativo con forward/backward fill √∫nicamente...\")\n",
    "        \n",
    "        try:\n",
    "            # M√©todo alternativo: solo usar forward/backward fill\n",
    "            df_filled = df.copy()\n",
    "            df_filled = df_filled.bfill().ffill()\n",
    "            \n",
    "            missing_despues = df_filled.isnull().sum().sum()\n",
    "            valores_interpolados = missing_antes - missing_despues\n",
    "            \n",
    "            print(f\"   üìä Resultados (m√©todo alternativo):\")\n",
    "            print(f\"      Valores faltantes antes: {missing_antes:,}\")\n",
    "            print(f\"      Valores faltantes despu√©s: {missing_despues:,}\")\n",
    "            print(f\"      Valores rellenados: {valores_interpolados:,}\")\n",
    "            \n",
    "            if missing_antes > 0:\n",
    "                print(f\"      Tasa de √©xito: {(valores_interpolados/missing_antes*100):.1f}%\")\n",
    "            \n",
    "            return df_filled, missing_antes, valores_interpolados\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Error tambi√©n en m√©todo alternativo: {str(e2)}\")\n",
    "            print(\"   ‚ö†Ô∏è  Retornando DataFrame original sin cambios\")\n",
    "            return df, missing_antes, 0\n",
    "\n",
    "\n",
    "# Ejecutar an√°lisis y tratamiento de valores faltantes\n",
    "missing_stats = analizar_valores_faltantes(df_typed)\n",
    "df_no_missing, missing_original, interpolated = tratar_valores_faltantes(df_typed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_outliers(df):\n",
    "    \"\"\"\n",
    "    Analiza outliers en las columnas num√©ricas del dataset\n",
    "    \"\"\"\n",
    "    print(\"üîç Analizando outliers en columnas num√©ricas...\")\n",
    "    \n",
    "    # Seleccionar solo columnas num√©ricas\n",
    "    cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(cols_numericas) == 0:\n",
    "        print(\"‚ö†Ô∏è  No se encontraron columnas num√©ricas\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    outlier_stats = []\n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        serie = df[col].dropna()\n",
    "        \n",
    "        if len(serie) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular percentiles\n",
    "        q01 = serie.quantile(0.01)\n",
    "        q99 = serie.quantile(0.99)\n",
    "        q25 = serie.quantile(0.25)\n",
    "        q75 = serie.quantile(0.75)\n",
    "        \n",
    "        # Contar outliers usando m√©todo de percentiles\n",
    "        outliers_bajos = (serie < q01).sum()\n",
    "        outliers_altos = (serie > q99).sum()\n",
    "        total_outliers = outliers_bajos + outliers_altos\n",
    "        \n",
    "        # Contar outliers usando m√©todo IQR (para comparaci√≥n)\n",
    "        iqr = q75 - q25\n",
    "        lower_iqr = q25 - 1.5 * iqr\n",
    "        upper_iqr = q75 + 1.5 * iqr\n",
    "        outliers_iqr = ((serie < lower_iqr) | (serie > upper_iqr)).sum()\n",
    "        \n",
    "        outlier_stats.append({\n",
    "            'columna': col,\n",
    "            'total_valores': len(serie),\n",
    "            'outliers_percentil': total_outliers,\n",
    "            'outliers_iqr': outliers_iqr,\n",
    "            'porcentaje_percentil': (total_outliers / len(serie) * 100),\n",
    "            'porcentaje_iqr': (outliers_iqr / len(serie) * 100),\n",
    "            'q01': q01,\n",
    "            'q99': q99,\n",
    "            'min_original': serie.min(),\n",
    "            'max_original': serie.max()\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_stats)\n",
    "    outlier_df = outlier_df.sort_values('porcentaje_percentil', ascending=False)\n",
    "    \n",
    "    # Resumen general\n",
    "    total_outliers_percentil = outlier_df['outliers_percentil'].sum()\n",
    "    total_valores = outlier_df['total_valores'].sum()\n",
    "    \n",
    "    print(f\"\\nüìä Resumen de Outliers:\")\n",
    "    print(f\"   Columnas analizadas: {len(cols_numericas)}\")\n",
    "    print(f\"   Total outliers (percentil 1-99): {total_outliers_percentil:,} ({total_outliers_percentil/total_valores*100:.2f}%)\")\n",
    "    \n",
    "    # Mostrar top columnas con m√°s outliers\n",
    "    print(f\"\\nüìã Top 10 columnas con m√°s outliers (m√©todo percentil):\")\n",
    "    for _, row in outlier_df.head(10).iterrows():\n",
    "        print(f\"   {row['columna']}: {row['outliers_percentil']:,} outliers ({row['porcentaje_percentil']:.1f}%)\")\n",
    "    \n",
    "    return outlier_df\n",
    "\n",
    "def aplicar_clipping_outliers(df):\n",
    "    \"\"\"\n",
    "    Aplica clipping de outliers usando percentiles 1 y 99\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß Aplicando clipping de outliers...\")\n",
    "    \n",
    "    # Seleccionar columnas num√©ricas\n",
    "    cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(cols_numericas) == 0:\n",
    "        print(\"‚ö†Ô∏è  No se encontraron columnas num√©ricas para procesar\")\n",
    "        return df, 0\n",
    "    \n",
    "    df_clipped = df.copy()\n",
    "    total_clipped = 0\n",
    "    \n",
    "    print(f\"   üìä Procesando {len(cols_numericas)} columnas num√©ricas...\")\n",
    "    \n",
    "    for col in cols_numericas:\n",
    "        serie_original = df_clipped[col]\n",
    "        serie_valida = serie_original.dropna()\n",
    "        \n",
    "        if len(serie_valida) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular l√≠mites de clipping\n",
    "        limite_inferior = serie_valida.quantile(0.01)\n",
    "        limite_superior = serie_valida.quantile(0.99)\n",
    "        \n",
    "        # Aplicar clipping\n",
    "        serie_clipped = serie_original.clip(lower=limite_inferior, upper=limite_superior)\n",
    "        \n",
    "        # Contar valores modificados\n",
    "        valores_modificados = (serie_original != serie_clipped).sum()\n",
    "        total_clipped += valores_modificados\n",
    "        \n",
    "        # Actualizar la columna\n",
    "        df_clipped[col] = serie_clipped\n",
    "        \n",
    "        if valores_modificados > 0:\n",
    "            porcentaje = (valores_modificados / len(serie_original) * 100)\n",
    "            print(f\"      {col}: {valores_modificados:,} valores clipped ({porcentaje:.1f}%) [{limite_inferior:.2f}, {limite_superior:.2f}]\")\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Clipping completado: {total_clipped:,} valores modificados en total\")\n",
    "    \n",
    "    return df_clipped, total_clipped\n",
    "\n",
    "# Ejecutar an√°lisis y tratamiento de outliers\n",
    "outlier_stats = analizar_outliers(df_no_missing)\n",
    "df_final, total_clipped = aplicar_clipping_outliers(df_no_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìä Validaci√≥n de Calidad de Datos\n",
    "\n",
    "### üéØ M√©tricas de Calidad\n",
    "Evaluaremos la calidad del preprocesamiento mediante m√©tricas cuantitativas que demuestren la efectividad de cada paso del pipeline de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_reporte_calidad(df_original, df_final, missing_original, interpolated, total_clipped):\n",
    "    \"\"\"\n",
    "    Genera un reporte completo de la calidad del preprocesamiento\n",
    "    \"\"\"\n",
    "    print(\"üìã REPORTE DE CALIDAD DEL PREPROCESAMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Estad√≠sticas dimensionales\n",
    "    print(\"\\n1Ô∏è‚É£  DIMENSIONES DEL DATASET\")\n",
    "    print(f\"   Filas originales: {len(df_original):,}\")\n",
    "    print(f\"   Filas finales: {len(df_final):,}\")\n",
    "    print(f\"   Columnas originales: {df_original.shape[1]}\")\n",
    "    print(f\"   Columnas finales: {df_final.shape[1]}\")\n",
    "    \n",
    "    # 2. Calidad de datos\n",
    "    print(\"\\n2Ô∏è‚É£  CALIDAD DE DATOS\")\n",
    "    missing_final = df_final.isnull().sum().sum()\n",
    "    total_valores = df_final.size\n",
    "    \n",
    "    print(f\"   Valores faltantes originales: {missing_original:,}\")\n",
    "    print(f\"   Valores interpolados: {interpolated:,}\")\n",
    "    print(f\"   Valores faltantes finales: {missing_final:,}\")\n",
    "    print(f\"   Tasa de completitud: {((total_valores - missing_final) / total_valores * 100):.2f}%\")\n",
    "    \n",
    "    # 3. Tratamiento de outliers\n",
    "    print(\"\\n3Ô∏è‚É£  TRATAMIENTO DE OUTLIERS\")\n",
    "    print(f\"   Valores clipped: {total_clipped:,}\")\n",
    "    print(f\"   Porcentaje de valores modificados: {(total_clipped / total_valores * 100):.2f}%\")\n",
    "    \n",
    "    # 4. Tipos de datos\n",
    "    print(\"\\n4Ô∏è‚É£  TIPOS DE DATOS\")\n",
    "    tipos_finales = df_final.dtypes.value_counts()\n",
    "    for tipo, cantidad in tipos_finales.items():\n",
    "        print(f\"   {tipo}: {cantidad} columnas\")\n",
    "    \n",
    "    # 5. Informaci√≥n temporal\n",
    "    print(\"\\n5Ô∏è‚É£  INFORMACI√ìN TEMPORAL\")\n",
    "    if isinstance(df_final.index, pd.DatetimeIndex):\n",
    "        print(f\"   √çndice temporal: ‚úÖ Configurado\")\n",
    "        print(f\"   Rango temporal: {df_final.index.min()} a {df_final.index.max()}\")\n",
    "        print(f\"   Duraci√≥n total: {df_final.index.max() - df_final.index.min()}\")\n",
    "    else:\n",
    "        print(f\"   √çndice temporal: ‚ùå No configurado (√≠ndice num√©rico)\")\n",
    "    \n",
    "    # 6. Resumen de columnas\n",
    "    print(\"\\n6Ô∏è‚É£  RESUMEN DE COLUMNAS\")\n",
    "    cols_numericas = df_final.select_dtypes(include=[np.number]).columns\n",
    "    cols_temporales = df_final.select_dtypes(include=['datetime']).columns\n",
    "    cols_otros = df_final.select_dtypes(exclude=[np.number, 'datetime']).columns\n",
    "    \n",
    "    print(f\"   Columnas num√©ricas: {len(cols_numericas)}\")\n",
    "    print(f\"   Columnas temporales: {len(cols_temporales)}\")\n",
    "    print(f\"   Otras columnas: {len(cols_otros)}\")\n",
    "    \n",
    "    # 7. Estad√≠sticas descriptivas b√°sicas\n",
    "    if len(cols_numericas) > 0:\n",
    "        print(\"\\n7Ô∏è‚É£  ESTAD√çSTICAS DESCRIPTIVAS (Columnas Num√©ricas)\")\n",
    "        stats_desc = df_final[cols_numericas].describe()\n",
    "        print(f\"   Promedio de medias: {stats_desc.loc['mean'].mean():.2f}\")\n",
    "        print(f\"   Promedio de desv. est√°ndar: {stats_desc.loc['std'].mean():.2f}\")\n",
    "        print(f\"   Rango de valores m√≠nimos: [{stats_desc.loc['min'].min():.2f}, {stats_desc.loc['min'].max():.2f}]\")\n",
    "        print(f\"   Rango de valores m√°ximos: [{stats_desc.loc['max'].min():.2f}, {stats_desc.loc['max'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ PREPROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Generar reporte de calidad\n",
    "generar_reporte_calidad(df_raw, df_final, missing_original, interpolated, total_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üíæ Guardado del Dataset Procesado\n",
    "\n",
    "### üéØ Objetivo Final\n",
    "Guardar el dataset completamente procesado en formato CSV para uso en las siguientes fases del proyecto:\n",
    "- **Feature Engineering** (03_feature_engineering.ipynb)\n",
    "- **Model Training** (04_model_training.ipynb)\n",
    "- **Model Evaluation** (05_model_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_dataset_final(df, ruta_destino, nombre_base='clean_timeseries_data',\n",
    "                         archivos_procesados=None, archivos_fallidos=None,\n",
    "                         valores_interpolados=0, valores_clipped=0):\n",
    "    \"\"\"\n",
    "    Guarda dataset en CSV (siempre) y Parquet (usando subprocess para evitar conflictos)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a guardar\n",
    "        ruta_destino: Path del directorio destino\n",
    "        nombre_base: Nombre base sin extensi√≥n\n",
    "        archivos_procesados, archivos_fallidos, valores_interpolados, valores_clipped: Para metadatos\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados del guardado\n",
    "    \"\"\"\n",
    "    ruta_destino = Path(ruta_destino)\n",
    "    ruta_destino.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Guardando dataset final...\")\n",
    "    print(f\"   üìä Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # ========== CSV (SIEMPRE) ==========\n",
    "    try:\n",
    "        archivo_csv = ruta_destino / f\"{nombre_base}.csv\"\n",
    "        print(f\"   üìÑ Guardando CSV...\")\n",
    "        df.to_csv(archivo_csv, index=True, encoding='utf-8')\n",
    "        tama√±o_mb = archivo_csv.stat().st_size / (1024 * 1024)\n",
    "        print(f\"      ‚úÖ CSV: {tama√±o_mb:.1f} MB\")\n",
    "        resultados['csv'] = {'exito': True, 'tama√±o_mb': tama√±o_mb}\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå CSV Error: {str(e)}\")\n",
    "        resultados['csv'] = {'exito': False, 'error': str(e)}\n",
    "        return resultados\n",
    "    \n",
    "    # ========== PARQUET (SUBPROCESS) ==========\n",
    "    archivo_parquet = ruta_destino / f\"{nombre_base}.parquet\"\n",
    "    print(f\"   üì¶ Guardando Parquet (subprocess aislado)...\")\n",
    "    \n",
    "    try:\n",
    "        # Guardar CSV temporal\n",
    "        temp_csv = ruta_destino / 'temp_for_parquet.csv'\n",
    "        df.to_csv(temp_csv, index=True)\n",
    "        \n",
    "        # Script Python completamente aislado\n",
    "        script_content = f'''\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    # Cargar datos\n",
    "    df = pd.read_csv(\"{temp_csv}\", index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Limpiar para Parquet\n",
    "    if hasattr(df.index, 'hasnans') and df.index.hasnans:\n",
    "        df = df[df.index.notna()]\n",
    "    \n",
    "    if hasattr(df.index, 'tz'):\n",
    "        df = df.reset_index()\n",
    "    elif str(type(df.index)) == \"<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\":\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    # Guardar Parquet con configuraci√≥n m√≠nima\n",
    "    df.to_parquet(\"{archivo_parquet}\", engine='pyarrow', index=False, compression='snappy')\n",
    "    print(\"PARQUET_SUCCESS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PARQUET_ERROR: {{e}}\")\n",
    "'''\n",
    "        \n",
    "        # Ejecutar en subprocess aislado\n",
    "        result = subprocess.run([sys.executable, '-c', script_content], \n",
    "                              capture_output=True, text=True, timeout=120)\n",
    "        \n",
    "        # Limpiar temporal\n",
    "        if temp_csv.exists():\n",
    "            temp_csv.unlink()\n",
    "        \n",
    "        # Verificar resultado\n",
    "        if result.returncode == 0 and \"PARQUET_SUCCESS\" in result.stdout:\n",
    "            tama√±o_mb = archivo_parquet.stat().st_size / (1024 * 1024)\n",
    "            print(f\"      ‚úÖ Parquet: {tama√±o_mb:.1f} MB\")\n",
    "            resultados['parquet'] = {'exito': True, 'tama√±o_mb': tama√±o_mb}\n",
    "        else:\n",
    "            error_msg = result.stderr or \"Subprocess failed\"\n",
    "            print(f\"      ‚ùå Parquet: {error_msg[:100]}\")\n",
    "            resultados['parquet'] = {'exito': False, 'error': error_msg}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Parquet Exception: {str(e)}\")\n",
    "        resultados['parquet'] = {'exito': False, 'error': str(e)}\n",
    "    \n",
    "    # ========== METADATOS ==========\n",
    "    try:\n",
    "        archivo_meta = ruta_destino / 'preprocessing_metadata.txt'\n",
    "        with open(archivo_meta, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"METADATOS DEL PREPROCESAMIENTO\\\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "            f.write(f\"Dimensiones: {df.shape[0]:,} √ó {df.shape[1]}\\\\n\")\n",
    "            f.write(f\"Archivos procesados: {len(archivos_procesados or [])}\\\\n\")\n",
    "            f.write(f\"Archivos fallidos: {len(archivos_fallidos or [])}\\\\n\")\n",
    "            f.write(f\"Valores interpolados: {valores_interpolados:,}\\\\n\")\n",
    "            f.write(f\"Valores clipped: {valores_clipped:,}\\\\n\")\n",
    "            \n",
    "            for formato, resultado in resultados.items():\n",
    "                status = \"‚úÖ\" if resultado['exito'] else \"‚ùå\" \n",
    "                f.write(f\"{formato.upper()}: {status}\\\\n\")\n",
    "        \n",
    "        print(f\"   üìÑ Metadatos generados\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # ========== RESUMEN ==========\n",
    "    print(f\"\\\\nüìã Resumen:\")\n",
    "    exitosos = sum(1 for r in resultados.values() if r['exito'])\n",
    "    for formato, resultado in resultados.items():\n",
    "        if resultado['exito']:\n",
    "            print(f\"   ‚úÖ {formato.upper()}: {resultado['tama√±o_mb']:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {formato.upper()}: Error\")\n",
    "    \n",
    "    print(f\"‚úÖ Guardado completado: {exitosos}/{len(resultados)} formatos\")\n",
    "    return resultados\n",
    "    \n",
    "exito_guardado = guardar_dataset_final(df_final, ruta_processed)\n",
    "\n",
    "if exito_guardado:\n",
    "    print(\"\\nüéâ ¬°PREPROCESAMIENTO COMPLETADO EXITOSAMENTE!\")\n",
    "    print(\"\\nüìã Archivos generados en data/processed/:\")\n",
    "    print(\"   üóÉÔ∏è  clean_timeseries_data.csv - Dataset principal\")\n",
    "    print(\"   üìÑ preprocessing_metadata.txt - Metadatos del procesamiento\")\n",
    "    print(\"   üìä column_summary.csv - Resumen de columnas\")\n",
    "    print(\"\\n‚û°Ô∏è  Listo para la siguiente fase: Feature Engineering (03_feature_engineering.ipynb)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Error en el proceso de guardado. Revisar logs anteriores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Resumen del Preprocesamiento\n",
    "\n",
    "### ‚úÖ Tareas Completadas\n",
    "\n",
    "1. **Carga y Consolidaci√≥n Inteligente**\n",
    "   - ‚úÖ Detecci√≥n autom√°tica de estructura de archivos Excel\n",
    "   - ‚úÖ Consolidaci√≥n de 28 archivos en un dataset √∫nico\n",
    "   - ‚úÖ Manejo robusto de diferentes formatos (.xls/.xlsx)\n",
    "\n",
    "2. **Limpieza de Estructura de Datos**\n",
    "   - ‚úÖ Normalizaci√≥n de nombres de columnas a formato snake_case\n",
    "   - ‚úÖ Conversi√≥n inteligente de tipos de datos\n",
    "   - ‚úÖ Manejo de diferentes formatos temporales\n",
    "\n",
    "3. **Tratamiento de Calidad de Datos**\n",
    "   - ‚úÖ Interpolaci√≥n temporal de valores faltantes\n",
    "   - ‚úÖ Clipping estad√≠stico de outliers (percentiles 1-99)\n",
    "   - ‚úÖ Preservaci√≥n de informaci√≥n operacional relevante\n",
    "\n",
    "4. **Validaci√≥n y Documentaci√≥n**\n",
    "   - ‚úÖ M√©tricas completas de calidad de datos\n",
    "   - ‚úÖ Documentaci√≥n exhaustiva del procesamiento\n",
    "   - ‚úÖ Archivos de metadatos para trazabilidad\n",
    "\n",
    "### üîÑ Pipeline de Calidad Implementado\n",
    "\n",
    "```\n",
    "Datos Raw ‚Üí Detecci√≥n Autom√°tica ‚Üí Limpieza ‚Üí Conversi√≥n de Tipos ‚Üí \n",
    "Interpolaci√≥n ‚Üí Clipping ‚Üí Validaci√≥n ‚Üí Dataset Limpio\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üìä Estado**: ‚úÖ **Preprocesamiento Completado**  \n",
    "**‚û°Ô∏è  Siguiente fase**: `03_feature_engineering.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
